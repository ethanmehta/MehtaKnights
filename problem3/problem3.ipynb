{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving CartPole-v0 Efficiently with A3C\n",
    "\n",
    "In this problem, we will return to CartPole-v0, but this time we will employ a different DeepRL strategy known as Asynchronous Advantage Actor-Critic (A3C). A3C tends to offer significantly better results than VPG, as you shall soon see!    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For file saving operations\n",
    "import os\n",
    "\n",
    "# For multiple worker threads\n",
    "from threading import Thread, Lock\n",
    "import multiprocessing\n",
    "from queue import Queue\n",
    "\n",
    "# For math/plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For testing in simulated environments\n",
    "import gym\n",
    "\n",
    "# For training the Actor-Critic model\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the custom `ActorCriticModel`, which inherits from `keras.Model`.\n",
    "\n",
    "You will need to initialize the layers for the neural net as follows:\n",
    "- Dense layer of size 100, with ReLU activation\n",
    "- Logits output layer\n",
    "- Dense layer of size 100, with ReLU activation\n",
    "- Action output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticModel(keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Init from keras.Model\n",
    "        super(ActorCriticModel, self).__init__()\n",
    "        \n",
    "        # Constants for available state space and action space sizes\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        ### TODO: Initialize layers for the neural net ###\n",
    "        self.dense1 = layers.Dense(100, activation='relu')\n",
    "        self.policy_logits = layers.Dense(action_size)\n",
    "        self.dense2 = layers.Dense(100, activation='relu')\n",
    "        self.values = layers.Dense(1)\n",
    "        \n",
    "#         self.dense1 = None\n",
    "#         self.policy_logits = None\n",
    "#         self.dense2 = None\n",
    "#         self.values = None\n",
    "        \n",
    "        ### TODO ###\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Overloaded from keras.Model\n",
    "        \n",
    "        ### TODO: compute forward pass through neural net ###\n",
    "\n",
    "        x = self.dense1(inputs)\n",
    "        logits = self.policy_logits(x)\n",
    "        v1 = self.dense2(inputs)\n",
    "        values = self.values(v1)\n",
    "        \n",
    "#         logits = None\n",
    "#         values = None\n",
    "\n",
    "        ### TODO ###\n",
    "        return logits, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have provided you with a `BaseAgent` class from which our DeepRL agents will inherit. Note that the subclassed agent will specify a `train` method (for training the agent) and a `play` method (for playing back a single episode) - you will end up writing parts of these methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    \"\"\"\n",
    "    Base class from which all custom agents will inherit.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env_name, num_episodes):\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.num_episodes = num_episodes\n",
    "        \n",
    "        self.result_queue = Queue()\n",
    "        \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def play(self, render=False):\n",
    "        pass\n",
    "    \n",
    "    def record_episode(self, episode_num, episode_reward, episode_loss, episode_steps, average_reward = 0, worker_idx = None):\n",
    "        '''\n",
    "        Helper function to log output about completed episode, while also updating the result queue\n",
    "        '''\n",
    "        \n",
    "        average_reward = average_reward * 0.99 + episode_reward * 0.01 if average_reward > 0 else episode_reward\n",
    "        \n",
    "        self.result_queue.put(average_reward)\n",
    "        \n",
    "        print(\n",
    "          f\"Episode: {episode_num} | \"\n",
    "          f\"Moving Average Reward: {round(average_reward)} | \"\n",
    "          f\"Episode Reward: {round(episode_reward)} | \"\n",
    "          f\"Loss per step: {round(float(episode_loss) / episode_steps, 3)} | \"\n",
    "          f\"Steps: {episode_steps} | \"\n",
    "          f\"Worker: {worker_idx}\"\n",
    "        )\n",
    "        return average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: RandomAgent Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will implement a `RandomAgent` so that we have a baseline performance. This will be very similar to what you have seen on earlier problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent that chooses actions randomly\n",
    "    \"\"\"\n",
    "    def play(self, render=False):\n",
    "        # Reset environment\n",
    "        self.env.reset()\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps_taken = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            ### TODO: Choose a random action from the environment's action space ###\n",
    "            _, reward, done, _ = self.env.step(self.env.action_space.sample())\n",
    "#             \n",
    "            ### TODO ###\n",
    "            \n",
    "            if render:\n",
    "                self.env.render() # Only render if requested\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps_taken += 1\n",
    "        \n",
    "        self.env.close()        \n",
    "        return total_reward, steps_taken\n",
    "    \n",
    "    def train(self):\n",
    "        # Since this is a random agent, we won't actually do any real training.\n",
    "        # Instead, we'll simply run through the episodes using play()\n",
    "        weighted_average_reward = 0\n",
    "        total_reward = 0\n",
    "        for episode in range(self.num_episodes):\n",
    "            reward, steps = self.play()\n",
    "            \n",
    "            weighted_average_reward = self.record_episode(episode, reward, 0, steps, weighted_average_reward)\n",
    "            \n",
    "            total_reward += reward \n",
    "\n",
    "        simple_average_reward = total_reward / self.num_episodes\n",
    "        print(f\"After {self.num_episodes}, simple average reward is {simple_average_reward} and weighted average reward is {weighted_average_reward}\")\n",
    "        return weighted_average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've implemented the RandomAgent, let's see what its baseline performance looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Moving Average Reward: 28 | Episode Reward: 28 | Loss per step: 0.0 | Steps: 28 | Worker: None\n",
      "Episode: 1 | Moving Average Reward: 28 | Episode Reward: 21 | Loss per step: 0.0 | Steps: 21 | Worker: None\n",
      "Episode: 2 | Moving Average Reward: 28 | Episode Reward: 21 | Loss per step: 0.0 | Steps: 21 | Worker: None\n",
      "Episode: 3 | Moving Average Reward: 28 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 4 | Moving Average Reward: 28 | Episode Reward: 22 | Loss per step: 0.0 | Steps: 22 | Worker: None\n",
      "Episode: 5 | Moving Average Reward: 27 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 6 | Moving Average Reward: 28 | Episode Reward: 33 | Loss per step: 0.0 | Steps: 33 | Worker: None\n",
      "Episode: 7 | Moving Average Reward: 27 | Episode Reward: 10 | Loss per step: 0.0 | Steps: 10 | Worker: None\n",
      "Episode: 8 | Moving Average Reward: 27 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 9 | Moving Average Reward: 27 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 10 | Moving Average Reward: 27 | Episode Reward: 54 | Loss per step: 0.0 | Steps: 54 | Worker: None\n",
      "Episode: 11 | Moving Average Reward: 27 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 12 | Moving Average Reward: 27 | Episode Reward: 25 | Loss per step: 0.0 | Steps: 25 | Worker: None\n",
      "Episode: 13 | Moving Average Reward: 27 | Episode Reward: 35 | Loss per step: 0.0 | Steps: 35 | Worker: None\n",
      "Episode: 14 | Moving Average Reward: 28 | Episode Reward: 74 | Loss per step: 0.0 | Steps: 74 | Worker: None\n",
      "Episode: 15 | Moving Average Reward: 28 | Episode Reward: 26 | Loss per step: 0.0 | Steps: 26 | Worker: None\n",
      "Episode: 16 | Moving Average Reward: 28 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 17 | Moving Average Reward: 28 | Episode Reward: 24 | Loss per step: 0.0 | Steps: 24 | Worker: None\n",
      "Episode: 18 | Moving Average Reward: 28 | Episode Reward: 21 | Loss per step: 0.0 | Steps: 21 | Worker: None\n",
      "Episode: 19 | Moving Average Reward: 27 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 20 | Moving Average Reward: 27 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 21 | Moving Average Reward: 27 | Episode Reward: 28 | Loss per step: 0.0 | Steps: 28 | Worker: None\n",
      "Episode: 22 | Moving Average Reward: 27 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 23 | Moving Average Reward: 27 | Episode Reward: 35 | Loss per step: 0.0 | Steps: 35 | Worker: None\n",
      "Episode: 24 | Moving Average Reward: 27 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 25 | Moving Average Reward: 27 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 26 | Moving Average Reward: 27 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 27 | Moving Average Reward: 27 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 28 | Moving Average Reward: 27 | Episode Reward: 44 | Loss per step: 0.0 | Steps: 44 | Worker: None\n",
      "Episode: 29 | Moving Average Reward: 27 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 30 | Moving Average Reward: 27 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 31 | Moving Average Reward: 26 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 32 | Moving Average Reward: 26 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 33 | Moving Average Reward: 26 | Episode Reward: 30 | Loss per step: 0.0 | Steps: 30 | Worker: None\n",
      "Episode: 34 | Moving Average Reward: 26 | Episode Reward: 25 | Loss per step: 0.0 | Steps: 25 | Worker: None\n",
      "Episode: 35 | Moving Average Reward: 26 | Episode Reward: 22 | Loss per step: 0.0 | Steps: 22 | Worker: None\n",
      "Episode: 36 | Moving Average Reward: 26 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 37 | Moving Average Reward: 26 | Episode Reward: 19 | Loss per step: 0.0 | Steps: 19 | Worker: None\n",
      "Episode: 38 | Moving Average Reward: 26 | Episode Reward: 25 | Loss per step: 0.0 | Steps: 25 | Worker: None\n",
      "Episode: 39 | Moving Average Reward: 26 | Episode Reward: 17 | Loss per step: 0.0 | Steps: 17 | Worker: None\n",
      "Episode: 40 | Moving Average Reward: 26 | Episode Reward: 18 | Loss per step: 0.0 | Steps: 18 | Worker: None\n",
      "Episode: 41 | Moving Average Reward: 26 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 42 | Moving Average Reward: 26 | Episode Reward: 43 | Loss per step: 0.0 | Steps: 43 | Worker: None\n",
      "Episode: 43 | Moving Average Reward: 26 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 44 | Moving Average Reward: 26 | Episode Reward: 9 | Loss per step: 0.0 | Steps: 9 | Worker: None\n",
      "Episode: 45 | Moving Average Reward: 26 | Episode Reward: 32 | Loss per step: 0.0 | Steps: 32 | Worker: None\n",
      "Episode: 46 | Moving Average Reward: 26 | Episode Reward: 18 | Loss per step: 0.0 | Steps: 18 | Worker: None\n",
      "Episode: 47 | Moving Average Reward: 26 | Episode Reward: 35 | Loss per step: 0.0 | Steps: 35 | Worker: None\n",
      "Episode: 48 | Moving Average Reward: 26 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 49 | Moving Average Reward: 26 | Episode Reward: 18 | Loss per step: 0.0 | Steps: 18 | Worker: None\n",
      "Episode: 50 | Moving Average Reward: 25 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 51 | Moving Average Reward: 26 | Episode Reward: 38 | Loss per step: 0.0 | Steps: 38 | Worker: None\n",
      "Episode: 52 | Moving Average Reward: 26 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 53 | Moving Average Reward: 25 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 54 | Moving Average Reward: 25 | Episode Reward: 28 | Loss per step: 0.0 | Steps: 28 | Worker: None\n",
      "Episode: 55 | Moving Average Reward: 25 | Episode Reward: 9 | Loss per step: 0.0 | Steps: 9 | Worker: None\n",
      "Episode: 56 | Moving Average Reward: 25 | Episode Reward: 22 | Loss per step: 0.0 | Steps: 22 | Worker: None\n",
      "Episode: 57 | Moving Average Reward: 25 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 58 | Moving Average Reward: 25 | Episode Reward: 33 | Loss per step: 0.0 | Steps: 33 | Worker: None\n",
      "Episode: 59 | Moving Average Reward: 26 | Episode Reward: 72 | Loss per step: 0.0 | Steps: 72 | Worker: None\n",
      "Episode: 60 | Moving Average Reward: 26 | Episode Reward: 24 | Loss per step: 0.0 | Steps: 24 | Worker: None\n",
      "Episode: 61 | Moving Average Reward: 26 | Episode Reward: 39 | Loss per step: 0.0 | Steps: 39 | Worker: None\n",
      "Episode: 62 | Moving Average Reward: 26 | Episode Reward: 34 | Loss per step: 0.0 | Steps: 34 | Worker: None\n",
      "Episode: 63 | Moving Average Reward: 26 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 64 | Moving Average Reward: 26 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 65 | Moving Average Reward: 26 | Episode Reward: 15 | Loss per step: 0.0 | Steps: 15 | Worker: None\n",
      "Episode: 66 | Moving Average Reward: 25 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 67 | Moving Average Reward: 25 | Episode Reward: 30 | Loss per step: 0.0 | Steps: 30 | Worker: None\n",
      "Episode: 68 | Moving Average Reward: 26 | Episode Reward: 62 | Loss per step: 0.0 | Steps: 62 | Worker: None\n",
      "Episode: 69 | Moving Average Reward: 26 | Episode Reward: 29 | Loss per step: 0.0 | Steps: 29 | Worker: None\n",
      "Episode: 70 | Moving Average Reward: 26 | Episode Reward: 24 | Loss per step: 0.0 | Steps: 24 | Worker: None\n",
      "Episode: 71 | Moving Average Reward: 26 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 72 | Moving Average Reward: 26 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 73 | Moving Average Reward: 25 | Episode Reward: 10 | Loss per step: 0.0 | Steps: 10 | Worker: None\n",
      "Episode: 74 | Moving Average Reward: 25 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 75 | Moving Average Reward: 25 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 76 | Moving Average Reward: 25 | Episode Reward: 31 | Loss per step: 0.0 | Steps: 31 | Worker: None\n",
      "Episode: 77 | Moving Average Reward: 25 | Episode Reward: 33 | Loss per step: 0.0 | Steps: 33 | Worker: None\n",
      "Episode: 78 | Moving Average Reward: 25 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 79 | Moving Average Reward: 25 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 80 | Moving Average Reward: 25 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 81 | Moving Average Reward: 25 | Episode Reward: 15 | Loss per step: 0.0 | Steps: 15 | Worker: None\n",
      "Episode: 82 | Moving Average Reward: 25 | Episode Reward: 32 | Loss per step: 0.0 | Steps: 32 | Worker: None\n",
      "Episode: 83 | Moving Average Reward: 25 | Episode Reward: 17 | Loss per step: 0.0 | Steps: 17 | Worker: None\n",
      "Episode: 84 | Moving Average Reward: 25 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 85 | Moving Average Reward: 25 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 86 | Moving Average Reward: 25 | Episode Reward: 18 | Loss per step: 0.0 | Steps: 18 | Worker: None\n",
      "Episode: 87 | Moving Average Reward: 25 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 88 | Moving Average Reward: 25 | Episode Reward: 29 | Loss per step: 0.0 | Steps: 29 | Worker: None\n",
      "Episode: 89 | Moving Average Reward: 25 | Episode Reward: 28 | Loss per step: 0.0 | Steps: 28 | Worker: None\n",
      "Episode: 90 | Moving Average Reward: 25 | Episode Reward: 15 | Loss per step: 0.0 | Steps: 15 | Worker: None\n",
      "Episode: 91 | Moving Average Reward: 24 | Episode Reward: 10 | Loss per step: 0.0 | Steps: 10 | Worker: None\n",
      "Episode: 92 | Moving Average Reward: 25 | Episode Reward: 54 | Loss per step: 0.0 | Steps: 54 | Worker: None\n",
      "Episode: 93 | Moving Average Reward: 25 | Episode Reward: 15 | Loss per step: 0.0 | Steps: 15 | Worker: None\n",
      "Episode: 94 | Moving Average Reward: 24 | Episode Reward: 9 | Loss per step: 0.0 | Steps: 9 | Worker: None\n",
      "Episode: 95 | Moving Average Reward: 25 | Episode Reward: 37 | Loss per step: 0.0 | Steps: 37 | Worker: None\n",
      "Episode: 96 | Moving Average Reward: 25 | Episode Reward: 22 | Loss per step: 0.0 | Steps: 22 | Worker: None\n",
      "Episode: 97 | Moving Average Reward: 24 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 98 | Moving Average Reward: 24 | Episode Reward: 18 | Loss per step: 0.0 | Steps: 18 | Worker: None\n",
      "Episode: 99 | Moving Average Reward: 24 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "After 100, simple average reward is 22.31 and weighted average reward is 24.252331623834582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28.0, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_agent = RandomAgent(\"CartPole-v0\", 100)\n",
    "random_agent.train()\n",
    "\n",
    "random_agent.play(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw, the episode ends almost instantly, because the random policy is not very good! With an average reward of only about 20, we are very far away from the required 195 reward to meet OpenAI's criteria for solving `CartPole-v0`.\n",
    "\n",
    "Let's see how we can use the A3C approach to get significantly better results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: A3C Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's review what the A3C algorithm is. \n",
    "\n",
    "A3C stands for **Asynchronous Advantage Actor-Critic**. Let's break it down:\n",
    "1. **Aysnchronous:** One key advantage of A3C is the comparative speed at which is can be trained compared to other DeepRL methods like DQN. The secret to this speed is a parallelized approach. In A3C, several worker agents are working in parallel on their own individual, local copies of the environment. After several steps, the worker agents update their local models, and then compare their local models to the single global model. If any individual worker has made an improvement to the global model, the global model is updated and all the asynchronous workers synchronize their local models to the new global model. This approach means we can benefit significantly from the speedups in parallel computation. You will learn much more about parallelization in CS 61C!\n",
    "\n",
    "2. **Advantage:** As you have learned, Advantage is defined as the difference between a better Q-state and the Value of a state per an existing policy. That is: $$A(s, a) = Q(s, a) - V^\\pi(s)$$ A3C focuses on estimating the advantages of actions, instead of trying to specifically determine the individual Q-values.\n",
    "\n",
    "3. **Actor-Critic:** The Actor-Critic model is a typical setup for DeepRL, in which an Actor tries to determine the best action from a given state, and in which a Critic evaluates the chosen action for long-term reward. Usually, the Critic is estimating the Value of the state achieved after taking the action. However, in A3C, the Critic instead estimates the Advantage of the state-action pair from the Actor. The interplay between Actor and Critic helps the system converge faster and achieve better results than either system alone.\n",
    "\n",
    "Now that you have a high-level understanding of what A3C is, let's get into the details and start implementing it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create a class representing the Asynchronous Workers that do the heavy lifting in the A3C approach. Each asynchronous worker maintains a local copy of the model and its own environment, and then periodically checks and synchronizes with the global model. \n",
    "\n",
    "Since this may be your first exposure to multithreaded programming, we have provided most of that structure for you. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncWorkerAgent(Thread, BaseAgent):\n",
    "    \"\"\"\n",
    "    Aysnchronous worker that trains on an individual instance of the environment\n",
    "    \"\"\"\n",
    "\n",
    "    # Global variables across threads\n",
    "    mutex = Lock()\n",
    "    global_episode_idx = 0\n",
    "    global_moving_average_reward = 0\n",
    "    best_reward = 0\n",
    "    \n",
    "    def __init__(self, worker_idx, env_name, num_episodes, global_model, optimizer, global_result_queue, save_dir, update_frequency=20, gamma=0.99):\n",
    "        Thread.__init__(self) \n",
    "        BaseAgent.__init__(self, env_name, num_episodes)\n",
    "        \n",
    "        self.worker_idx = worker_idx\n",
    "        \n",
    "        self.global_model = global_model\n",
    "        self.optimizer = optimizer\n",
    "        self.result_queue = global_result_queue\n",
    "        self.save_dir = save_dir\n",
    "        self.update_frequency = update_frequency \n",
    "        self.gamma = gamma # The discount factor for rewards\n",
    "\n",
    "        ### TODO: Get the size of the state and action spaces from self.env ###\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        \n",
    "#         self.state_size = None\n",
    "#         self.action_size = None\n",
    "        ### TODO ###\n",
    "    \n",
    "        self.local_model = ActorCriticModel(self.state_size, self.action_size)\n",
    "        \n",
    "        # Reset global variables\n",
    "        AsyncWorkerAgent.global_episode_idx = 0\n",
    "        AsyncWorkerAgent.global_moving_average_reward = 0\n",
    "        AsyncWorkerAgent.best_reward = 0\n",
    "        \n",
    "    def run(self):\n",
    "        # Overloaded from Thread\n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        # Even though we have multiple workers, the total number of episodes run is still num_episodes\n",
    "        while AsyncWorkerAgent.global_episode_idx < self.num_episodes:\n",
    "            episode_idx = AsyncWorkerAgent.global_episode_idx\n",
    "            current_state = self.env.reset()\n",
    "        \n",
    "            total_reward = 0\n",
    "            total_loss = 0.0\n",
    "            steps_taken = 0\n",
    "            \n",
    "            # Maintain a buffer of the most recent states, actions, and rewards to train the Actor-Critic from\n",
    "            buffer_states = []\n",
    "            buffer_actions = []\n",
    "            buffer_rewards = []\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                ### TODO: Choose action according to probabilities from neural net ###\n",
    "                logits, _ = self.local_model(tf.convert_to_tensor(current_state[None, :], dtype=tf.float32))\n",
    "                probs = tf.nn.softmax(logits)\n",
    "\n",
    "                action = np.random.choice(self.action_size, p=probs.numpy()[0])\n",
    "\n",
    "    #           # Note that the action space is discrete, so a valid action is an integer between [0, size of space]\n",
    "    #           action = None\n",
    "                ### TODO ###\n",
    "\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                                \n",
    "                buffer_states.append(current_state)\n",
    "                buffer_actions.append(action)\n",
    "                buffer_rewards.append(reward)\n",
    "\n",
    "                total_reward += reward\n",
    "                steps_taken += 1\n",
    "\n",
    "                # Periodically compute gradients to update model\n",
    "                if steps_taken % self.update_frequency == 0 or done:\n",
    "                    # We use TensorFlow's GradientTape functionality to track the gradient of our custom loss function\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        ### TODO: Use the compute_loss() helper function to obtain the loss ###                        \n",
    "                        loss = self.compute_loss(done, new_state, buffer_states, buffer_actions, buffer_rewards, self.gamma)\n",
    "                        #loss = 0\n",
    "                        ### TODO ###\n",
    "            \n",
    "                    buffer_states, buffer_actions, buffer_rewards = [], [], []\n",
    "                    total_loss += loss\n",
    "\n",
    "                    # Calculate local gradients\n",
    "                    grads = tape.gradient(loss, self.local_model.trainable_weights)\n",
    "\n",
    "                    # Push local gradients to global model\n",
    "                    self.optimizer.apply_gradients(zip(grads, self.global_model.trainable_weights))\n",
    "                    \n",
    "                    # Update local model with new weights\n",
    "                    self.local_model.set_weights(self.global_model.get_weights())  \n",
    "                    \n",
    "                current_state = new_state\n",
    "\n",
    "            AsyncWorkerAgent.global_moving_average_reward = \\\n",
    "                  self.record_episode(episode_idx, total_reward, total_loss, steps_taken, AsyncWorkerAgent.global_moving_average_reward, self.worker_idx)\n",
    "\n",
    "            # Check if our local model has surpassed \n",
    "            if total_reward > AsyncWorkerAgent.best_reward:\n",
    "                # Acquire the mutex to save weights without any data races\n",
    "                with AsyncWorkerAgent.mutex:\n",
    "                    weights_file = os.path.join(self.save_dir,\n",
    "                                     f'model_{self.env_name}_{total_reward}.h5')\n",
    "                    print(f\"Saving best model to {weights_file}, episode score: {total_reward}\")\n",
    "                    self.global_model.save_weights(weights_file)\n",
    "                    AsyncWorkerAgent.best_reward = total_reward\n",
    "            \n",
    "            AsyncWorkerAgent.global_episode_idx += 1\n",
    "                \n",
    "\n",
    "    def compute_loss(self, done, state, prev_states, prev_actions, prev_rewards, gamma):\n",
    "        \"\"\"\n",
    "        Helper function to compute custom loss for A3C.\n",
    "        \"\"\"\n",
    "        ### TODO: Initialize reward_sum based on the expected future reward from the current state ###\n",
    "        if done:\n",
    "            reward_sum = 0  # terminal\n",
    "        else:\n",
    "            reward_sum = self.local_model(tf.convert_to_tensor(state[None, :], dtype=tf.float32))[-1].numpy()[0]\n",
    "        \n",
    "#         if done:\n",
    "#             reward_sum = 0\n",
    "#         else:\n",
    "#             reward_sum = 0\n",
    "        ### TODO ###\n",
    "\n",
    "        ### TODO: Discount previous rewards using discount factor\n",
    "        discounted_rewards = []\n",
    "        for reward in prev_rewards[::-1]:\n",
    "            reward_sum = reward + gamma * reward_sum\n",
    "            discounted_rewards.append(reward_sum)\n",
    "        discounted_rewards.reverse()\n",
    "        \n",
    "#         discounted_rewards = []\n",
    "        ### TODO ###\n",
    "\n",
    "        ### TODO: Compute the logits and values using our local model ###\n",
    "        logits, values = self.local_model(tf.convert_to_tensor(np.vstack(prev_states), dtype=tf.float32))\n",
    "#         logits, values = None, None\n",
    "        ### TODO ###\n",
    "        \n",
    "        # Get our advantages\n",
    "        advantage = tf.convert_to_tensor(np.array(discounted_rewards)[:, None],\n",
    "                                dtype=tf.float32) - values\n",
    "        # Value loss\n",
    "        value_loss = advantage ** 2\n",
    "\n",
    "        # Calculate our policy loss\n",
    "        policy = tf.nn.softmax(logits)\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy, logits=logits)\n",
    "\n",
    "        policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=prev_actions,\n",
    "                                                                     logits=logits)\n",
    "        policy_loss *= tf.stop_gradient(advantage)\n",
    "        policy_loss -= 0.01 * entropy\n",
    "        total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've built the individual Asynchronous workers, it's time to build the master A3C agent that will put the workers to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent that uses Asynchronous Advantage Actor-Critic (A3C) method for learning\n",
    "    \"\"\"\n",
    "    def __init__(self, env_name, num_episodes):\n",
    "        # Call base constructor\n",
    "        super().__init__(env_name, num_episodes)\n",
    "\n",
    "        self.save_dir = \"/tmp/\"\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "\n",
    "        ### TODO: Get the size of the state and action spaces from self.env ###\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        \n",
    "#         self.state_size = None\n",
    "#         self.action_size = None\n",
    "        ### TODO ###\n",
    "        \n",
    "        learning_rate = 0.001\n",
    "        ### TODO: Initialize Adam optimizer with learning rate 0.001 ###\n",
    "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate, use_locking=True)       \n",
    "#         self.optimizer = None\n",
    "        ### TODO ###\n",
    "        \n",
    "        ### TODO: Create an ActorCritic model using our custom class, and initialize it with random data ###\n",
    "        self.global_model = ActorCriticModel(self.state_size, self.action_size)  \n",
    "        self.global_model(tf.convert_to_tensor(np.random.random((1, self.state_size)), dtype=tf.float32))\n",
    "        #         self.global_model = None\n",
    "        ### TODO ###        \n",
    "        \n",
    "    def train(self):\n",
    "\n",
    "        ### TODO: Create 4 workers and store them in this list ###\n",
    "        # (Normally, we would limit based on number of cores on the computer for parallelization)\n",
    "        workers = [ AsyncWorkerAgent(i, self.env_name, self.num_episodes, self.global_model, self.optimizer, self.result_queue, self.save_dir) \n",
    "                           for i in range(4)] \n",
    "#         workers = []\n",
    "        ### TODO ###\n",
    "        for i, worker in enumerate(workers):\n",
    "            print(f\"Starting worker {i}\")\n",
    "            worker.start()\n",
    "            \n",
    "        # Wait for all workers to finish\n",
    "        [w.join() for w in workers]\n",
    "        \n",
    "        ### TODO: Plot a graph of rewards from self.result_queue over time ### \n",
    "        plt.plot(list(self.result_queue.queue))\n",
    "        plt.ylabel('Moving average rewards')\n",
    "        plt.xlabel('Step')\n",
    "        plt.savefig(os.path.join(self.save_dir,\n",
    "                                 '{} Moving Average.png'.format(self.env_name)))\n",
    "        plt.show()\n",
    "#         pass\n",
    "        ###\n",
    "    \n",
    "    def play(self, render=False, model_path=\"\"):\n",
    "        # If desired, load model from file\n",
    "        if model_path != \"\":\n",
    "            model_path = os.path.join(self.save_dir, model_path)\n",
    "            print(f'Loading model from: {model_path}')\n",
    "            self.global_model.load_weights(model_path)\n",
    "\n",
    "        # Reset environment\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps_taken = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            ### TODO: Extract the logits and value from the global model ###\n",
    "            logits, value = self.global_model(tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
    "            policy = tf.nn.softmax(logits)\n",
    "\n",
    "#             logits, value = None, None\n",
    "#             policy = None\n",
    "            ### TODO ###\n",
    "            \n",
    "            ### TODO: Identify optimal action from policy ###\n",
    "            action = np.argmax(policy)\n",
    "#             action = None\n",
    "            ### TODO ###\n",
    "            \n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "        \n",
    "            if render:\n",
    "                self.env.render(mode='rgb_array') # Only render if requested\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps_taken += 1\n",
    "\n",
    "            print(f\"Step {steps_taken}: Total Reward: {total_reward}, Action: {action}\")\n",
    "        \n",
    "        self.env.close()\n",
    "        return total_reward, steps_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! Time to train the agent with 1000 episodes. **This will take a while.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting worker 0\n",
      "Starting worker 1\n",
      "Starting worker 2\n",
      "Starting worker 3\n",
      "Episode: 0 | Moving Average Reward: 19 | Episode Reward: 19 | Loss per step: 3.281 | Steps: 19 | Worker: 1\n",
      "Saving best model to /tmp/model_CartPole-v0_19.0.h5, episode score: 19.0\n",
      "Episode: 0 | Moving Average Reward: 19 | Episode Reward: 14 | Loss per step: 2.755 | Steps: 14 | Worker: 2\n",
      "Saving best model to /tmp/model_CartPole-v0_14.0.h5, episode score: 14.0\n",
      "Episode: 0 | Moving Average Reward: 19 | Episode Reward: 21 | Loss per step: 3.333 | Steps: 21 | Worker: 3\n",
      "Saving best model to /tmp/model_CartPole-v0_21.0.h5, episode score: 21.0\n",
      "Episode: 0 | Moving Average Reward: 19 | Episode Reward: 22 | Loss per step: 3.206 | Steps: 22 | Worker: 0\n",
      "Saving best model to /tmp/model_CartPole-v0_22.0.h5, episode score: 22.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = A3CAgent(\"CartPole-v0\", 1)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe that the agent converges to a solution near the maximum reward of 200. (If not, something is probably wrong with your code!)\n",
    "\n",
    "Let's see what that looks like in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /tmp/model_CartPole-v0_200.0.h5\n",
      "Step 1: Total Reward: 1.0, Action: 0\n",
      "Step 2: Total Reward: 2.0, Action: 1\n",
      "Step 3: Total Reward: 3.0, Action: 0\n",
      "Step 4: Total Reward: 4.0, Action: 1\n",
      "Step 5: Total Reward: 5.0, Action: 0\n",
      "Step 6: Total Reward: 6.0, Action: 1\n",
      "Step 7: Total Reward: 7.0, Action: 0\n",
      "Step 8: Total Reward: 8.0, Action: 1\n",
      "Step 9: Total Reward: 9.0, Action: 0\n",
      "Step 10: Total Reward: 10.0, Action: 1\n",
      "Step 11: Total Reward: 11.0, Action: 0\n",
      "Step 12: Total Reward: 12.0, Action: 1\n",
      "Step 13: Total Reward: 13.0, Action: 1\n",
      "Step 14: Total Reward: 14.0, Action: 0\n",
      "Step 15: Total Reward: 15.0, Action: 1\n",
      "Step 16: Total Reward: 16.0, Action: 0\n",
      "Step 17: Total Reward: 17.0, Action: 1\n",
      "Step 18: Total Reward: 18.0, Action: 0\n",
      "Step 19: Total Reward: 19.0, Action: 1\n",
      "Step 20: Total Reward: 20.0, Action: 0\n",
      "Step 21: Total Reward: 21.0, Action: 1\n",
      "Step 22: Total Reward: 22.0, Action: 0\n",
      "Step 23: Total Reward: 23.0, Action: 0\n",
      "Step 24: Total Reward: 24.0, Action: 1\n",
      "Step 25: Total Reward: 25.0, Action: 0\n",
      "Step 26: Total Reward: 26.0, Action: 1\n",
      "Step 27: Total Reward: 27.0, Action: 0\n",
      "Step 28: Total Reward: 28.0, Action: 1\n",
      "Step 29: Total Reward: 29.0, Action: 0\n",
      "Step 30: Total Reward: 30.0, Action: 1\n",
      "Step 31: Total Reward: 31.0, Action: 0\n",
      "Step 32: Total Reward: 32.0, Action: 1\n",
      "Step 33: Total Reward: 33.0, Action: 1\n",
      "Step 34: Total Reward: 34.0, Action: 0\n",
      "Step 35: Total Reward: 35.0, Action: 1\n",
      "Step 36: Total Reward: 36.0, Action: 0\n",
      "Step 37: Total Reward: 37.0, Action: 1\n",
      "Step 38: Total Reward: 38.0, Action: 0\n",
      "Step 39: Total Reward: 39.0, Action: 1\n",
      "Step 40: Total Reward: 40.0, Action: 0\n",
      "Step 41: Total Reward: 41.0, Action: 0\n",
      "Step 42: Total Reward: 42.0, Action: 1\n",
      "Step 43: Total Reward: 43.0, Action: 0\n",
      "Step 44: Total Reward: 44.0, Action: 1\n",
      "Step 45: Total Reward: 45.0, Action: 0\n",
      "Step 46: Total Reward: 46.0, Action: 1\n",
      "Step 47: Total Reward: 47.0, Action: 0\n",
      "Step 48: Total Reward: 48.0, Action: 1\n",
      "Step 49: Total Reward: 49.0, Action: 0\n",
      "Step 50: Total Reward: 50.0, Action: 1\n",
      "Step 51: Total Reward: 51.0, Action: 0\n",
      "Step 52: Total Reward: 52.0, Action: 1\n",
      "Step 53: Total Reward: 53.0, Action: 1\n",
      "Step 54: Total Reward: 54.0, Action: 0\n",
      "Step 55: Total Reward: 55.0, Action: 1\n",
      "Step 56: Total Reward: 56.0, Action: 0\n",
      "Step 57: Total Reward: 57.0, Action: 1\n",
      "Step 58: Total Reward: 58.0, Action: 0\n",
      "Step 59: Total Reward: 59.0, Action: 1\n",
      "Step 60: Total Reward: 60.0, Action: 0\n",
      "Step 61: Total Reward: 61.0, Action: 0\n",
      "Step 62: Total Reward: 62.0, Action: 1\n",
      "Step 63: Total Reward: 63.0, Action: 0\n",
      "Step 64: Total Reward: 64.0, Action: 1\n",
      "Step 65: Total Reward: 65.0, Action: 0\n",
      "Step 66: Total Reward: 66.0, Action: 1\n",
      "Step 67: Total Reward: 67.0, Action: 1\n",
      "Step 68: Total Reward: 68.0, Action: 0\n",
      "Step 69: Total Reward: 69.0, Action: 1\n",
      "Step 70: Total Reward: 70.0, Action: 0\n",
      "Step 71: Total Reward: 71.0, Action: 0\n",
      "Step 72: Total Reward: 72.0, Action: 1\n",
      "Step 73: Total Reward: 73.0, Action: 0\n",
      "Step 74: Total Reward: 74.0, Action: 1\n",
      "Step 75: Total Reward: 75.0, Action: 0\n",
      "Step 76: Total Reward: 76.0, Action: 1\n",
      "Step 77: Total Reward: 77.0, Action: 1\n",
      "Step 78: Total Reward: 78.0, Action: 0\n",
      "Step 79: Total Reward: 79.0, Action: 0\n",
      "Step 80: Total Reward: 80.0, Action: 1\n",
      "Step 81: Total Reward: 81.0, Action: 1\n",
      "Step 82: Total Reward: 82.0, Action: 0\n",
      "Step 83: Total Reward: 83.0, Action: 1\n",
      "Step 84: Total Reward: 84.0, Action: 0\n",
      "Step 85: Total Reward: 85.0, Action: 0\n",
      "Step 86: Total Reward: 86.0, Action: 1\n",
      "Step 87: Total Reward: 87.0, Action: 0\n",
      "Step 88: Total Reward: 88.0, Action: 1\n",
      "Step 89: Total Reward: 89.0, Action: 0\n",
      "Step 90: Total Reward: 90.0, Action: 1\n",
      "Step 91: Total Reward: 91.0, Action: 1\n",
      "Step 92: Total Reward: 92.0, Action: 0\n",
      "Step 93: Total Reward: 93.0, Action: 1\n",
      "Step 94: Total Reward: 94.0, Action: 0\n",
      "Step 95: Total Reward: 95.0, Action: 0\n",
      "Step 96: Total Reward: 96.0, Action: 1\n",
      "Step 97: Total Reward: 97.0, Action: 0\n",
      "Step 98: Total Reward: 98.0, Action: 1\n",
      "Step 99: Total Reward: 99.0, Action: 1\n",
      "Step 100: Total Reward: 100.0, Action: 0\n",
      "Step 101: Total Reward: 101.0, Action: 0\n",
      "Step 102: Total Reward: 102.0, Action: 1\n",
      "Step 103: Total Reward: 103.0, Action: 1\n",
      "Step 104: Total Reward: 104.0, Action: 0\n",
      "Step 105: Total Reward: 105.0, Action: 0\n",
      "Step 106: Total Reward: 106.0, Action: 1\n",
      "Step 107: Total Reward: 107.0, Action: 0\n",
      "Step 108: Total Reward: 108.0, Action: 1\n",
      "Step 109: Total Reward: 109.0, Action: 1\n",
      "Step 110: Total Reward: 110.0, Action: 0\n",
      "Step 111: Total Reward: 111.0, Action: 1\n",
      "Step 112: Total Reward: 112.0, Action: 0\n",
      "Step 113: Total Reward: 113.0, Action: 0\n",
      "Step 114: Total Reward: 114.0, Action: 1\n",
      "Step 115: Total Reward: 115.0, Action: 0\n",
      "Step 116: Total Reward: 116.0, Action: 1\n",
      "Step 117: Total Reward: 117.0, Action: 1\n",
      "Step 118: Total Reward: 118.0, Action: 0\n",
      "Step 119: Total Reward: 119.0, Action: 0\n",
      "Step 120: Total Reward: 120.0, Action: 1\n",
      "Step 121: Total Reward: 121.0, Action: 1\n",
      "Step 122: Total Reward: 122.0, Action: 0\n",
      "Step 123: Total Reward: 123.0, Action: 0\n",
      "Step 124: Total Reward: 124.0, Action: 1\n",
      "Step 125: Total Reward: 125.0, Action: 1\n",
      "Step 126: Total Reward: 126.0, Action: 0\n",
      "Step 127: Total Reward: 127.0, Action: 0\n",
      "Step 128: Total Reward: 128.0, Action: 1\n",
      "Step 129: Total Reward: 129.0, Action: 0\n",
      "Step 130: Total Reward: 130.0, Action: 1\n",
      "Step 131: Total Reward: 131.0, Action: 1\n",
      "Step 132: Total Reward: 132.0, Action: 0\n",
      "Step 133: Total Reward: 133.0, Action: 0\n",
      "Step 134: Total Reward: 134.0, Action: 1\n",
      "Step 135: Total Reward: 135.0, Action: 1\n",
      "Step 136: Total Reward: 136.0, Action: 0\n",
      "Step 137: Total Reward: 137.0, Action: 0\n",
      "Step 138: Total Reward: 138.0, Action: 1\n",
      "Step 139: Total Reward: 139.0, Action: 0\n",
      "Step 140: Total Reward: 140.0, Action: 1\n",
      "Step 141: Total Reward: 141.0, Action: 1\n",
      "Step 142: Total Reward: 142.0, Action: 0\n",
      "Step 143: Total Reward: 143.0, Action: 1\n",
      "Step 144: Total Reward: 144.0, Action: 0\n",
      "Step 145: Total Reward: 145.0, Action: 0\n",
      "Step 146: Total Reward: 146.0, Action: 1\n",
      "Step 147: Total Reward: 147.0, Action: 0\n",
      "Step 148: Total Reward: 148.0, Action: 1\n",
      "Step 149: Total Reward: 149.0, Action: 1\n",
      "Step 150: Total Reward: 150.0, Action: 0\n",
      "Step 151: Total Reward: 151.0, Action: 1\n",
      "Step 152: Total Reward: 152.0, Action: 0\n",
      "Step 153: Total Reward: 153.0, Action: 0\n",
      "Step 154: Total Reward: 154.0, Action: 1\n",
      "Step 155: Total Reward: 155.0, Action: 0\n",
      "Step 156: Total Reward: 156.0, Action: 1\n",
      "Step 157: Total Reward: 157.0, Action: 0\n",
      "Step 158: Total Reward: 158.0, Action: 1\n",
      "Step 159: Total Reward: 159.0, Action: 0\n",
      "Step 160: Total Reward: 160.0, Action: 1\n",
      "Step 161: Total Reward: 161.0, Action: 1\n",
      "Step 162: Total Reward: 162.0, Action: 0\n",
      "Step 163: Total Reward: 163.0, Action: 1\n",
      "Step 164: Total Reward: 164.0, Action: 0\n",
      "Step 165: Total Reward: 165.0, Action: 1\n",
      "Step 166: Total Reward: 166.0, Action: 0\n",
      "Step 167: Total Reward: 167.0, Action: 1\n",
      "Step 168: Total Reward: 168.0, Action: 0\n",
      "Step 169: Total Reward: 169.0, Action: 0\n",
      "Step 170: Total Reward: 170.0, Action: 1\n",
      "Step 171: Total Reward: 171.0, Action: 0\n",
      "Step 172: Total Reward: 172.0, Action: 1\n",
      "Step 173: Total Reward: 173.0, Action: 0\n",
      "Step 174: Total Reward: 174.0, Action: 1\n",
      "Step 175: Total Reward: 175.0, Action: 0\n",
      "Step 176: Total Reward: 176.0, Action: 1\n",
      "Step 177: Total Reward: 177.0, Action: 0\n",
      "Step 178: Total Reward: 178.0, Action: 1\n",
      "Step 179: Total Reward: 179.0, Action: 0\n",
      "Step 180: Total Reward: 180.0, Action: 1\n",
      "Step 181: Total Reward: 181.0, Action: 1\n",
      "Step 182: Total Reward: 182.0, Action: 0\n",
      "Step 183: Total Reward: 183.0, Action: 1\n",
      "Step 184: Total Reward: 184.0, Action: 0\n",
      "Step 185: Total Reward: 185.0, Action: 1\n",
      "Step 186: Total Reward: 186.0, Action: 0\n",
      "Step 187: Total Reward: 187.0, Action: 1\n",
      "Step 188: Total Reward: 188.0, Action: 0\n",
      "Step 189: Total Reward: 189.0, Action: 0\n",
      "Step 190: Total Reward: 190.0, Action: 1\n",
      "Step 191: Total Reward: 191.0, Action: 0\n",
      "Step 192: Total Reward: 192.0, Action: 1\n",
      "Step 193: Total Reward: 193.0, Action: 0\n",
      "Step 194: Total Reward: 194.0, Action: 1\n",
      "Step 195: Total Reward: 195.0, Action: 0\n",
      "Step 196: Total Reward: 196.0, Action: 1\n",
      "Step 197: Total Reward: 197.0, Action: 1\n",
      "Step 198: Total Reward: 198.0, Action: 0\n",
      "Step 199: Total Reward: 199.0, Action: 1\n",
      "Step 200: Total Reward: 200.0, Action: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200.0, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In case you need to re-run this cell because of a notebook crash, you can load training weights from a file\n",
    "# Either scroll through the training output or scan through the save directory for the weights with best performance\n",
    "# agent.play(render=True, model_path=\"/tmp/model_CartPole-v0_200.0.h5\")\n",
    "\n",
    "agent.play(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! If your model looks good qualitatively, you are done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
