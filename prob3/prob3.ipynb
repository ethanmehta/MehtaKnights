{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving CartPole-v0 Efficiently with A3C\n",
    "\n",
    "In this problem, we will return to CartPole-v0, but this time we will employ a different DeepRL strategy known as Asynchronous Advantage Actor-Critic (A3C). A3C tends to offer significantly better results than VPG, as you shall soon see!    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /Users/jsboygenius/.local/lib/python3.8/site-packages (0.17.3)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /Users/jsboygenius/.local/lib/python3.8/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/jsboygenius/.local/lib/python3.8/site-packages (from gym) (1.18.5)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /Users/jsboygenius/anaconda3/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: scipy in /Users/jsboygenius/anaconda3/lib/python3.8/site-packages (from gym) (1.5.2)\n",
      "Requirement already satisfied: future in /Users/jsboygenius/anaconda3/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "# We have tested these imports on several machines, \n",
    "# but depending on your computer, you may need to \n",
    "# run some of these imports in your local environment.\n",
    "\n",
    "# For file saving operations\n",
    "import os\n",
    "\n",
    "# For multiple worker threads\n",
    "from threading import Thread, Lock\n",
    "import multiprocessing\n",
    "from queue import Queue\n",
    "\n",
    "# For math/plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For testing in simulated environments\n",
    "!pip install --user gym\n",
    "import gym\n",
    "\n",
    "# For training the Actor-Critic model\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the custom `ActorCriticModel`, which inherits from `keras.Model`.\n",
    "\n",
    "You will need to initialize the layers for the neural net as follows:\n",
    "- Dense layer of size 100, with ReLU activation\n",
    "- Logits output layer\n",
    "- Dense layer of size 100, with ReLU activation\n",
    "- Action output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticModel(keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Init from keras.Model\n",
    "        super(ActorCriticModel, self).__init__()\n",
    "        \n",
    "        # Constants for available state space and action space sizes\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        ### TODO: Initialize layers for the neural net ###        \n",
    "        self.dense1 = None\n",
    "        self.policy_logits = None\n",
    "        self.dense2 = None\n",
    "        self.values = None        \n",
    "        ### TODO ###\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Overloaded from keras.Model\n",
    "        \n",
    "        ### TODO: compute forward pass through neural net ###\n",
    "        logits = None\n",
    "        values = None\n",
    "        ### TODO ###\n",
    "        return logits, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have provided you with a `BaseAgent` class from which our DeepRL agents will inherit. Note that the subclassed agent will specify a `train` method (for training the agent) and a `play` method (for playing back a single episode) - you will end up writing parts of these methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    \"\"\"\n",
    "    Base class from which all custom agents will inherit.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env_name, num_episodes):\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.num_episodes = num_episodes\n",
    "        \n",
    "        self.result_queue = Queue()\n",
    "        \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def play(self, render=False):\n",
    "        pass\n",
    "    \n",
    "    def record_episode(self, episode_num, episode_reward, episode_loss, episode_steps, average_reward = 0, worker_idx = None):\n",
    "        '''\n",
    "        Helper function to log output about completed episode, while also updating the result queue\n",
    "        '''\n",
    "        \n",
    "        average_reward = average_reward * 0.99 + episode_reward * 0.01 if average_reward > 0 else episode_reward\n",
    "        \n",
    "        self.result_queue.put(average_reward)\n",
    "        \n",
    "        print(\n",
    "          f\"Episode: {episode_num} | \"\n",
    "          f\"Moving Average Reward: {round(average_reward)} | \"\n",
    "          f\"Episode Reward: {round(episode_reward)} | \"\n",
    "          f\"Loss per step: {round(float(episode_loss) / episode_steps, 3)} | \"\n",
    "          f\"Steps: {episode_steps} | \"\n",
    "          f\"Worker: {worker_idx}\"\n",
    "        )\n",
    "        return average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: RandomAgent Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will implement a `RandomAgent` so that we have a baseline performance. This will be very similar to what you have seen on earlier problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent that chooses actions randomly\n",
    "    \"\"\"\n",
    "    def play(self, render=False):\n",
    "        # Reset environment\n",
    "        self.env.reset()\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps_taken = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            ### TODO: Choose and take a random action from the environment's action space ###\n",
    "\n",
    "            ### TODO ###\n",
    "            \n",
    "            if render:\n",
    "                self.env.render() # Only render if requested\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps_taken += 1\n",
    "        \n",
    "        self.env.close()        \n",
    "        return total_reward, steps_taken\n",
    "    \n",
    "    def train(self):\n",
    "        # Since this is a random agent, we won't actually do any real training.\n",
    "        # Instead, we'll simply run through the episodes using play()\n",
    "        weighted_average_reward = 0\n",
    "        total_reward = 0\n",
    "        for episode in range(self.num_episodes):\n",
    "            reward, steps = self.play()\n",
    "            \n",
    "            weighted_average_reward = self.record_episode(episode, reward, 0, steps, weighted_average_reward)\n",
    "            \n",
    "            total_reward += reward \n",
    "\n",
    "        simple_average_reward = total_reward / self.num_episodes\n",
    "        print(f\"After {self.num_episodes}, simple average reward is {simple_average_reward} and weighted average reward is {weighted_average_reward}\")\n",
    "        return weighted_average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've implemented the RandomAgent, let's see what its baseline performance looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 | Moving Average Reward: 15 | Episode Reward: 15 | Loss per step: 0.0 | Steps: 15 | Worker: None\n",
      "Episode: 1 | Moving Average Reward: 15 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 2 | Moving Average Reward: 15 | Episode Reward: 15 | Loss per step: 0.0 | Steps: 15 | Worker: None\n",
      "Episode: 3 | Moving Average Reward: 15 | Episode Reward: 17 | Loss per step: 0.0 | Steps: 17 | Worker: None\n",
      "Episode: 4 | Moving Average Reward: 15 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 5 | Moving Average Reward: 15 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 6 | Moving Average Reward: 15 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 7 | Moving Average Reward: 15 | Episode Reward: 26 | Loss per step: 0.0 | Steps: 26 | Worker: None\n",
      "Episode: 8 | Moving Average Reward: 15 | Episode Reward: 19 | Loss per step: 0.0 | Steps: 19 | Worker: None\n",
      "Episode: 9 | Moving Average Reward: 15 | Episode Reward: 38 | Loss per step: 0.0 | Steps: 38 | Worker: None\n",
      "Episode: 10 | Moving Average Reward: 15 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 11 | Moving Average Reward: 15 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 12 | Moving Average Reward: 15 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 13 | Moving Average Reward: 15 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 14 | Moving Average Reward: 15 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 15 | Moving Average Reward: 15 | Episode Reward: 25 | Loss per step: 0.0 | Steps: 25 | Worker: None\n",
      "Episode: 16 | Moving Average Reward: 16 | Episode Reward: 26 | Loss per step: 0.0 | Steps: 26 | Worker: None\n",
      "Episode: 17 | Moving Average Reward: 16 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 18 | Moving Average Reward: 16 | Episode Reward: 21 | Loss per step: 0.0 | Steps: 21 | Worker: None\n",
      "Episode: 19 | Moving Average Reward: 16 | Episode Reward: 29 | Loss per step: 0.0 | Steps: 29 | Worker: None\n",
      "Episode: 20 | Moving Average Reward: 16 | Episode Reward: 24 | Loss per step: 0.0 | Steps: 24 | Worker: None\n",
      "Episode: 21 | Moving Average Reward: 16 | Episode Reward: 26 | Loss per step: 0.0 | Steps: 26 | Worker: None\n",
      "Episode: 22 | Moving Average Reward: 16 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 23 | Moving Average Reward: 16 | Episode Reward: 27 | Loss per step: 0.0 | Steps: 27 | Worker: None\n",
      "Episode: 24 | Moving Average Reward: 16 | Episode Reward: 17 | Loss per step: 0.0 | Steps: 17 | Worker: None\n",
      "Episode: 25 | Moving Average Reward: 16 | Episode Reward: 15 | Loss per step: 0.0 | Steps: 15 | Worker: None\n",
      "Episode: 26 | Moving Average Reward: 16 | Episode Reward: 18 | Loss per step: 0.0 | Steps: 18 | Worker: None\n",
      "Episode: 27 | Moving Average Reward: 17 | Episode Reward: 76 | Loss per step: 0.0 | Steps: 76 | Worker: None\n",
      "Episode: 28 | Moving Average Reward: 17 | Episode Reward: 19 | Loss per step: 0.0 | Steps: 19 | Worker: None\n",
      "Episode: 29 | Moving Average Reward: 17 | Episode Reward: 25 | Loss per step: 0.0 | Steps: 25 | Worker: None\n",
      "Episode: 30 | Moving Average Reward: 17 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 31 | Moving Average Reward: 17 | Episode Reward: 31 | Loss per step: 0.0 | Steps: 31 | Worker: None\n",
      "Episode: 32 | Moving Average Reward: 17 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 33 | Moving Average Reward: 17 | Episode Reward: 24 | Loss per step: 0.0 | Steps: 24 | Worker: None\n",
      "Episode: 34 | Moving Average Reward: 17 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 35 | Moving Average Reward: 17 | Episode Reward: 21 | Loss per step: 0.0 | Steps: 21 | Worker: None\n",
      "Episode: 36 | Moving Average Reward: 17 | Episode Reward: 15 | Loss per step: 0.0 | Steps: 15 | Worker: None\n",
      "Episode: 37 | Moving Average Reward: 17 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 38 | Moving Average Reward: 17 | Episode Reward: 22 | Loss per step: 0.0 | Steps: 22 | Worker: None\n",
      "Episode: 39 | Moving Average Reward: 17 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 40 | Moving Average Reward: 17 | Episode Reward: 48 | Loss per step: 0.0 | Steps: 48 | Worker: None\n",
      "Episode: 41 | Moving Average Reward: 17 | Episode Reward: 30 | Loss per step: 0.0 | Steps: 30 | Worker: None\n",
      "Episode: 42 | Moving Average Reward: 17 | Episode Reward: 9 | Loss per step: 0.0 | Steps: 9 | Worker: None\n",
      "Episode: 43 | Moving Average Reward: 17 | Episode Reward: 15 | Loss per step: 0.0 | Steps: 15 | Worker: None\n",
      "Episode: 44 | Moving Average Reward: 17 | Episode Reward: 15 | Loss per step: 0.0 | Steps: 15 | Worker: None\n",
      "Episode: 45 | Moving Average Reward: 17 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 46 | Moving Average Reward: 17 | Episode Reward: 18 | Loss per step: 0.0 | Steps: 18 | Worker: None\n",
      "Episode: 47 | Moving Average Reward: 17 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 48 | Moving Average Reward: 17 | Episode Reward: 40 | Loss per step: 0.0 | Steps: 40 | Worker: None\n",
      "Episode: 49 | Moving Average Reward: 17 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 50 | Moving Average Reward: 18 | Episode Reward: 46 | Loss per step: 0.0 | Steps: 46 | Worker: None\n",
      "Episode: 51 | Moving Average Reward: 18 | Episode Reward: 8 | Loss per step: 0.0 | Steps: 8 | Worker: None\n",
      "Episode: 52 | Moving Average Reward: 18 | Episode Reward: 19 | Loss per step: 0.0 | Steps: 19 | Worker: None\n",
      "Episode: 53 | Moving Average Reward: 17 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 54 | Moving Average Reward: 17 | Episode Reward: 15 | Loss per step: 0.0 | Steps: 15 | Worker: None\n",
      "Episode: 55 | Moving Average Reward: 18 | Episode Reward: 22 | Loss per step: 0.0 | Steps: 22 | Worker: None\n",
      "Episode: 56 | Moving Average Reward: 17 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 57 | Moving Average Reward: 17 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 58 | Moving Average Reward: 17 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 59 | Moving Average Reward: 17 | Episode Reward: 29 | Loss per step: 0.0 | Steps: 29 | Worker: None\n",
      "Episode: 60 | Moving Average Reward: 17 | Episode Reward: 21 | Loss per step: 0.0 | Steps: 21 | Worker: None\n",
      "Episode: 61 | Moving Average Reward: 18 | Episode Reward: 22 | Loss per step: 0.0 | Steps: 22 | Worker: None\n",
      "Episode: 62 | Moving Average Reward: 17 | Episode Reward: 10 | Loss per step: 0.0 | Steps: 10 | Worker: None\n",
      "Episode: 63 | Moving Average Reward: 17 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 64 | Moving Average Reward: 17 | Episode Reward: 10 | Loss per step: 0.0 | Steps: 10 | Worker: None\n",
      "Episode: 65 | Moving Average Reward: 17 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 66 | Moving Average Reward: 17 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 67 | Moving Average Reward: 17 | Episode Reward: 29 | Loss per step: 0.0 | Steps: 29 | Worker: None\n",
      "Episode: 68 | Moving Average Reward: 17 | Episode Reward: 26 | Loss per step: 0.0 | Steps: 26 | Worker: None\n",
      "Episode: 69 | Moving Average Reward: 17 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 70 | Moving Average Reward: 17 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 71 | Moving Average Reward: 17 | Episode Reward: 25 | Loss per step: 0.0 | Steps: 25 | Worker: None\n",
      "Episode: 72 | Moving Average Reward: 18 | Episode Reward: 22 | Loss per step: 0.0 | Steps: 22 | Worker: None\n",
      "Episode: 73 | Moving Average Reward: 18 | Episode Reward: 49 | Loss per step: 0.0 | Steps: 49 | Worker: None\n",
      "Episode: 74 | Moving Average Reward: 18 | Episode Reward: 21 | Loss per step: 0.0 | Steps: 21 | Worker: None\n",
      "Episode: 75 | Moving Average Reward: 18 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 76 | Moving Average Reward: 18 | Episode Reward: 11 | Loss per step: 0.0 | Steps: 11 | Worker: None\n",
      "Episode: 77 | Moving Average Reward: 18 | Episode Reward: 49 | Loss per step: 0.0 | Steps: 49 | Worker: None\n",
      "Episode: 78 | Moving Average Reward: 18 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 79 | Moving Average Reward: 18 | Episode Reward: 36 | Loss per step: 0.0 | Steps: 36 | Worker: None\n",
      "Episode: 80 | Moving Average Reward: 18 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 81 | Moving Average Reward: 18 | Episode Reward: 10 | Loss per step: 0.0 | Steps: 10 | Worker: None\n",
      "Episode: 82 | Moving Average Reward: 18 | Episode Reward: 17 | Loss per step: 0.0 | Steps: 17 | Worker: None\n",
      "Episode: 83 | Moving Average Reward: 18 | Episode Reward: 12 | Loss per step: 0.0 | Steps: 12 | Worker: None\n",
      "Episode: 84 | Moving Average Reward: 18 | Episode Reward: 9 | Loss per step: 0.0 | Steps: 9 | Worker: None\n",
      "Episode: 85 | Moving Average Reward: 18 | Episode Reward: 27 | Loss per step: 0.0 | Steps: 27 | Worker: None\n",
      "Episode: 86 | Moving Average Reward: 18 | Episode Reward: 18 | Loss per step: 0.0 | Steps: 18 | Worker: None\n",
      "Episode: 87 | Moving Average Reward: 18 | Episode Reward: 19 | Loss per step: 0.0 | Steps: 19 | Worker: None\n",
      "Episode: 88 | Moving Average Reward: 18 | Episode Reward: 21 | Loss per step: 0.0 | Steps: 21 | Worker: None\n",
      "Episode: 89 | Moving Average Reward: 18 | Episode Reward: 25 | Loss per step: 0.0 | Steps: 25 | Worker: None\n",
      "Episode: 90 | Moving Average Reward: 18 | Episode Reward: 30 | Loss per step: 0.0 | Steps: 30 | Worker: None\n",
      "Episode: 91 | Moving Average Reward: 18 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 92 | Moving Average Reward: 18 | Episode Reward: 14 | Loss per step: 0.0 | Steps: 14 | Worker: None\n",
      "Episode: 93 | Moving Average Reward: 18 | Episode Reward: 31 | Loss per step: 0.0 | Steps: 31 | Worker: None\n",
      "Episode: 94 | Moving Average Reward: 18 | Episode Reward: 13 | Loss per step: 0.0 | Steps: 13 | Worker: None\n",
      "Episode: 95 | Moving Average Reward: 18 | Episode Reward: 20 | Loss per step: 0.0 | Steps: 20 | Worker: None\n",
      "Episode: 96 | Moving Average Reward: 18 | Episode Reward: 29 | Loss per step: 0.0 | Steps: 29 | Worker: None\n",
      "Episode: 97 | Moving Average Reward: 19 | Episode Reward: 27 | Loss per step: 0.0 | Steps: 27 | Worker: None\n",
      "Episode: 98 | Moving Average Reward: 19 | Episode Reward: 16 | Loss per step: 0.0 | Steps: 16 | Worker: None\n",
      "Episode: 99 | Moving Average Reward: 18 | Episode Reward: 9 | Loss per step: 0.0 | Steps: 9 | Worker: None\n",
      "After 100, simple average reward is 20.41 and weighted average reward is 18.424775332267508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24.0, 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_agent = RandomAgent(\"CartPole-v0\", 100)\n",
    "random_agent.train()\n",
    "\n",
    "random_agent.play(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw, the episode ends almost instantly, because the random policy is not very good! With an average reward of only about 20, we are very far away from the required 195 reward to meet OpenAI's criteria for solving `CartPole-v0`.\n",
    "\n",
    "Let's see how we can use the A3C approach to get significantly better results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: A3C Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's review what the A3C algorithm is. \n",
    "\n",
    "A3C stands for **Asynchronous Advantage Actor-Critic**. Let's break it down:\n",
    "1. **Aysnchronous:** One key advantage of A3C is the comparative speed at which is can be trained compared to other DeepRL methods like DQN. The secret to this speed is a parallelized approach. In A3C, several worker agents are working in parallel on their own individual, local copies of the environment. After several steps, the worker agents update their local models, and then compare their local models to the single global model. If any individual worker has made an improvement to the global model, the global model is updated and all the asynchronous workers synchronize their local models to the new global model. This approach means we can benefit significantly from the speedups in parallel computation. You will learn much more about parallelization in CS 61C!\n",
    "\n",
    "2. **Advantage:** As you have learned, A3C focuses on estimating the Advantages of actions, exploring parts of the action space that were surprisingly better than expected. Advantage is defined as the difference between a better Q-state and the Value of a state per an existing policy. That is: $$A(s, a) = Q(s, a) - V^\\pi(s)$$ Since this math is outside the scope of this project, we will implement the Advantage calculation for you.\n",
    "\n",
    "3. **Actor-Critic:** The Actor-Critic model is a typical setup for DeepRL, in which an Actor tries to determine the best action from a given state, and in which a Critic evaluates the chosen action for long-term reward. Usually, the Critic is estimating the Value of the state achieved after taking the action. However, in A3C, the Critic instead estimates the Advantage of the state-action pair from the Actor. The interplay between Actor and Critic helps the system converge faster and achieve better results than either system alone.\n",
    "\n",
    "Now that you have a high-level understanding of what A3C is, let's get into the details and start implementing it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create a class representing the Asynchronous Workers that do the heavy lifting in the A3C approach. Each asynchronous worker maintains a local copy of the model and its own environment, and then periodically checks and synchronizes with the global model. \n",
    "\n",
    "Since this may be your first exposure to multithreaded programming, we have provided most of that structure for you. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncWorkerAgent(Thread, BaseAgent):\n",
    "    \"\"\"\n",
    "    Aysnchronous worker that trains on an individual instance of the environment\n",
    "    \"\"\"\n",
    "\n",
    "    # Global variables across threads\n",
    "    mutex = Lock()\n",
    "    global_episode_idx = 0\n",
    "    global_moving_average_reward = 0\n",
    "    best_reward = 0\n",
    "    \n",
    "    def __init__(self, worker_idx, env_name, num_episodes, global_model, optimizer, global_result_queue, save_dir, update_frequency=20, gamma=0.99):\n",
    "        Thread.__init__(self) \n",
    "        BaseAgent.__init__(self, env_name, num_episodes)\n",
    "        \n",
    "        self.worker_idx = worker_idx\n",
    "        \n",
    "        self.global_model = global_model\n",
    "        self.optimizer = optimizer\n",
    "        self.result_queue = global_result_queue\n",
    "        self.save_dir = save_dir\n",
    "        self.update_frequency = update_frequency \n",
    "        self.gamma = gamma # The discount factor for rewards\n",
    "\n",
    "        ### TODO: Get the size of the state and action spaces from self.env ###        \n",
    "        self.state_size = None\n",
    "        self.action_size = None\n",
    "        ### TODO ###\n",
    "    \n",
    "        self.local_model = ActorCriticModel(self.state_size, self.action_size)\n",
    "        \n",
    "        # Reset global variables\n",
    "        AsyncWorkerAgent.global_episode_idx = 0\n",
    "        AsyncWorkerAgent.global_moving_average_reward = 0\n",
    "        AsyncWorkerAgent.best_reward = 0\n",
    "        \n",
    "    def run(self):\n",
    "        # Overloaded from Thread\n",
    "        self.train()\n",
    "        \n",
    "    def train(self):\n",
    "        # Even though we have multiple workers, the total number of episodes run is still num_episodes\n",
    "        while AsyncWorkerAgent.global_episode_idx < self.num_episodes:\n",
    "            episode_idx = AsyncWorkerAgent.global_episode_idx\n",
    "            current_state = self.env.reset()\n",
    "        \n",
    "            total_reward = 0\n",
    "            total_loss = 0.0\n",
    "            steps_taken = 0\n",
    "            \n",
    "            # Maintain a buffer of the most recent states, actions, and rewards to train the Actor-Critic from\n",
    "            buffer_states = []\n",
    "            buffer_actions = []\n",
    "            buffer_rewards = []\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                ### TODO: Choose action according to probabilities from neural net ###\n",
    "                # Note that the action space is discrete, so a valid action is an integer between [0, size of space]\n",
    "                action = None\n",
    "                ### TODO ###\n",
    "\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                                \n",
    "                buffer_states.append(current_state)\n",
    "                buffer_actions.append(action)\n",
    "                buffer_rewards.append(reward)\n",
    "\n",
    "                total_reward += reward\n",
    "                steps_taken += 1\n",
    "\n",
    "                # Periodically compute gradients to update model\n",
    "                if steps_taken % self.update_frequency == 0 or done:\n",
    "                    # We use TensorFlow's GradientTape functionality to track the gradient of our custom loss function\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        ### TODO: Use the compute_loss() helper function to obtain the loss ###                        \n",
    "                        loss = 0\n",
    "                        ### TODO ###\n",
    "            \n",
    "                    buffer_states, buffer_actions, buffer_rewards = [], [], []\n",
    "                    total_loss += loss\n",
    "\n",
    "                    # Calculate local gradients\n",
    "                    grads = tape.gradient(loss, self.local_model.trainable_weights)\n",
    "\n",
    "                    # Push local gradients to global model\n",
    "                    self.optimizer.apply_gradients(zip(grads, self.global_model.trainable_weights))\n",
    "                    \n",
    "                    # Update local model with new weights\n",
    "                    self.local_model.set_weights(self.global_model.get_weights())  \n",
    "                    \n",
    "                current_state = new_state\n",
    "\n",
    "            AsyncWorkerAgent.global_moving_average_reward = \\\n",
    "                  self.record_episode(episode_idx, total_reward, total_loss, steps_taken, AsyncWorkerAgent.global_moving_average_reward, self.worker_idx)\n",
    "\n",
    "            # Check if our local model has surpassed \n",
    "            if total_reward > AsyncWorkerAgent.best_reward:\n",
    "                # Acquire the mutex to save weights without any data races\n",
    "                with AsyncWorkerAgent.mutex:\n",
    "                    weights_file = os.path.join(self.save_dir,\n",
    "                                     f'model_{self.env_name}_{total_reward}.h5')\n",
    "                    print(f\"Saving best model to {weights_file}, episode score: {total_reward}\")\n",
    "                    self.global_model.save_weights(weights_file)\n",
    "                    AsyncWorkerAgent.best_reward = total_reward\n",
    "            \n",
    "            AsyncWorkerAgent.global_episode_idx += 1\n",
    "                \n",
    "\n",
    "    def compute_loss(self, done, state, prev_states, prev_actions, prev_rewards, gamma):\n",
    "        \"\"\"\n",
    "        Helper function to compute custom loss for A3C.\n",
    "        \"\"\"\n",
    "        ### TODO: Initialize reward_sum based on the expected future reward from the current state ###       \n",
    "        if done:\n",
    "            reward_sum = 0\n",
    "        else:\n",
    "            reward_sum = 0\n",
    "        ## TODO ###\n",
    "\n",
    "        ### TODO: Discount previous rewards using discount factor\n",
    "        discounted_rewards = []\n",
    "        ### TODO ###\n",
    "\n",
    "        ### TODO: Compute the logits and values using our local model ###\n",
    "        logits, values = None, None\n",
    "        ### TODO ###\n",
    "        \n",
    "        # Get our advantages\n",
    "        advantage = tf.convert_to_tensor(np.array(discounted_rewards)[:, None],\n",
    "                                dtype=tf.float32) - values\n",
    "        # Value loss\n",
    "        value_loss = advantage ** 2\n",
    "\n",
    "        # Calculate our policy loss\n",
    "        policy = tf.nn.softmax(logits)\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy, logits=logits)\n",
    "\n",
    "        policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=prev_actions,\n",
    "                                                                     logits=logits)\n",
    "        policy_loss *= tf.stop_gradient(advantage)\n",
    "        policy_loss -= 0.01 * entropy\n",
    "        total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've built the individual Asynchronous workers, it's time to build the master A3C agent that will put the workers to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent that uses Asynchronous Advantage Actor-Critic (A3C) method for learning\n",
    "    \"\"\"\n",
    "    def __init__(self, env_name, num_episodes):\n",
    "        # Call base constructor\n",
    "        super().__init__(env_name, num_episodes)\n",
    "\n",
    "        self.save_dir = \"/tmp/\"\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "\n",
    "        ### TODO: Get the size of the state and action spaces from self.env ###  \n",
    "        self.state_size = None\n",
    "        self.action_size = None\n",
    "        ### TODO ###\n",
    "        \n",
    "        learning_rate = 0.001\n",
    "        ### TODO: Initialize Adam optimizer in locked mode with learning rate 0.001 ###\n",
    "        self.optimizer = None\n",
    "        ### TODO ###\n",
    "        \n",
    "        ### TODO: Create an ActorCritic model using our custom class, and initialize it with random data ###\n",
    "        self.global_model = None\n",
    "        ### TODO ###        \n",
    "        \n",
    "    def train(self):\n",
    "\n",
    "        ### TODO: Create 4 workers and store them in this list ###\n",
    "        # (Normally, we would limit based on number of cores on the computer for parallelization)\n",
    "        workers = []\n",
    "        ### TODO ###\n",
    "        \n",
    "        for i, worker in enumerate(workers):\n",
    "            print(f\"Starting worker {i}\")\n",
    "            worker.start()\n",
    "            \n",
    "        # Wait for all workers to finish\n",
    "        [w.join() for w in workers]\n",
    "        \n",
    "        ### TODO: Plot a graph of rewards from self.result_queue over time ### \n",
    "        \n",
    "        ### TODO ###\n",
    "    \n",
    "    def play(self, render=False, model_path=\"\"):\n",
    "        # If desired, load model from file\n",
    "        if model_path != \"\":\n",
    "            model_path = os.path.join(self.save_dir, model_path)\n",
    "            print(f'Loading model from: {model_path}')\n",
    "            self.global_model.load_weights(model_path)\n",
    "\n",
    "        # Reset environment\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps_taken = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            ### TODO: Extract the logits and value from the global model ###\n",
    "            logits, value = None, None\n",
    "            policy = None\n",
    "            ### TODO ###\n",
    "            \n",
    "            ### TODO: Identify optimal action from policy ###\n",
    "            action = None\n",
    "            ### TODO ###\n",
    "            \n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "        \n",
    "            if render:\n",
    "                self.env.render(mode='rgb_array') # Only render if requested\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps_taken += 1\n",
    "\n",
    "            print(f\"Step {steps_taken}: Total Reward: {total_reward}, Action: {action}\")\n",
    "        \n",
    "        self.env.close()\n",
    "        return total_reward, steps_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! Time to train the agent with 1000 episodes. **This will take a while.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting worker 0\n",
      "Starting worker 1\n",
      "Starting worker 2\n",
      "Starting worker 3\n",
      "Episode: 0 | Moving Average Reward: 8 | Episode Reward: 8 | Loss per step: 1.717 | Steps: 8 | Worker: 3\n",
      "Saving best model to /tmp/model_CartPole-v0_8.0.h5, episode score: 8.0\n",
      "Episode: 0 | Moving Average Reward: 8 | Episode Reward: 30 | Loss per step: 3.08 | Steps: 30 | Worker: 2\n",
      "Saving best model to /tmp/model_CartPole-v0_30.0.h5, episode score: 30.0\n",
      "Episode: 0 | Moving Average Reward: 8 | Episode Reward: 30 | Loss per step: 3.001 | Steps: 30 | Worker: 0\n",
      "Episode: 0 | Moving Average Reward: 9 | Episode Reward: 55 | Loss per step: 3.262 | Steps: 55 | Worker: 1\n",
      "Saving best model to /tmp/model_CartPole-v0_55.0.h5, episode score: 55.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = A3CAgent(\"CartPole-v0\", 1000)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-check:\n",
    "\n",
    "You should observe that the agent converges to a solution near the maximum reward of 200. OpenAI formally defines 'solving' CartPole once your algorithm achieves a running average of 195.0 reward or higher for 100 episodes. Depending on some random seeding, you may observe that your model doesn't exactly meet this requirement, but as long as the individual episode rewards are near 200 you are fine.\n",
    "\n",
    "If your solution is lagging significantly below the 200 reward level, check to make sure that your loss function is implemented as per the specifications, as this is the most common student error.\n",
    "\n",
    "Let's see what that looks like in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Total Reward: 1.0, Action: 0\n",
      "Step 2: Total Reward: 2.0, Action: 0\n",
      "Step 3: Total Reward: 3.0, Action: 0\n",
      "Step 4: Total Reward: 4.0, Action: 0\n",
      "Step 5: Total Reward: 5.0, Action: 0\n",
      "Step 6: Total Reward: 6.0, Action: 0\n",
      "Step 7: Total Reward: 7.0, Action: 0\n",
      "Step 8: Total Reward: 8.0, Action: 0\n",
      "Step 9: Total Reward: 9.0, Action: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.0, 9)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In case you need to re-run this cell because of a notebook crash, you can load training weights from a file\n",
    "# Either scroll through the training output or scan through the save directory for the weights with best performance\n",
    "# agent.play(render=True, model_path=\"/tmp/model_CartPole-v0_200.0.h5\")\n",
    "\n",
    "agent.play(render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! If your model looks good qualitatively, you are done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
