{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: MDPs and Policy\n",
    "In this part of the project, we will be using OpenAI's gym API for various reinforcement learning exercises.\n",
    "\n",
    "In this part of the project, we will explore each of the following topics:\n",
    "\n",
    "1. Markov Decision Processes (2 Hours)\n",
    "    * States\n",
    "    * Actions\n",
    "    * Discount Factors\n",
    "    * Transition Function\n",
    "    * Reward Function\n",
    "    * Introduction to OpenAI Gym API\n",
    "2. Solving Markov Decision Processes (1 Hour)\n",
    "    * Simple MDP\n",
    "    * Policies\n",
    "    * Value Iteration\n",
    "    * Policy Extraction\n",
    "    * Policy Iteration\n",
    "    * Policy Evaluation\n",
    "3. Reinforcement Learning (2 Hours)\n",
    "    * Online Planning vs Offline Planning\n",
    "    * Online Planning: Model-Based vs Model-Free Approaches\n",
    "    * Model-Free: A Closer Look\n",
    "    * Model-Free Policy Evaluation: Direct Evaluation (Passive RL)\n",
    "    * Model-Free Policy Evaluation: Temporal Difference Learning (Passive RL)\n",
    "    * Model-Free Policy Learning/Updates: Q-Learning (Active RL)\n",
    "4. Wrapping Up\n",
    "    * Opportunities in RL\n",
    "    \n",
    "The majority of the time you will be spending on this project will be in writing the various agents to solve the various gym environments. Throughout this notebook, you will be asked to reflect on various demonstrations. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Markov Decision Processes (2 hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Finish MDP explanation\n",
    "\n",
    "\n",
    "### What is a Markov Decision Process? (30 min)\n",
    "We can represent the world (like Taxi World and like Chain World) with a model. This model is called a Markov Decision Process. It has a number of components. Let's take a look at each of these components for a model we call chain world.\n",
    "\n",
    "1. What are States?\n",
    "\n",
    "2. What are Actions?\n",
    "\n",
    "3. What are Transition Functions?\n",
    "\n",
    "4. What are Rewards?\n",
    "\n",
    "5. What are Discount Factors?\n",
    "\n",
    "6. What are Q-States, Q-Values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym API (30 mins)\n",
    "The OpenAI Gym API provides an easy way for students and researchers to test out their Reinforcement Learning agents in pre-specified environments. The API also provides a framework to build your own environments to train your agents on. Throughout this project, you will be using the OpenAI Gym API. Here's a quick tutorial: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to OpenAI Gym Through Chain World\n",
    "\n",
    "To create chain world, we built our own environment using the OpenAI Gym API. An environment is its own simulated world where RL agents can explore, learn, and reap rewards. Every environment is defined just as we would define an MDP -- with states, action spaces, slippage probabilities (transition functions), rewards, etc.\n",
    "\n",
    "Please take a few minutes to look over the various environments included in the API at this link: [OpenAI Environments](https://gym.openai.com/envs/#classic_control). \n",
    "\n",
    "**Please carefully follow the code in the cells below and read the comments to get a full picture of the API you will be using throughout this part of the project.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing gym\n",
    "import gym\n",
    "\n",
    "# Installing the chain world environment\n",
    "!pip install -e ./gym-note4-mdp/ -q\n",
    "\n",
    "# Importing chain world environment\n",
    "import gym_note4_mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the environment\n",
    "env = gym.make('note4-mdp-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment\n",
      "+---------+\n",
      "|B: :\u001b[43m \u001b[0m: :S|\n",
      "+---------+\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at this environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `render()` function, you can visualize the chain world environment. This visualization has three parts. The _Environment_ part shows the agent in the environment. The _States_ part is a reference to quickly cross check which state the agent is in. The _Agent_ part shows the agent.\n",
    "\n",
    "**Let's go over the specifics of the chain world as we are learning the API. Look to the code blocks for specific functions.**\n",
    "\n",
    "### Chain World\n",
    "\n",
    "This is a simple MDP which represents a chain of states. \n",
    "\n",
    "#### States\n",
    "\n",
    "The states are 0, 1, 2, 3, and 4 and they correspond to the position of the agent (the yellow rectangle) in the chain. Initially the agent starts deterministically at State 2. Verify that the setup makes sense by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment\n",
      "+---------+\n",
      "|B: :\u001b[43m \u001b[0m: :S|\n",
      "+---------+\n",
      "Agent's current state: 2\n"
     ]
    }
   ],
   "source": [
    "# The env.reset() method reinitializes the env to default values\n",
    "env.reset()\n",
    "env.render()\n",
    "# We can access the environments's current state by using env.state\n",
    "print(\"Agent's current state:\", env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "In the chain world, there are three actions that the agent can take:\n",
    "     \n",
    "* (1) forward, which moves the agent forward along the chain\n",
    "* (0) backward, which moves the agent backward along the chain\n",
    "* (2) exit, which tries to exit the world\n",
    "\n",
    "The cell below will show you how to make actions using the `step(action)` function and will take you through one episode where the agent goes forward, backward, backward, backward, and then exits with reward +10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1: Initial State\n",
      "Environment\n",
      "+---------+\n",
      "|B: :\u001b[43m \u001b[0m: :S|\n",
      "+---------+\n",
      "\n",
      "Frame 2: Agent moves forward to state 3 and receives reward 0\n",
      "Environment\n",
      "+---------+\n",
      "|B: : :\u001b[43m \u001b[0m:S|\n",
      "+---------+\n",
      "\n",
      "Frame 3: Agent moves backward to state 2 and receives reward 0\n",
      "Environment\n",
      "+---------+\n",
      "|B: :\u001b[43m \u001b[0m: :S|\n",
      "+---------+\n",
      "\n",
      "Frame 4: Agent moves backward to state 1 and receives reward 0\n",
      "Environment\n",
      "+---------+\n",
      "|B:\u001b[43m \u001b[0m: : :S|\n",
      "+---------+\n",
      "\n",
      "Frame 5: Agent moves backward to state 0 and receives reward 0\n",
      "Environment\n",
      "+---------+\n",
      "|\u001b[43mB\u001b[0m: : : :S|\n",
      "+---------+\n",
      "\n",
      "Frame 6: Agent moves exit to state 0 and receives reward 10\n",
      "Environment\n",
      "+---------+\n",
      "|\u001b[42mB\u001b[0m: : : :S|\n",
      "+---------+\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.discount_factor = 1\n",
    "done = False\n",
    "\n",
    "env.slip = 0 # This is the probability of the agent slipping\n",
    "print(\"Frame 1: Initial State\")\n",
    "env.render(done=done)\n",
    "\n",
    "state, reward, done, _ = env.step(1) # .step(action) function takes an action on the environment\n",
    "print(\"\\nFrame 2: Agent moves {} to state {} and receives reward {}\".format(\"forward\", state, reward))\n",
    "env.render(done=done)\n",
    "\n",
    "state, reward, done, _ = env.step(0) # moves backward\n",
    "print(\"\\nFrame 3: Agent moves {} to state {} and receives reward {}\".format(\"backward\", state, reward))\n",
    "env.render(done=done)\n",
    "\n",
    "state, reward, done, _ = env.step(0) # moves backward\n",
    "print(\"\\nFrame 4: Agent moves {} to state {} and receives reward {}\".format(\"backward\", state, reward))\n",
    "env.render(done=done)\n",
    "\n",
    "state, reward, done, _ = env.step(0) # moves backward\n",
    "print(\"\\nFrame 5: Agent moves {} to state {} and receives reward {}\".format(\"backward\", state, reward))\n",
    "env.render(done=done)\n",
    "\n",
    "state, reward, done, _ = env.step(2) # exits\n",
    "print(\"\\nFrame 6: Agent moves {} to state {} and receives reward {}\".format(\"exit\", state, reward))\n",
    "env.render(done=done)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Functions\n",
    "\n",
    "From any state, the agent can try to move fowards (1), move backwards (0), or exit (2). But there is no guarantee that the agent will succeed. You can imagine that chain world is a slippery place. The transitions are not determistic. If you choose to go forwards, you could slip and go backwards with some probability and vice versa. Also, if the agent tries to exit from any state other than the 0 or 4, nothing will happen because that is not valid action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slippage probability in deterministic world: 0\n",
      "Slippage probability in uncertain world: 0.2\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.slip = 0\n",
    "print(\"Slippage probability in deterministic world: {}\".format(env.slip))\n",
    "\n",
    "env.reset() # By default the world is uncertain\n",
    "print(\"Slippage probability in uncertain world: {}\".format(env.slip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewards\n",
    "     \n",
    "At the beginning of the chain (State 0) there is a big reward of +10 (B) that the agent will be rewarded if it chooses to exit (Action 2) when in State 0.\n",
    "\n",
    "At the end of the chain (State 4) there is a small reward of +1 (S) that the agent will be rewarded if it chooses to exit (Action 2) when in State 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small Reward: 1\n",
      "Big Reward: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Small Reward:\", env.small)\n",
    "print(\"Big Reward:\", env.large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discount Factor\n",
    "    \n",
    "The discount factor is $\\gamma = 0.1$. So as we take steps the rewards are discounted such that $${reward_{t+1}} = {reward_t}*{\\gamma}$$ \n",
    "\n",
    "Observe what happens to the small and large rewards as we take steps in the cells below for various discount factors. Describe what you see in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discount Factor: 0.1 \n",
      "\n",
      "\t After 0 moves\n",
      "\t Small Reward: 1\n",
      "\t Big Reward: 10 \n",
      "\n",
      "\t After 1 moves\n",
      "\t Small Reward: 0.1\n",
      "\t Big Reward: 1.0 \n",
      "\n",
      "\t After 2 moves\n",
      "\t Small Reward: 0.010000000000000002\n",
      "\t Big Reward: 0.1 \n",
      "\n",
      "\t After 3 moves\n",
      "\t Small Reward: 0.0010000000000000002\n",
      "\t Big Reward: 0.010000000000000002 \n",
      "\n",
      "\t After 4 moves\n",
      "\t Small Reward: 0.00010000000000000003\n",
      "\t Big Reward: 0.0010000000000000002 \n",
      "\n",
      "Discount Factor: 0.5 \n",
      "\n",
      "\t After 0 moves\n",
      "\t Small Reward: 1\n",
      "\t Big Reward: 10 \n",
      "\n",
      "\t After 1 moves\n",
      "\t Small Reward: 0.5\n",
      "\t Big Reward: 5.0 \n",
      "\n",
      "\t After 2 moves\n",
      "\t Small Reward: 0.25\n",
      "\t Big Reward: 2.5 \n",
      "\n",
      "\t After 3 moves\n",
      "\t Small Reward: 0.125\n",
      "\t Big Reward: 1.25 \n",
      "\n",
      "\t After 4 moves\n",
      "\t Small Reward: 0.0625\n",
      "\t Big Reward: 0.625 \n",
      "\n",
      "Discount Factor: 0.9 \n",
      "\n",
      "\t After 0 moves\n",
      "\t Small Reward: 1\n",
      "\t Big Reward: 10 \n",
      "\n",
      "\t After 1 moves\n",
      "\t Small Reward: 0.9\n",
      "\t Big Reward: 9.0 \n",
      "\n",
      "\t After 2 moves\n",
      "\t Small Reward: 0.81\n",
      "\t Big Reward: 8.1 \n",
      "\n",
      "\t After 3 moves\n",
      "\t Small Reward: 0.7290000000000001\n",
      "\t Big Reward: 7.29 \n",
      "\n",
      "\t After 4 moves\n",
      "\t Small Reward: 0.6561000000000001\n",
      "\t Big Reward: 6.561 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for discount_factor in [0.1, 0.5, 0.9]:\n",
    "    env.reset()\n",
    "    env.discount_factor = discount_factor\n",
    "    print(\"Discount Factor: {} \\n\".format(env.discount_factor))\n",
    "    print(\"\\t After {} moves\".format(0))\n",
    "    print(\"\\t Small Reward: {}\".format(env.small))\n",
    "    print(\"\\t Big Reward: {} \\n\".format(env.large))\n",
    "\n",
    "    action_space = [0, 1, 2]\n",
    "    for i in range(1, 5):\n",
    "        action = env.sample() #This lets us randomly sample an action\n",
    "        env.step(action)\n",
    "        print(\"\\t After {} moves\".format(i))\n",
    "        print(\"\\t Small Reward: {}\".format(env.small))\n",
    "        print(\"\\t Big Reward: {} \\n\".format(env.large))\n",
    "    env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your understanding #1:** What impact does changing the discount factor have? Why do we need a discount factor in the first place? (Hint: What happens when the discount factor = 1?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your understanding #2:** What would you do if you were the agent in chain world? Think about this question in a general way. For example, what would you do if you were initially in state 4 and the discount factor was 0.1? What would you do if the slippage probability was greater than 0.5? We don't expect rigorous justification here, we just want to get you thinking about _policy_ -- a topic we will be looking at in much greater detail later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi World\n",
    "\n",
    "Now that you've gotten a chance to familiarize yourself with the OpenAI Gym API and the simple chain world environment, let's take a look at a more complicated (albeit still very simple) environment we call Taxi World. Fill in the code when prompted. \n",
    "\n",
    "After looking at the Taxi World in depth, you will be writing a \"dumb agent\" that chooses moves randomly for Taxi World and Chain World that will serve as a baseline for better agents we write later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "import numpy as np\n",
    "!pip install cmake 'gym[atari]' scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make the taxi world environment\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Render the environment using the OpenAI Gym API\n",
    "## Your code here ## (Ans: env.render())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Taxi World, the agent is a taxi cab. In the visualization, the taxi cab is the yellow rectangle. The Taxi World has walls, represented by | and the taxi agent cannot go through these walls. The Taxi World also has pickup/dropoff locations represented by the letters (R, G, Y, B) in the visualization. \n",
    "\n",
    "In each run of Taxi World, the taxi agent must pickup a passenger at the blue letter location, transport them to the pink letter location, and dropoff the passenger at the pink letter location.\n",
    "\n",
    "When the Taxi has picked up a passenger, its color will change from yellow to blue.\n",
    "\n",
    "Formally, we can represent this as an MDP! Let's specify each component of the underlying MDP.\n",
    "\n",
    "* **States**\n",
    "    * How many states are there in the Taxi World?\n",
    "    * The Taxi World environment has many more states than the Chain World environment. For the chain world, it was relatively easy to see that the chain world had 6 states (0, 1, 2, 3, 4, terminal state), but how would we figure out the number of states in the Taxi World?\n",
    "    * **Let's go through this exercise:**\n",
    "\n",
    "    * How many possible locations are there for the taxi agent?\n",
    "        * Answer: The taxi world is a 5x5 grid so there are 25 possible locations.\n",
    "    * For each location of the taxi, how many possible passenger locations are there?\n",
    "        * Answer: There are 4 possible passenger locations if the passenger has not yet been picked up and 1 possible passenger location if the passenger has been picked up. This gives us a total of 5 passenger locations.\n",
    "     \n",
    "    * For each location of the taxi and passenger location, how many drop off locations are there?\n",
    "        * Answer: There are still 4 possible drop off locations\n",
    "    * Answer: So, in total there are $5*5*5*4 = 500$ states in this world! That's a lot more than there were in the chain world!  \n",
    "\n",
    "* **Actions**\n",
    "    * 0: south\n",
    "    * 1: north\n",
    "    * 2: east\n",
    "    * 3: west\n",
    "    * 4: pickup\n",
    "    * 5: dropoff\n",
    "\n",
    "* **Transitions**\n",
    "    * For now, the taxi world is deterministic, so the taxi will go to the next state based solely on the action chosen by the taxi agent.\n",
    "    \n",
    "* **Rewards**\n",
    "    * Movement actions have reward -1.0. A negative reward makes moving costly. This makes sense, because the taxi agent should be incentivized to pickup and dropoff passengers as efficiently as possible.\n",
    "    * Pickup/dropoff actions have reward -10 if the taxi agent is not at a pickup or dropoff location. This is a large penalty.\n",
    "    * At dropoff/pickup locations, dropoff and pickup would have higher rewards (+20), if the agent succeeds in picking up and dropping off the passenger at the correct locations.\n",
    "    \n",
    "* **Discount Factor**\n",
    "    * The discount factor is 1. The rewards do not diminish over time.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise: Random Agents (30 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will be using what you have learned to write code for an agent that moves randomly through the world. The recap section below provides some code that should be helpful. In the cell below, write code for a taxi agent that at each step chooses a random move. While the taxi agent has not dropped off its passenger at the correct dropoff location, continue making random moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(6)\n",
      "Size of action space: 6\n",
      "Initial State\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n",
      "Make a random action: 0\n",
      "\n",
      " State: 248, Reward: -1, Done: False\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "# Recap of API\n",
    "print(\"Action space: {}\".format(env.action_space)) # Gives you the action space\n",
    "print(\"Size of action space: {}\".format(env.action_space.n))\n",
    "print(\"Initial State\")\n",
    "env.render()\n",
    "\n",
    "action = env.action_space.sample()# Sample an action randomly from the sample space\n",
    "print(\"\\nMake a random action: {}\".format(action))\n",
    "state, reward, done, _ = env.step(action) # Take an action\n",
    "print(f\"\\n State: {state}, Reward: {reward}, Done: {done}\")\n",
    "env.render() # Draw the state\n",
    "\n",
    "# state is the new state\n",
    "# reward is the reward associated with making that action\n",
    "# done is a boolean telling us whether we have reached a terminal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivate RL With Random moving agent in Chain World. Have students code the random moving behavior in both of these agents. (15 min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE FOR RANDOM TAXI WORLD AGENT HERE ##\n",
    "def random_agent(env):\n",
    "    frames = []\n",
    "    env.reset()\n",
    "    ## Keep track of actions_taken, and total_reward in your code\n",
    "    actions_taken = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    # Answer #\n",
    "    # TODO: change the condition\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        actions_taken += 1\n",
    "        \n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'total reward': total_reward\n",
    "            }\n",
    "        )\n",
    "          \n",
    "    ## END OF YOUR CODE ##\n",
    "        \n",
    "    return frames, actions_taken, total_reward\n",
    "    \n",
    "## Utils Functions to Run and Visualize ## \n",
    "# Source: see reference [1]\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames, t=0.1):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Time: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        print(f\"Total Reward: {frame['total reward']}\")\n",
    "        sleep(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions taken: 459\n",
      "Total Reward: -1680\n"
     ]
    }
   ],
   "source": [
    "frames, actions_taken, total_reward = random_agent(env)\n",
    "print(\"Actions taken: {}\".format(actions_taken))\n",
    "print(\"Total Reward: {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Time: 459\n",
      "State: 410\n",
      "Action: 5\n",
      "Reward: 20\n",
      "Total Reward: -1680\n"
     ]
    }
   ],
   "source": [
    "# Hit ctrl-c to interrupt this if you would rather not watch the agent the whole time\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about the random agent in the Taxi world? About how many actions does the taxi agent take before picking up the passenger for the first time? How many actions does the random agent take before finally stumbling into the terminal state? How much reward does it incurr in total? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terrible performance of this random agent should motivate our study of reinforcement learning. The random agents performance in our taxi world will serve as our baseline for future agents we write.\n",
    "\n",
    "For good measure, to showcase that the algorithm would work on any environment, run the random agent in the chain world environment in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions taken: 5\n",
      "Total Reward: 1.0000000000000004e-05\n"
     ]
    }
   ],
   "source": [
    "# Reimplemented with different sample method\n",
    "# Get the action space\n",
    "def agent(env):\n",
    "    env.reset()\n",
    "    frames, actions_taken, total_reward, done = [], 0, 0, False\n",
    "    \n",
    "    while not done:\n",
    "        action = env.sample()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        actions_taken += 1\n",
    "\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi', done=done),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'total reward': total_reward\n",
    "            }\n",
    "        )\n",
    "    return frames, actions_taken, total_reward\n",
    "\n",
    "env = gym.make('note4-mdp-v0')\n",
    "frames, actions_taken, total_reward = agent(env)\n",
    "print(\"Actions taken: {}\".format(actions_taken))\n",
    "print(\"Total Reward: {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment\n",
      "+---------+\n",
      "|B: : : :\u001b[42mS\u001b[0m|\n",
      "+---------+\n",
      "\n",
      "Time: 5\n",
      "State: 4\n",
      "Action: 2\n",
      "Reward: 1.0000000000000004e-05\n",
      "Total Reward: 1.0000000000000004e-05\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral of this exercise is that we can do better! We can do better by learning from our experience. Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Exercise: Define the MDP for Black Jack. (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Exercise II: Define the MDP for Pacman. (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Solving MDPs  (1 Hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve an MDP is to find an optimal policy. What is a policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Exercise: Test out different policies in the MDP provided\n",
    "\n",
    "Specify the gym-note4-env and have students visualize different policies, enter their own policies, and see the effect of the discount factor on changing the optimal policy. This should be very hand wavy because students do not really know about Policy updates, Bellman, Value Iteration, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# Justify why the equation is what it is.\n",
    "# Draw out each part of the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain value iteration.\n",
    "Do value iteration on the note4 simple MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Will be getting into Direct evaluation and temporal \n",
    "# difference learning as a way of evaluation later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Reinforcement Learning: Evaluating an Learning Policies! (2 Hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now students should have experience writing random moving agents, working with OpenAI gym API, and solving MDPs using value iteration, and learning optimal policies by hand on simple chain world.\n",
    "\n",
    "Let's apply what we have learned to the Chain World and Taxi World by doing policy extraction, policy iteration, and policy evaluation in chain world. And then learning in both chain world and taxi world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline vs Online Planning\n",
    "\n",
    "What if we don't know the underlying MDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Planning: Model-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO think of something for students to do here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Planning: Model-Free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Online Planning: How would we do policy evaluation?\n",
    "# 1. Direct Evaluation\n",
    "# 2. Temporal Difference Learning\n",
    "\n",
    "### Take students through chain world direct evaluation.\n",
    "### Have them define some policies and try them out, \n",
    "# change slippage factor, change discount factor. Visualize in tables.\n",
    "\n",
    "\n",
    "### Take students through temporal difference learning. \n",
    "### Have them implement the update rule. \n",
    "### Have them add a way of tuning hyperparameters like alpha (learnnig rate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### But wait isn't the goal to learn a policy?\n",
    "#### Enter Q-Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
