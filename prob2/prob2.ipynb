{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Policy Gradients\n",
    "\n",
    "Authors: Sean Lin, Jaiveer Singh, Ethan Mehta\n",
    "\n",
    "Estimated Time: 3 hrs.\n",
    "\n",
    "References: \n",
    "\n",
    "- OpenAI Spinning Up (https://spinningup.openai.com/en/latest/index.html)\n",
    "<br> OpenAI's open source resource on reinforcement learning, Spinning Up, provides utils files and skeleton code for vanilla policy gradients that we built upon for this assignment\n",
    "\n",
    "- EECS 16A Segway Tours Problem: https://eecs16a.org/homework/prob4.pdf\n",
    "\n",
    "- Vanilla Policy Gradients: https://medium.com/@aniket.tcdav/vanilla-policy-gradient-with-tensorflow-2-9855df271472, https://www.janisklaise.com/post/rl-policy-gradients/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall the classic EECS 16AB self-balancing segway problem. The segway problem is used as a means to develop fundamental understanding of control theory and linear algebra. Interestingly, we can use reinforcement learning methods to teach a segway how to self-balance! In the previous walkthrough assignment, you were introduced to the fundamental concepts of reinforcement of learning: actions, states, policies, and rewards. This assignment will use the segway problem as a means to introduce students to reinforcement learning, policy gradients as a means of optimizing policies, and deep RL.\n",
    "\n",
    "<img src=\"images/segway.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We have prepared a conda environment for you to work on this assignment, since this assignment uses packages such as tensorflow, numpy, and OpenAI gym that may cause problems if the versions are incompatible and different. Follow the following steps to set up the environment:\n",
    "\n",
    "1) In this problem's, you should see an environment.yml file.\n",
    "<br> 2) Create a new conda environment named spinningup by running `conda env create --file environment.yaml`\n",
    "<br> 3) Run `conda activate spinningup` to activate the environment\n",
    "<br> 4) Run `python -m ipykernel install --user --name=spinningup` to export this environment into an iPython notebook kernel so that we can use it in this notebook.\n",
    "<br> 5) Restart the jupyter notebook. In the menu, go to `Kernel -> Change Kernel`. You should see an environment named `spinningup` among the list of kernels. Switch your kernel to `spinningup`, and now you are all set!\n",
    "<br>\n",
    "<img src='images/kernel_change.png' width=\"909\" height=\"500\">\n",
    "</br>\n",
    "<br> 6) Run the imports below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym.wrappers.monitor import Monitor, load_results\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import core\n",
    "import vpg\n",
    "from utils.logx import EpochLogger\n",
    "from utils.mpi_tf import MpiAdamOptimizer, sync_all_params\n",
    "from utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n",
    "from utils.run_utils import setup_logger_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Note:</b> Because machine learning, and specifically deep reinforcement learning, is still a relatively nascent field, many machine-learning libraries are fragile and require dependencies that may easily become deprecated (i.e. tensorflow 1.15 vs. tensorflow 2.0). To reproduce machine learning experiments and projects, it is paramount that one knows how to freeze and export an environment for later usage as we did for you in this assignment. We recommend that all students taking EECS 16ML be well-versed in this practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Segway to Stand Upright\n",
    "\n",
    "In the segway problem, the segway can move anywhere on the line, and lean in any way. Therefore, we can define the state of the world as the following: \n",
    "$$[\\text{position of cart}, \\text{velocity of cart}, \\text{angle of pole}, \\text{rotation rate of pole}]$$\n",
    "\n",
    "The actions that the segway can take to change its state are to 1) lean left and 2) lean right.\n",
    "We can imagine the reward as the amount of time that the segway stays upright without falling down. Now, we just need to determine a policy such that the segway can take the optimal actions, given the state, to maximize the reward.\n",
    "\n",
    "As always, let's start with a simple example. We will program an agent that uses a logistic policy to linearly classify our decision to either lean left or right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement the gradient ascent step for a basic policy\n",
    "\n",
    "Below, we define the `LogisticPolicy` class. Most of the code is already implemented for you, including the code to calculate the reward and the gradient of the log likelihood. Your task is to implement the gradient ascent step in the `update` function of the class. Recall that the gradient ascent step is defined as the following:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha *(E_\\theta[\\sum_{t=1}^T \\nabla_\\theta \\log P(s_{t+1} | s_t, a_t) R(\\tau)])$$\n",
    "\n",
    "The reward is calculated in `discount_rewards` and the gradient of the log likelihood is calculated in `grad_log_p`. Fill in the code for `update` to complete gradient ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticPolicy:\n",
    "\n",
    "    def __init__(self, theta, alpha, gamma):\n",
    "        # Initialize paramters θ, learning rate α and discount factor γ\n",
    "\n",
    "        self.theta = theta\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def logistic(self, y):\n",
    "        # definition of logistic function\n",
    "\n",
    "        return 1/(1 + np.exp(-y))\n",
    "\n",
    "    def probs(self, x):\n",
    "        # returns probabilities of two actions\n",
    "\n",
    "        y = x @ self.theta\n",
    "        prob0 = self.logistic(y)\n",
    "\n",
    "        return np.array([prob0, 1-prob0])\n",
    "\n",
    "    def get_action(self, x):\n",
    "        # sample an action in proportion to probabilities\n",
    "\n",
    "        probs = self.probs(x)\n",
    "        action = np.random.choice([0, 1], p=probs)\n",
    "\n",
    "        return action, probs[action]\n",
    "\n",
    "    def grad_log_p(self, x):\n",
    "        # calculate grad-log-probs\n",
    "\n",
    "        y = x @ self.theta\n",
    "        grad_log_p0 = x - x*self.logistic(y)\n",
    "        grad_log_p1 = - x*self.logistic(y)\n",
    "\n",
    "        return grad_log_p0, grad_log_p1\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        # calculate discounted rewards\n",
    "\n",
    "        discounted_rewards = np.zeros(len(rewards))\n",
    "        cumulative_rewards = 0\n",
    "        for i in reversed(range(0, len(rewards))):\n",
    "            cumulative_rewards = cumulative_rewards * self.gamma + rewards[i]\n",
    "            discounted_rewards[i] = cumulative_rewards\n",
    "\n",
    "        return discounted_rewards\n",
    "\n",
    "    def update(self, rewards, obs, actions):\n",
    "        # calculate gradients for each action over all observations\n",
    "        grad_log_p = np.array([self.grad_log_p(ob)[action] for ob,action in zip(obs,actions)])\n",
    "\n",
    "        assert grad_log_p.shape == (len(obs), 4)\n",
    "\n",
    "        # calculate the rewards earned for a specific trajectory of actions\n",
    "        discounted_rewards = self.discount_rewards(rewards)\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        self.theta = \n",
    "        ### END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implement the train step of the reinforcement learning trainer\n",
    "\n",
    "Now, we will implement the model trainer, which will iterate through the data and optimize our policy. Below, we define the `RLTrainer` class. Most of the code is already implemented for you; all you have to do is fill in the code inside of the `train` loop. The loop should do three things:\n",
    "1) Run an episode (simulation) of the segway world given the current policy<br>\n",
    "2) Append the total reward computed from the episode to the list `episode_rewards` for future graphical analysis<br>\n",
    "3) Update the model policy given the rewards, observations, and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLTrainer:\n",
    "    \n",
    "    def __init__(self, policy):\n",
    "        self.policy = policy\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "    def run_episode(self, model_policy, render=True):\n",
    "\n",
    "        observation = self.env.reset()\n",
    "        totalreward = 0\n",
    "\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        probs = []\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                self.env.render()\n",
    "\n",
    "            observations.append(observation)\n",
    "\n",
    "            action, prob = model_policy.get_action(observation)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "\n",
    "            totalreward += reward\n",
    "            rewards.append(reward)\n",
    "            actions.append(action)\n",
    "            probs.append(prob)\n",
    "\n",
    "        return totalreward, np.array(rewards), np.array(observations), np.array(actions), np.array(probs)\n",
    "    \n",
    "    def train(self, theta, alpha, gamma, MAX_EPISODES=1000, seed=np.random.seed(0)):\n",
    "        # Initialize variables\n",
    "        episode_rewards = []\n",
    "        model_policy = self.policy(theta, alpha, gamma)\n",
    "\n",
    "        # train until MAX_EPISODES\n",
    "        for i in range(MAX_EPISODES):\n",
    "            \n",
    "            ### YOUR CODE HERE ###\n",
    "\n",
    "            ### END ###\n",
    "            \n",
    "            print(\"EP: \" + str(i) + \" Score: \" + str(total_reward) + \" \",end=\"\\r\", flush=False)\n",
    "        self.env.close()\n",
    "        return episode_rewards, model_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented the model policy and trainer, run the code block below to watch our segway learn to stand upright! We train with a learning rate of 0.002, gamma of 0.99, and 300 episodes.\n",
    "\n",
    "A reward of 200 means that the segway successfully learned how to stand upright for a substantial period of time, and thus completed its challenge!\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1) Around how many episodes does it take for the CartPole to complete the challenge?\n",
    "<br> <i> Answer: </i>\n",
    "\n",
    "\n",
    "2) What do you notice about the CartPole as it is training? Describe its improvement over time.\n",
    "<br> <i> Answer: </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RLTrainer(LogisticPolicy)\n",
    "\n",
    "episode_rewards, policy = trainer.train(theta=np.random.rand(4),\n",
    "                                alpha=0.002,\n",
    "                                gamma=0.99,\n",
    "                                MAX_EPISODES=300)\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Graph of the reward obtained at each episode\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Policies: Intro to Deep RL\n",
    "\n",
    "In the previous section, we used a logistic policy as a means to teach our segway to stand upright. While the segway did imporve and eventually complete the challenge, you probably noticed that its performance fluctuated: on one episode it may complete the challenge, and on the next episode it may completely fail! \n",
    "\n",
    "Perhaps we can develop a more sophisticated policy to further improve our segway's performance? This is where neural networks come in handy. Given observations as input, we can develop a classification neural network that outputs the probabilities that we pick an action. Given these probabilities, we then sample from a multinomial distribution defined by these probabilities to pick what action to take.\n",
    "\n",
    "In the case of the CartPole, we have two actions that we can take: move left or move right. After feeding our observational data into our neural network. We do some processing to obtain the probabilities of moving left ($p_l$) and right ($p_r$) respectively. Since we have two actions, we can use a $binomial(p_l)$ distribution to sample for which action to take! In the following exercises, we will implement such a categorical policy using TensorFlow.\n",
    "\n",
    "\n",
    "<img src=\"images/policy_nn.png\" width=\"576\" height=\"300\">\n",
    "\n",
    "### Task 3: Implement Multilayer Perceptron\n",
    "\n",
    "In this task, we will define a categorical policy computed via a multilayer perceptron. \n",
    "\n",
    "First, fill in the function `mlp`. `mlp` builds a multilayer perceptron. It takes in as parameter layers, which specifies the number of units per layer in the multilayer perceptron (i.e. if layers=[64,64,2], we have 2 fully connected layers of 64 units, and an output layer of 2 units). It also takes in x, the input tensor, and activation functions that are necessary for the construction of the neural network.\n",
    "\n",
    "Hint: You my find the function `tf.layers.dense` helpful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, layers=[64,64,2], activation=tf.tanh, output_activation=None):\n",
    "    \"\"\"\n",
    "    Builds a multi-layer perceptron in Tensorflow.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor.\n",
    "\n",
    "        layers: Tuple, list, or other iterable giving the number of units\n",
    "            for each layer of the MLP.\n",
    "\n",
    "        activation: Activation function for all layers except last.\n",
    "\n",
    "        output_activation: Activation function for last layer.\n",
    "\n",
    "    Returns:\n",
    "        A TF symbol for the output of an MLP that takes x as an input.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    ### END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Implement Categorical Policy\n",
    "\n",
    "Now that we have a multilayer perceptron that can take in observations as input and compute the log-odds (logits) of our two actions, we want to define a categorical policy that makes use of this MLP to determine our action. Follow the following steps to implement our categorical policy:\n",
    "\n",
    "1) Call `mlp` on the appropriate arguments to obtain a tensor of logits (log-odds) for our actions. Logits are defined as follows:\n",
    "\n",
    "$$logit(p_a) = \\log{\\frac{p_a}{1-p_a}}$$\n",
    "\n",
    "where $p$ is defined as the probability of taking action $a$.\n",
    "\n",
    "2) To convert logits to probabilities, we can take a softmax of the logits. For an action a, the softmax is defined as follows: \n",
    "\n",
    "$$ Softmax(a) = \\frac{e^{l_a}}{\\sum_{a' \\in A} e^{l_a'}} $$\n",
    "\n",
    "where $l_a$ is the logit for action $a$, and $A$ is the set of all actions.\n",
    "<br>In this case, we want to take the log of the softmax because it is easier to compute gradients with log softmaxes. \n",
    "\n",
    "3) Sample from a multinomial distribution to obtain our action.\n",
    "\n",
    "<b> Hint 1: </b> Read up on the functions `tf.nn.log_softmax` and `tf.multinomial`\n",
    "<br><b> Hint 2: </b> The number of actions that we can take is represented as `action_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_categorical_policy(x, a, nn_sizes, activation, output_activation, action_space):\n",
    "    \"\"\"\n",
    "    Builds TF symbols to sample actions and compute log-probabilities of those actions.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of states. Shape [batch, obs_dim].\n",
    "\n",
    "        a: Input tensor of actions. Shape [batch, act_dim].\n",
    "\n",
    "        nn_sizes: Sizes of the layers for action network MLP, excluding the output layer.\n",
    "\n",
    "        activation: Activation function for all layers except last.\n",
    "\n",
    "        output_activation: Activation function for last layer (action layer).\n",
    "\n",
    "        action_space: A gym.spaces object describing the action space of the\n",
    "            environment this agent will interact with.\n",
    "\n",
    "    Returns:\n",
    "        pi: A symbol for sampling stochastic actions from a multinomial distribution\n",
    "\n",
    "        logp: A symbol for computing log-likelihoods of actions from a multinomial distribution.\n",
    "\n",
    "        logp_pi: A symbol for computing log-likelihoods of actions in pi from a multinomial distribution\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ### SOLUTION ###\n",
    "    logits =\n",
    "    logp_all =\n",
    "    sample =\n",
    "    ### END ###\n",
    "    pi = tf.squeeze(sample, axis=1)\n",
    "    logp = tf.reduce_sum(tf.one_hot(a, depth=act_dim) * logp_all, axis=1)\n",
    "    logp_pi = tf.reduce_sum(tf.one_hot(pi, depth=act_dim) * logp_all, axis=1)\n",
    "    return pi, logp, logp_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Run the Segway Experiment again!\n",
    "\n",
    "Below is the starter code to run the CartPole experiment with our more powerful model using a neural network policy. We use an algorithm called Vanilla Policy Gradient (VPG) that takes in your categorical policy and trains on the segway problem. All that is left to do is initialize some of the experiment parameters. We want our categorical policy to be a 3 layer neural network. The first two layers consist of 64 units. Given this information, fill in `nn_units` and `depth` accordingly. Remember that the last layer of the neural network is not defined here, but in the `mlp_categorical_policy` function that you wrote above.\n",
    "\n",
    "Train for 100 epochs and 4000 steps per epoch. The model should take roughly 20-30 min. to train. As you train your RL model, you will be able to see the cartpole training. Pay attention to the metric AverageEpRet––this is the average reward that your model achieved in the epoch. If your code was implemented correctly, AverageEpRet should achieve scores of roughly 200 or higher in the last 10 epochs.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1) What is the AverageEpRet of your model in the final 10 epochs? \n",
    "<br> <i> Answer: </i>\n",
    "\n",
    "\n",
    "2) What do you notice about the CartPole as it is training? Describe its improvement over time.\n",
    "<br> <i> Answer: </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'CartPole-v1'\n",
    "\n",
    "### SOLUTION ###\n",
    "nn_units =\n",
    "depth =\n",
    "steps =\n",
    "epochs =\n",
    "### END ###\n",
    "gamma = 0.99\n",
    "seed = 0\n",
    "# parser.add_argument('--cpu', type=int, default=2)\n",
    "exp_name = 'vpg'\n",
    "\n",
    "# Reset the default graph to prevent errors on multiple runs of Vanilla Policy Gradient\n",
    "tf.reset_default_graph()\n",
    "logger_kwargs = setup_logger_kwargs(exp_name, seed)\n",
    "\n",
    "vpg.run(lambda : gym.make(env), actor_critic=core.mlp_actor_critic,\n",
    "    ac_kwargs=dict(policy=mlp_categorical_policy, nn_sizes=[nn_units]*depth), gamma=gamma, \n",
    "    seed=seed, steps_per_epoch=steps, epochs=epochs,\n",
    "    logger_kwargs=logger_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You're done! Isn't it cool what reinforcement learning can do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
