{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: MDPs and Policy\n",
    "In this part of the project, we will be using OpenAI's gym API for various reinforcement learning exercises.\n",
    "\n",
    "In this part of the project, we will explore each of the following topics:\n",
    "\n",
    "1. Markov Decision Processes (2 Hours)\n",
    "    * States\n",
    "    * Actions\n",
    "    * Discount Factors\n",
    "    * Transition Function\n",
    "    * Reward Function\n",
    "    * Introduction to OpenAI Gym API\n",
    "2. Solving Markov Decision Processes (1 Hour)\n",
    "    * Simple MDP\n",
    "    * Policies\n",
    "    * Value Iteration\n",
    "    * Policy Extraction\n",
    "    * Policy Iteration\n",
    "    * Policy Evaluation\n",
    "3. Reinforcement Learning (2 Hours)\n",
    "    * Online Planning vs Offline Planning\n",
    "    * Online Planning: Model-Based vs Model-Free Approaches\n",
    "    * Model-Free: A Closer Look\n",
    "    * Model-Free Policy Evaluation: Direct Evaluation (Passive RL)\n",
    "    * Model-Free Policy Evaluation: Temporal Difference Learning (Passive RL)\n",
    "    * Model-Free Policy Learning/Updates: Q-Learning (Active RL)\n",
    "    \n",
    "The majority of the time you will be spending on this project will be in writing the various agents to solve the various gym environments. Throughout this notebook, you will be asked to reflect on various demonstrations. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Markov Decision Processes (2 hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP explanation\n",
    "\n",
    "\n",
    "### What is a Markov Decision Process?\n",
    "We can represent the world (like Taxi World and like Chain World) with a model. This model is called a Markov Decision Process. It has a number of components. Using the note, make sure you can answer these questions before moving on.\n",
    "\n",
    "1. What are States?\n",
    "\n",
    "2. What are Actions?\n",
    "\n",
    "3. What are Transition Functions?\n",
    "\n",
    "4. What are Rewards?\n",
    "\n",
    "5. What are Discount Factors?\n",
    "\n",
    "6. What are Q-States, Q-Values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym API\n",
    "The OpenAI Gym API provides an easy way for students and researchers to test out their Reinforcement Learning agents in pre-specified environments. The API also provides a framework to build your own environments to train your agents on. Throughout this project, you will be using the OpenAI Gym API. Here's a quick tutorial: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to OpenAI Gym Through Chain World\n",
    "\n",
    "To create chain world, we built our own environment using the OpenAI Gym API. An environment is its own simulated world where RL agents can explore, learn, and reap rewards. Every environment is defined just as we would define an MDP -- with states, action spaces, slippage probabilities (transition functions), rewards, etc.\n",
    "\n",
    "Please take a few minutes to look over the various environments included in the API at this link: [OpenAI Environments](https://gym.openai.com/envs/#classic_control). \n",
    "\n",
    "**Please carefully follow the code in the cells below and read the comments to get a full picture of the API you will be using throughout this part of the project.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing gym\n",
    "import gym\n",
    "\n",
    "# Installing the chain world environment\n",
    "!pip install -e ./gym-note4-mdp/ -q\n",
    "\n",
    "# Importing chain world environment\n",
    "import gym_note4_mdp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the environment\n",
    "chain_env = gym.make('note4-mdp-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at this environment\n",
    "chain_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `render()` function, you can visualize the chain world environment. This visualization has different parts. The _Environment_ part shows the agent in the environment. The agent is the yellow rectangle.\n",
    "\n",
    "**Let's go over the specifics of the chain world as we are learning the API. Look to the code blocks for specific functions.**\n",
    "\n",
    "### Chain World\n",
    "\n",
    "This is a simple MDP which represents a chain of states. \n",
    "\n",
    "#### States\n",
    "\n",
    "The states are 0, 1, 2, 3, and 4 and they correspond to the position of the agent (the yellow rectangle) in the chain. Initially the agent starts deterministically at State 2. Verify that the setup makes sense by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The env.reset() method reinitializes the env to default values\n",
    "chain_env.reset()\n",
    "chain_env.render()\n",
    "# We can access the environments's current state by using env.state\n",
    "print(f\"Agent's current state: {chain_env.state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "In the chain world, there are three actions that the agent can take:\n",
    "     \n",
    "* (1) forward, which moves the agent forward along the chain\n",
    "* (0) backward, which moves the agent backward along the chain\n",
    "* (2) exit, which tries to exit the world\n",
    "\n",
    "The cell below will show you how to make actions using the `step(action)` function and will take you through one episode where the agent goes forward, backward, backward, backward, and then exits with reward +10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chain_env.reset()\n",
    "chain_env.discount_factor = 1\n",
    "done = False\n",
    "\n",
    "chain_env.slip = 0 # This is the probability of the agent slipping\n",
    "print(\"Frame 1: Initial State\")\n",
    "chain_env.render()\n",
    "\n",
    "state, reward, done, _ = chain_env.step(1) # .step(action) function takes an action on the environment\n",
    "print(f\"\\nFrame 2: Agent moves forward to state {state} and receives reward {reward}\")\n",
    "chain_env.render()\n",
    "\n",
    "state, reward, done, _ = chain_env.step(0) # moves backward\n",
    "print(f\"\\nFrame 3: Agent moves backward to state {state} and receives reward {reward}\")\n",
    "chain_env.render()\n",
    "\n",
    "state, reward, done, _ = chain_env.step(0) # moves backward\n",
    "print(f\"\\nFrame 4: Agent moves backward to state {state} and receives reward {reward}\")\n",
    "chain_env.render()\n",
    "\n",
    "state, reward, done, _ = chain_env.step(0) # moves backward\n",
    "print(f\"\\nFrame 5: Agent moves backward to state {state} and receives reward {reward}\")\n",
    "chain_env.render()\n",
    "\n",
    "state, reward, done, _ = chain_env.step(2) # exits\n",
    "print(f\"\\nFrame 6: Agent moves exit to state {state} (terminal state) and receives reward {reward}\")\n",
    "chain_env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Functions\n",
    "\n",
    "From any state, the agent can try to move fowards (1), move backwards (0), or exit (2). But there is no guarantee that the agent will succeed. You can imagine that chain world is a slippery place. The transitions are not determistic. If you choose to go forwards, you could slip and go backwards with some probability and vice versa. Also, if the agent tries to exit from any state other than the 0 or 4, nothing will happen because that is not valid action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_env.reset()\n",
    "chain_env.slip = 0\n",
    "print(f\"Slippage probability in deterministic world: {chain_env.slip}\")\n",
    "\n",
    "chain_env.reset() # By default the world is uncertain\n",
    "print(f\"Slippage probability in uncertain world: {chain_env.slip}\")\n",
    "\n",
    "\n",
    "# We can also verify the slippage probability experimentally. Convince yourself that 0.2 is the slip rate.\n",
    "slip_counts = []\n",
    "for j in range(10):\n",
    "    slip_count = 0\n",
    "    for i in range(100):\n",
    "        chain_env.reset()\n",
    "        prev_state = chain_env.state\n",
    "        chain_env.step(0) # Move backwards\n",
    "        if chain_env.state >= prev_state:\n",
    "            slip_count += 1\n",
    "    slip_counts.append(slip_count/100)\n",
    "\n",
    "print(f\"Experiment Measured slip rates: {slip_counts}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewards\n",
    "     \n",
    "At the beginning of the chain (State 0) there is a big reward of +10 (B) that the agent will be rewarded if it chooses to exit (Action 2) when in State 0.\n",
    "\n",
    "At the end of the chain (State 4) there is a small reward of +1 (S) that the agent will be rewarded if it chooses to exit (Action 2) when in State 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_env.reset()\n",
    "print(f\"Small Reward: {chain_env.small}\")\n",
    "print(f\"Big Reward: {chain_env.large}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discount Factor\n",
    "    \n",
    "The discount factor is $\\gamma = 0.1$. So as we take steps the rewards are discounted such that $${reward_{t+1}} = {reward_t}*{\\gamma}$$ \n",
    "\n",
    "Observe what happens to the small and large rewards as we take steps in the cells below for various discount factors. Describe what you see in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_env.reset()\n",
    "discount_factors = [0.1, 0.5, 0.9, 0.99]\n",
    "small_rewards = []\n",
    "large_rewards = []\n",
    "for j in range(len(discount_factors)):\n",
    "    discount_factor = discount_factors[j]\n",
    "    chain_env.reset()\n",
    "    chain_env.discount_factor = discount_factor\n",
    "    \n",
    "    smalls = []\n",
    "    larges = []\n",
    "    smalls.append(chain_env.small)\n",
    "    larges.append(chain_env.large)\n",
    "    for i in range(1, 5):\n",
    "        action = chain_env.action_space.sample() #This lets us randomly sample an action\n",
    "        chain_env.step(action)\n",
    "        smalls.append(chain_env.small)\n",
    "        larges.append(chain_env.large)\n",
    "        \n",
    "    small_rewards.append(smalls)\n",
    "    large_rewards.append(larges)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "steps = range(5)\n",
    "for i in small_rewards:\n",
    "    plt.plot(steps, i)\n",
    "\n",
    "plt.title(\"Small Rewards Decaying Over Time with Different Discount Factors\")\n",
    "plt.ylabel(\"Reward Size\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.legend(discount_factors)\n",
    "\n",
    "plt.figure()\n",
    "for i in large_rewards:\n",
    "    plt.plot(steps, i)\n",
    "\n",
    "plt.title(\"Large Rewards Decaying Over Time with Different Discount Factors\")\n",
    "plt.ylabel(\"Reward Size\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.legend(discount_factors)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your understanding #1:** What impact does changing the discount factor have? Why do we need a discount factor in the first place? (Hint: What happens when the discount factor = 1?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your understanding #2:** What would you do if you were the agent in chain world? Think about this question in a general way. For example, what would you do if you were initially in state 4 and the discount factor was 0.1? What would you do if the slippage probability was greater than 0.5? We don't expect rigorous justification here, we just want to get you thinking about _policy_ -- a topic we will be looking at in much greater detail later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi World\n",
    "\n",
    "Now that you've gotten a chance to familiarize yourself with the OpenAI Gym API and the simple chain world environment, let's take a look at a more complicated (albeit still very simple) environment we call Taxi World. Fill in the code when prompted. \n",
    "\n",
    "After looking at the Taxi World in depth, you will be writing a \"dumb agent\" that chooses moves randomly for Taxi World and Chain World that will serve as a baseline for better agents we write later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "import numpy as np\n",
    "!pip install cmake 'gym[atari]' scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the taxi world environment\n",
    "taxi_env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Render the environment using the OpenAI Gym API\n",
    "## Your code here ## (Ans: env.render())\n",
    "taxi_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Taxi World, the agent is a taxi cab. In the visualization, the taxi cab is the yellow rectangle. The Taxi World has walls, represented by | and the taxi agent cannot go through these walls. The Taxi World also has pickup/dropoff locations represented by the letters (R, G, Y, B) in the visualization. \n",
    "\n",
    "In each run of Taxi World, the taxi agent must pickup a passenger at the blue letter location, transport them to the pink letter location, and dropoff the passenger at the pink letter location.\n",
    "\n",
    "When the Taxi has picked up a passenger, its color will change from yellow to blue.\n",
    "\n",
    "Formally, we can represent this as an MDP! Let's specify each component of the underlying MDP.\n",
    "\n",
    "* **States**\n",
    "    * How many states are there in the Taxi World?\n",
    "    * The Taxi World environment has many more states than the Chain World environment. For the chain world, it was relatively easy to see that the chain world had 6 states (0, 1, 2, 3, 4, terminal state), but how would we figure out the number of states in the Taxi World?\n",
    "    * **Let's go through this exercise:**\n",
    "\n",
    "    * How many possible locations are there for the taxi agent?\n",
    "        * **Answer**: The taxi world is a 5x5 grid so there are 25 possible locations.\n",
    "    * For each location of the taxi, how many possible passenger locations are there?\n",
    "        * **Answer**: There are 4 possible passenger locations if the passenger has not yet been picked up and 1 possible passenger location if the passenger has been picked up. This gives us a total of 5 passenger locations.\n",
    "     \n",
    "    * For each location of the taxi and passenger location, how many drop off locations are there?\n",
    "        * **Answer**: There are still 4 possible drop off locations\n",
    "    * How many total states are there? What is the size of the state space?\n",
    "        * **Answer**: So, in total there are $5*5*5*4 = 500$ states in this world! That's a lot more than there were in the chain world!  \n",
    "\n",
    "* **Actions**\n",
    "    * 0: south\n",
    "    * 1: north\n",
    "    * 2: east\n",
    "    * 3: west\n",
    "    * 4: pickup\n",
    "    * 5: dropoff\n",
    "\n",
    "* **Transitions**\n",
    "    * For now, the taxi world is deterministic, so the taxi will go to the next state based solely on the action chosen by the taxi agent.\n",
    "    \n",
    "* **Rewards**\n",
    "    * Movement actions have reward -1.0. A negative reward makes moving costly. This makes sense, because the taxi agent should be incentivized to pickup and dropoff passengers as efficiently as possible.\n",
    "    * Pickup/dropoff actions have reward -10 if the taxi agent is not at a pickup or dropoff location. This is a large penalty.\n",
    "    * At dropoff/pickup locations, dropoff and pickup would have higher rewards (+20), if the agent succeeds in picking up and dropping off the passenger at the correct locations.\n",
    "    \n",
    "* **Discount Factor**\n",
    "    * The discount factor is 1. The rewards do not diminish over time.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise: Random Agents (30 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will be using what you have learned to write code for an agent that moves randomly through the world. The recap section below provides some code that should be helpful. In the cell labeled \"YOUR CODE FOR RANDOM AGENT HERE\", write code for a taxi agent that at each step chooses a random move. While the taxi agent has not dropped off its passenger at the correct dropoff location, continue making random moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recap of API\n",
    "taxi_env.reset()\n",
    "print(f\"Action space: {taxi_env.action_space}\") # Gives you the action space\n",
    "print(\"Size of action space: {taxi_env.action_space.n}\")\n",
    "print(\"Initial State\")\n",
    "taxi_env.render()\n",
    "\n",
    "action = taxi_env.action_space.sample()# Sample an action randomly from the sample space\n",
    "print(f\"\\nMake a random action: {action}\")\n",
    "state, reward, done, _ = taxi_env.step(action) # Take an action\n",
    "print(f\"\\n State: {state}, Reward: {reward}, Done: {done}\")\n",
    "taxi_env.render() # Draw the state\n",
    "\n",
    "# state is the new state\n",
    "# reward is the reward associated with making that action\n",
    "# done is a boolean telling us whether we have reached a terminal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details:\n",
    "\n",
    "In this coding exercise, you will be writing a random agent. While this agent has not reached a terminal state (signified by the variable `done`), it should continue making random moves by sampling from its action space.\n",
    "\n",
    "In addition, keep track of the total reward the agent has incurred and the total number of actions taken by the agent before entering a terminal state.\n",
    "\n",
    "HINT: What does the `env.step` method return?\n",
    "\n",
    "Also, for visualization purposes, we want to keep track of the frames generated by the render method. So, after each action, save the frame rendered of the environment using `frame = env.render(mode=\"ansi\")` and append `frame` to the empty list called `frames`. Here is some code for you to do that:\n",
    "    \n",
    "    frame = env.render(mode='ansi')\n",
    "    frames.append({\n",
    "            'frame': frame,\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'total reward': total_reward\n",
    "            }\n",
    "        )\n",
    "\n",
    "This method should return the frames list, the total action_taken, and the total_reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: YOUR CODE FOR RANDOM AGENT HERE ##\n",
    "def random_agent(env):\n",
    "    frames = []\n",
    "    ## Keep track of actions_taken, and total_reward in your code\n",
    "    actions_taken = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    ## TODO: YOUR CODE HERE ##\n",
    "    # Answer #\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        actions_taken += 1\n",
    "        \n",
    "        frame = env.render(mode='ansi')\n",
    "        frames.append({\n",
    "            'frame': frame,\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'total reward': total_reward\n",
    "            }\n",
    "        )\n",
    "          \n",
    "    ## TODO: END OF YOUR CODE ##\n",
    "        \n",
    "    return frames, actions_taken, total_reward\n",
    "    \n",
    "## Utils Functions to Run and Visualize ## \n",
    "# Source: see reference [1]\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames, t=0.1, time=True):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        if time:\n",
    "            print(f\"Time: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        print(f\"Total Reward: {frame['total reward']}\")\n",
    "        sleep(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_env.reset()\n",
    "frames, actions_taken, total_reward = random_agent(taxi_env)\n",
    "print(\"Actions taken: {}\".format(actions_taken))\n",
    "print(\"Total Reward: {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "Run the code cell below to visualize the random agent in the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hit ctrl-c to interrupt this if you would rather not watch the agent the whole time\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your understanding**: What can you say about the random agent in the Taxi world? About how many actions does the taxi agent take before picking up the passenger for the first time? How many actions does the random agent take before finally stumbling into the terminal state? How much reward does it incurr in total? Do you think we can do better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terrible performance of this random agent should motivate our study of reinforcement learning. The random agents performance in our taxi world will serve as our baseline for future agents we write.\n",
    "\n",
    "For good measure, showcase that the algorithm would work on any environment, run the random agent with the chain world environment in the cell below. Remember to store the results of the random agent method into variables called `frames, actions_taked, total_reward` in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_env.reset()\n",
    "## TODO: CALL RANDOM AGENT ON CHAIN ENV ##\n",
    "# Answer #\n",
    "frames, actions_taken, total_reward = random_agent(chain_env)\n",
    "## TODO: END ##\n",
    "\n",
    "print(\"Actions taken: {}\".format(actions_taken))\n",
    "print(\"Total Reward: {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_frames(frames, t=0.1) ## You can change t if you would like to slow down the visualization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral of this exercise is that we can do better! We can do better by learning from our experience. Before we move on to solving MDPs and Reinforcement Learning, you should have a strong grasp of MDPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Solving MDPs, Policy, Value Iteration, Policy Iteration, Policy Evaluation, and Policy Improvement (2 Hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will take a closer look at the following topics: \n",
    "* Policies\n",
    "* The Bellman Equation\n",
    "    * Values $V^*(s)$\n",
    "    * Q-Value $Q^*(s, a)$\n",
    "* Value Iteration\n",
    "* Policy Extraction\n",
    "* Policy Iteration\n",
    "* Policy Evaluation\n",
    "\n",
    "Each of the exercises you will do in this part will include reviewing the material, implementing the producedure in python, and seeing the results on the chain world and the taxi world. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in studying MDPs was to model the environment. We motivated MDPs by proclaiming that modeling the environment would help us use algorithms to learn the best strategies and optimal behavior. But what does it mean to behave optimally?\n",
    "\n",
    "To behave optimally is to follow the best **policy**. A **policy** is a mapping from states to actions. The policy is like the agent's intelligence, it tells the agent what action to take given a state. \n",
    "\n",
    "Now, it should be clear that our goal is to find the optimal policy. The optimal policy is the policy that will give the agent the maximum expected total reward in the environment. \n",
    "\n",
    "Let's take a look at two policies in the chain world and in the cell below compare the two policies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Exercise: Test out different policies in the MDP provided\n",
    "In the code cell below we have defined two policies, policy1 and policy2. Run the code cell with policy1 first and note how the agent moves through the world while following this policy, also note the total reward this policy yields the agent. Do the same for policy2. Which policy is better? After you have evaluated policy1 and policy2, write your own policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_env.reset()\n",
    "print(f\"discount factor is: {chain_env.discount_factor}\")\n",
    "\n",
    "## COMPARING POLICY CODE ##\n",
    "\n",
    "FORWARD = 1\n",
    "BACKWARD = 0\n",
    "EXIT = 2\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "# Notice how the policy is a mapping from states --> actions\n",
    "policy1 = {0: EXIT, 1: BACKWARD, 2: FORWARD, 3: FORWARD, 4: EXIT, 5:EXIT}\n",
    "policy2 = {0: EXIT, 1: BACKWARD, 2: BACKWARD, 3: BACKWARD, 4: EXIT, 5: EXIT}\n",
    "policy3 = {} ## <- ENTER YOUR POLICY HERE ##\n",
    "\n",
    "chain_env.state = 2 ## <- TODO: CHANGE THE STARTING STATE ##\n",
    "\n",
    "frames = [{'frame': chain_env.render(mode='ansi'),'state': state,'action': action,'reward': reward, 'total reward':0}]\n",
    "\n",
    "#Change the line below to change the policy from 1 to 2 to 3\n",
    "policy = policy1 ## <- TODO: CHANGE THE POLICY ##\n",
    "\n",
    "while not done:\n",
    "    # Make sure the following 3 lines make sense!\n",
    "    current_state = chain_env.state\n",
    "    action = policy[current_state] \n",
    "    state, reward, done, _ = chain_env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    frames.append({'frame': chain_env.render(mode='ansi'),'state': state,'action': action,'reward': reward, 'total reward':total_reward})    \n",
    "## RUN THE CELL BELOW TO VISUALIZE THE AGENT EXECUTING THE POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to visualize\n",
    "print_frames(frames, t=0.6) # You may see the agent not follow the policy! Why? Remember uncertainity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, while it is nice to manually choose polcies or to think about optimal policies and manually implement them as we did above, in difficult problems or in complicated MDPs this is not possible nor efficient.   \n",
    "\n",
    "Let's see how algorithms can come up with optimal policies for our agents to use! To start our exploration, let's derive the bellman equation. The bellman equation forms the corner stone for a lot of what we will be doing. You can also refer to the note for a more in depth explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it is important to understand two quantities: $V^*(s)$ and $Q^*(s, a)$.\n",
    "\n",
    "1. $V^*(s)$ is the expected value of the reward that an optimally behaving agent starting in state s will receive over its entire lifetime.[2]  $V^*(s)$ is the maximum reward an agent following an optimal **policy** will achieve over its lifetime.\n",
    "\n",
    "2. $Q^*(s, a)$ is the expected value of the reward that an optimally behaving agent starting in q-state (s,a) will receive over its entire lifetime. $Q^*(s, a)$ is the maximum reward an agent following an optimal **policy** will achieve over its lifetime given that it started in state s and has taken action a.\n",
    "\n",
    "The Bellman Equation provides an equation for $V^*(s)$ and $Q^*(s, a)$ in terms of simpler quantities. Let's derive this equation.\n",
    "\n",
    "If you were an agent and you knew $V^*(s)$ for all states, and you were in a state $s$, how would you choose which action to take?\n",
    "\n",
    "If you were reward maximizing, you would take the action that leads you to the state  $s'$ with the maximum $V^*(s')$. Why? Because, by definition this would maximize the total reward you would receive if you continued acting optimally.\n",
    "\n",
    "But, remember, there is uncertainty in the MDP, so each action comes with a probability that you will transition to any state, namely, $T(s, a, s')$ so you would like to choose the action that maximizes your _expected_ future reward. \n",
    "\n",
    "Let's put this into a mathematical equation. This is the Bellman Equation.\n",
    "\n",
    " $$V^*(s) = \\max_{{a \\in actions}}{\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V^*(s')]}$$\n",
    " \n",
    " Notice that this equation exactly follows our thinking. The optimal value of a state is the probability weighted reward the agent receives in transitioning from state s to s' and the maximum expected optimal value of the next state s' (discounted by gamma). This is a beautifully simple equation, that decomposes the quantity $V^*(s)$ into subproblems $V^*(s')$.\n",
    " \n",
    " Similarly, $$Q^*(s, a) = {\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V^*(s')]}$$ since in a q-state we are given an action, so there is no maximization over the actions.\n",
    " \n",
    " Putting the two equations together we have, \n",
    "     $$V^*(s) = \\max_{{a \\in actions}}Q^*(s, a)$$\n",
    "     \n",
    "While these elegant equations put represent the optimal values and q-values of states in terms of other states' optimal values and q-values, how would we go about solving for the optimal values and q-values? Enter value iteration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Bellman equation will only be satisfied if for every state $V(s) = V^*(s)$ where $V(s)$ is the value of a state s and $V^*(s)$ is the optimal value of a state s.\n",
    "\n",
    "In other words the bellman equation does not hold if the values that we have assigned to a state do not match the optimal values. Because of this feature, we can use the bellman equation to tell us whether our values are optimal.\n",
    "\n",
    "The idea of value iteration, is to iteratively improve our estimates of the values of each state until we have learned the true optimal values. The key subproblem that we will use to solve for $V^*(s)$ is $V_k(s)$.\n",
    "\n",
    "$V_k(s)$ is the expected value of the reward that an optimally behaving agent starting in state s will receive over the next k timesteps. Remember, $V^*(s)$ is the expected value of the reward that an optimally behaving agent starting in state s will receive over its entire lifetime.\n",
    "\n",
    "When, $V_k(s) = V_{k+1}(s) = ... = V_{k+i}(s) = ... = V^*(s)$ the value for a state will have converged and we will have found the optimal value of a state s.\n",
    "\n",
    "By expanding our time horizon (increasing k) until convergence we can iteratively find $V^*(s)$. What does the algorithm look like?\n",
    "\n",
    "1. For all states, initialize $V_0(s) = 0$.\n",
    "2. Repeat the following while $V_k(s) \\neq V_{k+1}(s)$,\n",
    "    $$V_{k+1}(s) \\leftarrow \\max_{{a \\in actions}}{\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V_k(s')]} $$\n",
    "    \n",
    "Let's implement this for our chain world!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful functions\n",
    "\n",
    "There is one last part of the OpenAI gym API that we haven't showed you yet, the MDP table. The MDP table stores all the information about the environment in an accessible way. A demo on how to access important information follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing The MDP table\n",
    "print(chain_env.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Woah that's a lot\n",
    "# The MDP table stores the following information for each state:\n",
    "# {action1: [(probability1, nextstate1, reward1, done1), (probability2, nextstate2, reward2, done2)], \n",
    "# action2: [(probability, nextstate, reward, done)], etc}\n",
    "\n",
    "# This the the entry for state 1.\n",
    "chain_env.P[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first row tells us what would happen if we took action 0 (backwards) from this state,\n",
    "# The second row tells us what would happen if we took action 1 (forwards) from this state,\n",
    "# The third row tells us what would happen if we took action 2 (exit) from this state\n",
    "\n",
    "# Let's look at what the first row is telling us.\n",
    "state = 1\n",
    "action = 0\n",
    "print(chain_env.P[state][action])\n",
    "# > [(0.8, 0, 0, False), (0.2, 2, 0, False)]\n",
    "\n",
    "# The first entry tells us that there is a 0.8 probability of transitioning to nextstate 0, receiving reward 0, and not entering a terminal state\n",
    "# The second entry tells us that there is a 0.2 probability of transitioning to nextstate 2, receving reward 0, and not entering a terminal state\n",
    "# This should make sense to you. Remember the probability of slipping is 0.2.\n",
    "\n",
    "# This is useful because it gives us the T(s,a,s') and R(s,a,s') for every state and action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise: Implementing Value Iteration\n",
    "\n",
    "In this coding exercise, you will be implementing value iteration for the chain world.\n",
    "\n",
    "First, instantiate a table of zeroes called v_table of size num_iters by num_states (env.observation_space.n). The kth row of this table should contain the $V_k(s)$ values. \n",
    "\n",
    "|  $k$        | $V_k(0)$    | $V_k(1)$      | $V_k(2)$    | ... |\n",
    "| :---        |    :----:   |          ---: |     ---:    | ---:|\n",
    "|   0         |             |               |             |     |\n",
    "|   1         |             |               |             |     |\n",
    "|   2         |             |               |             |     |\n",
    "|   ...       |             |               |             |     |\n",
    "|   num_iters         |             |               |             |     |\n",
    "\n",
    "\n",
    "Loop through possible actions and next states (s') and use the update rule to fill in the k+1th row of the table using the kth row of the table. This should follow from the update rule: \n",
    "$$V_{k+1}(s) \\leftarrow \\max_{{a \\in actions}}{\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V_k(s')]} $$\n",
    "\n",
    "or, in tabular form/pseudocode,\n",
    "\n",
    "`\n",
    "new_value_for_state = max(sum(transition_prob*(reward + discount*old_value_for_nextstate)\n",
    "`\n",
    "\n",
    "Remember you can get the transaction probability and the reward from `env.P` and can get the old values of the state from the value table as you create it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, num_iters):\n",
    "    ## This is necessary because taxi environment has no discount factor\n",
    "    try:\n",
    "        discount_factor = env.discount_factor\n",
    "    except:\n",
    "        discount_factor = 1\n",
    "        \n",
    "    # TODO: YOUR CODE HERE #\n",
    "    # Answer #\n",
    "    v_table = np.zeros([num_iters+1, env.observation_space.n])\n",
    "    \n",
    "    for k in range(0, num_iters):\n",
    "        for state in range(env.observation_space.n):\n",
    "            reward_for_action = []\n",
    "            \n",
    "            for action in range(env.action_space.n):\n",
    "                reward_for_state = 0\n",
    "                \n",
    "                for entry in range(len(env.P[state][action])):\n",
    "                    transition_probability, next_state, reward, done = env.P[state][action][entry]\n",
    "                    reward_for_state += transition_probability*(reward + discount_factor*v_table[k][next_state])\n",
    "                \n",
    "                reward_for_action.append(reward_for_state)\n",
    "                \n",
    "            v_table[k+1][state] = np.max(reward_for_action)\n",
    "    \n",
    "    # TODO: END OF YOUR CODE #\n",
    "    return v_table, num_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize convergence of values for different number of iterations. Run the cell below and change the number of iterations to see the convergence of value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = chain_env ## You can change this to taxi_env as well, but the taxi env is a larger environment so it will take more iterations.\n",
    "num_iters = 5 ## Change this value\n",
    "v_table, num_iters = value_iteration(env, num_iters)\n",
    "env.reset()\n",
    "\n",
    "def print_value_table(v_table, num_iters):\n",
    "    for i in range(len(v_table[-1])):\n",
    "        if i == 5:\n",
    "            print(f\"Value for terminal state after {num_iters} iterations = {v_table[-1][i]}\")\n",
    "        else:\n",
    "            print(f\"Value for State {i} after {num_iters} iterations = {v_table[-1][i]}\")\n",
    "\n",
    "def plot_convergence(v_table, num_iters):\n",
    "    plt.figure(figsize = (10, 4))\n",
    "    advantage = [np.sum([np.abs(v_table[i][j] - v_table[i-1][j]) for j in range(len(v_table[i]))]) for i in range(0, num_iters+1)]\n",
    "    plt.plot(range(0, num_iters+1), advantage)\n",
    "    plt.title(\"Convergence of Values in Value Iteration\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.ylabel(\"Difference between values\")\n",
    "\n",
    "# print_value_table(v_table, num_iters)\n",
    "plot_convergence(v_table, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the optimal values of each state for both taxi world and chain world, we would like to use these values to generate a policy. \n",
    "\n",
    "Remember a policy is like the agent's intelligence, given a state the policy will tell the agent what to do (what action to take). \n",
    "\n",
    "How would we extract a policy from the value table? With **policy extraction**, of course!\n",
    "\n",
    "**Policy extraction** is a method which extracts the actions that make up our policy.\n",
    "\n",
    "Again, let's logically reason through this. Given that you are in a state s, which action should you take? You should take the action which gives you the maximum expected total reward. Well, which action gives you the maximum expected total reward? If you remember the definition of a q-state from earlier. The action a that maximizes q(current state, a) will give you the biggest expected total utility. Going back to the bellman equation, this is: \n",
    "\n",
    "$$\\pi^*(s) = \\text{argmax}_{{a \\in actions}}{\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V^*(s')]}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: YOUR CODE HERE ##\n",
    "def policy_extraction(env, v_star):\n",
    "    '''v_star is an array with the optimal values of the states. It would be the last row of v_table.'''\n",
    "    # Fill in this dictionary with (state, action) pairs\n",
    "    policy = {}\n",
    "    \n",
    "    # TODO: START YOUR CODE HERE\n",
    "    # Answer\n",
    "    for state in range(env.observation_space.n):\n",
    "        reward_for_action = []\n",
    "        for action in range(env.action_space.n):\n",
    "            reward_for_state = 0\n",
    "            for entry in range(len(env.P[state][action])):\n",
    "                transition_probability, next_state, reward, done = env.P[state][action][entry]\n",
    "                reward_for_state += transition_probability*(reward + discount_factor*v_star[next_state])\n",
    "            reward_for_action.append(reward_for_state) \n",
    "\n",
    "        policy[state] = np.argmax(np.asarray(reward_for_action))\n",
    "    \n",
    "    ## TODO: END YOUR CODE\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see if your policy makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = chain_env\n",
    "env.reset()\n",
    "num_iters = 25\n",
    "v_table, _ = value_iteration(env, num_iters) # The best values are in the last row of the v_table\n",
    "\n",
    "def print_policy(env, policy):\n",
    "    if env.observation_space.n > 10:\n",
    "        print(\"We only print policies for chain world. If you want to, you can write your own code to print policy for taxi env.\")\n",
    "        return\n",
    "        \n",
    "    word = {0: \"BACKWARDS\", 1: \"FORWARDS\", 2: \"EXIT\"}\n",
    "    for state in policy:\n",
    "        if state != 5:\n",
    "            action = policy[state]\n",
    "            action = word[action]\n",
    "            print(f\"From state {state} take action {action} to maximize reward\")\n",
    "\n",
    "policy = policy_extraction(env, v_table[-1])\n",
    "print_policy(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to visualize the optimal policy that we learned through policy extraction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE POLICY IN ACTION\n",
    "env = chain_env\n",
    "env.reset()\n",
    "num_iters = 25\n",
    "v_table, _ = value_iteration(env, num_iters) \n",
    "policy = policy_extraction(env, v_table[-1])\n",
    "total_reward = 0\n",
    "# print_policy(env, policy)\n",
    "\n",
    "frames = [{'frame': env.render(mode='ansi'),'state': env.s,'action': 0,'reward': 0, 'total reward':0}]\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # Make sure the following 3 lines make sense!\n",
    "    current_state = env.s\n",
    "    action = policy[current_state] \n",
    "    state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    frames.append({'frame': env.render(mode='ansi'),'state': state,'action': action,'reward': reward, 'total reward':total_reward})    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_frames(frames, t=0.5) #Verify that this policy is optimal! Run the cell above and this cell a couple of times to \n",
    "# verify that even when the agent slips it makes the correct decisions afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this section we extracted a policy from values we learned through value iteration. But what if we could learn our policy directly without learning the values first? Let's see how we could do this in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of value iteration is that it can be very slow. We have to learn all the values before we can learn the policy. **Policy iteration** improves on value iteration because we work directly to improve some initial policy until our policy converges to the optimal policy.\n",
    "\n",
    "Here are the basic steps of the policy iteration algorithm.\n",
    "\n",
    "1. Initialize the policy to an arbitrary policy. Remember a policy is simply a mapping between states and actions.\n",
    "\n",
    "2. Repeat the following:\n",
    "    * Evaluate the policy (**policy evaluation**) by computing the expected total reward of starting in state s and following policy $\\pi$. This is represented by the following equation: $$V^\\pi(s) = {\\sum_{s'}T(s, \\pi(s), s')[R(s, \\pi(s), s') + \\gamma*V^\\pi(s')]}$$\n",
    "        * We can compute this by iteratively updating until $V^\\pi(s)$ converges. $$V^\\pi_{k+1}(s) = {\\sum_{s'}T(s, \\pi_i(s), s')[R(s, \\pi_i(s), s') + \\gamma*V^{\\pi_i}_k(s')]}$$\n",
    "    * Then improve the policy (**policy improvement**) by running the policy extraction routine on the values of the states generated as a result of policy evaluation.\n",
    "        * $$\\pi_{i+1}(s) = \\text{argmax}_{{a \\in actions}}{\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V^{\\pi_i}(s')]}$$\n",
    "    * We can continue this routine until the policy converges and the policy at time step i is the same as the policy in time step \n",
    "    \n",
    "Implement the policy iteration algorithm below. Be sure to reference earlier parts, and try not to repeat yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, value_convergence_threshold=1e-04):\n",
    "    ## TODO: IMPLEMENT POLICY ITERATION FOR CHAIN ENV ##\n",
    "\n",
    "    initial_policy = get_initial_policy(env) #This method just gives you a random policy\n",
    "    policy = initial_policy\n",
    "    num_iters_before_convergence = 0\n",
    "    \n",
    "    # ANSWER #\n",
    "    policies_converged = False\n",
    "    \n",
    "    while not policies_converged:\n",
    "        num_iters_before_convergence += 1\n",
    "        v_table = np.zeros([100, env.observation_space.n])\n",
    "        \n",
    "        values_converged = False\n",
    "        k = 0\n",
    "        while not values_converged:\n",
    "            for state in range(env.observation_space.n):\n",
    "                reward_for_action = 0\n",
    "            \n",
    "                action = policy[state]\n",
    "                \n",
    "                for entry in range(len(env.P[state][action])):\n",
    "                    transition_probability, next_state, reward, done = env.P[state][action][entry]\n",
    "                    reward_for_action += transition_probability*(reward + env.discount_factor*v_table[k][next_state])\n",
    "                \n",
    "                v_table[k+1][state] = reward_for_action\n",
    "            \n",
    "                \n",
    "            if np.all([np.abs(v_table[k+1][state] - v_table[k][state]) < value_convergence_threshold for state in range(env.observation_space.n)]):\n",
    "                values_converged = True\n",
    "            else:\n",
    "                k += 1\n",
    "        \n",
    "        new_policy = policy_extraction(env, v_table[k+1])\n",
    "        \n",
    "        policies_converged = is_same_policy(policy, new_policy)\n",
    "        policy = new_policy\n",
    "        \n",
    "    #TODO: END YOUR CODE #    \n",
    "    return policy, initial_policy, num_iters_before_convergence\n",
    "\n",
    "\n",
    "### Useful Utils ###\n",
    "def get_initial_policy(env, stable=False):\n",
    "    # This will return an arbitrarily chosen initial policy\n",
    "    if stable:\n",
    "        return {0: 2, 1: 1, 2: 1, 3: 1, 4: 2, 5: 0}\n",
    "    return {0: env.action_space.sample(), 1: env.action_space.sample(), 2: env.action_space.sample(), 3: env.action_space.sample(), 4: env.action_space.sample(), 5: env.action_space.sample()}\n",
    "    \n",
    "def is_same_policy(policy1, policy2):\n",
    "    for state in policy1:\n",
    "        if policy1[state] != policy2[state]:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "policy, initial_policy, num_iters_before_convergence = policy_iteration(chain_env)\n",
    "print(\" Initial Policy:\")\n",
    "print_policy(chain_env, initial_policy)\n",
    "print(\"\\n Optimal Policy:\")\n",
    "print_policy(chain_env, policy)\n",
    "print(f\"\\n Number of Iterations in Policy Iteration: {num_iters_before_convergence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code block above multiple times, and you have implemented policy iteration correctly, you should see that no matter the starting policy, policy iteration will learn the optimal policy we uncovered in the previous part in a few iterations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Reinforcement Learning! (2 Hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you should have experience writing random moving agents, working with OpenAI gym API, and solving MDPs using value iteration, and learning optimal policies by hand and by using value iteration with policy extraction and policy iteration on the simple chain world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline vs Online Planning\n",
    "\n",
    "What if we don't know the underlying MDP? Everything we have done so far assumes that the the agent has complete knowledge of both the transition function and the reward function. (We were directly accessing `env.P` the MDP table). This is called **offline planning** because the agent can learn an optimal policy without ever having to explore the world.\n",
    "\n",
    "The assumption that the agent has knowledge of the reward and transition function is not reasonable in the real world and in many environments. In situations where the agents does not know the transition of reward function, the agent must **explore** the environment to learn the environment's intrinsics. This is called **online planning** and it will be the focus of the remainder of this part of the project.\n",
    "\n",
    "There are two broad distinctions within online planning. The first category of algorithms is called **model based** and the second category is called **model free**. Let's take a closer look at both of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Planning: Model-Based\n",
    "\n",
    "In **model-based** learning algorithms, the agent tries to learn the underlying MDP through exploration. Essentially, the agent will simply maintain counts of the transitions and rewards it receives to build an approximation of the transition and reward functions. In other words, through exploration the agent will try to build the MDP table `env.P`.\n",
    "\n",
    "This is a rather simple learning algorithm and while it is effective, it is rather boring, since you would just be essentially letting the random agent move through the environment and keeping counts of the rewards and transitions as the agent explores and then normalizing these counts at the end of learning. Then, the underlying MDP could be solved using value iteration and policy extraction or policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Planning: Model-Free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **model-free** learning, the goal is to learn the policy directly without first estimating the intrinsics of the MDP (transition and reward functions). Let's first see how we can evaluate policies (**policy evaluation**) in a model-free learning scenario using **Direct Evaluation** and **Temporal Difference Learning**. Then, we will see how to learn the optimal policy through learning q-values in q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Free: Direct Evaluation\n",
    "\n",
    "In **Direct Evaluation** the goal is to evaluate a policy by following it for some number of **episodes**. An **episode** is a single run on the environment from intitial state to terminal state. As the agent goes through multiple episodes of following the policy, it maintains counts of the total reward it received from each each state (in all future steps after that state) and a count for how many times each state was visited. \n",
    "\n",
    "From the total reward received from each state and the number of times the state was visited, the agent can estimate $V^\\pi(s)$.\n",
    "\n",
    "Specifically, $V^\\pi(s) = \\frac{\\text{total reward}}{\\text{times state \n",
    "visited}}$\n",
    "\n",
    "Let's create a table that looks like the the table below and populate it with the results of running the agent on the environment following the specified policy.\n",
    "\n",
    "| State       | Total Reward| Times Visited |  $V^\\pi(s)$ |\n",
    "| :---        |    :----:   |          ---: |     ---:    |\n",
    "|   0         |             |               |             |\n",
    "|   1         |             |               |             |\n",
    "|   2         |             |               |             |\n",
    "|   3         |             |               |             |\n",
    "|   4         |             |               |             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_evaluation(env, policy, episodes=10):\n",
    "    table = np.zeros((env.observation_space.n, 3))\n",
    "    TOTAL_REWARD_COL = 0\n",
    "    TIMES_VISITED_COL = 1\n",
    "    VALUES_COL = 2\n",
    "    \n",
    "    # TODO: POPULATE THE TABLE BY RUNNING DIRECT EVALUATION\n",
    "    # ASSUME you can access the action from the policy using \"policy[state]\"\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        env.reset()\n",
    "        states_visited = []\n",
    "        done = False\n",
    "        \n",
    "        next_state = env.state\n",
    "        while not done:\n",
    "            current_state = next_state\n",
    "            states_visited.append(current_state)\n",
    "            table[current_state][TIMES_VISITED_COL] += 1\n",
    "        \n",
    "            action = policy[current_state]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            for s in states_visited:\n",
    "                table[s][TOTAL_REWARD_COL] += reward\n",
    "    \n",
    "    for state in table:\n",
    "        state[VALUES_COL] = float(state[TOTAL_REWARD_COL])/max(0.0000001, state[TIMES_VISITED_COL])\n",
    "    \n",
    "    # TODO: END OF CODE #\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize\n",
    "env = chain_env\n",
    "env.reset()\n",
    "random_policy = get_initial_policy(env, stable=True)\n",
    "table = direct_evaluation(env, random_policy)\n",
    "\n",
    "def print_direct_eval_table(table):\n",
    "    i = 0\n",
    "    print(\"Direct Evaluation Table:\")\n",
    "    print(\"{:<8} {:<25} {:<15} {:<10}\".format(\"state\", \"total reward\", \"times visited\", \"value\"))\n",
    "    for state in table:\n",
    "        print(\"{:<8} {:<25} {:<15} {:<10}\".format(i, state[0], state[1], state[2]))\n",
    "        i+=1\n",
    "print(\"Policy:\")\n",
    "print_policy(env, random_policy)\n",
    "print(\"\")\n",
    "print_direct_eval_table(table)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is one way to do policy evaluation without a model. Remember, we can always use policy evaluation along with policy improvement to iteratively improve our model. Let's take a look at another way to do model free policy evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Free: Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While, Direct Evaluation was very simple to understand and implement, there are some inconsistencies in the logic behind direct evaluation. Firstly, direct evaluation does not learn as it follows the policy, it simply collects information, and then, after reaching a terminal state, in combines all of this information into something it can learn from. While this model can work, in practice it would take much too long. Why don't we learn after each move?\n",
    "\n",
    "This is where temporal difference learning comes in. In temporal difference learning, the agent has a running estimate of each state's optimal value which it updates using new samples as it collects them. \n",
    "\n",
    "Let's see the procedure the agent follows and then we'll take it apart before implementing it.\n",
    "\n",
    "1. For all states s, set $V^\\pi(s) = 0$ to start.\n",
    "2. While the number of episodes of exploration have not finished:\n",
    "    * According to the agent's policy $\\pi$, take an action and receive reward $R(s, a, s')$.\n",
    "    * With this reward, construct a sample: $\\text{sample} = R(s, a, s') + \\gamma*V^\\pi(s')$, this sample is a new estimate of $V^\\pi(s)$.\n",
    "    * Update $V^\\pi(s)$ using the update rule: $$V^\\pi(s) \\leftarrow (1 - \\alpha)*V^\\pi(s) + \\alpha*\\text{sample}.$$\n",
    "    * As you progress, shrink $\\alpha$ as a function of the number of times we enter a state s'. (You do not have to implement this step.)\n",
    "    * If you reach a terminal state, increment the number of episodes, reset the environment, and continue learning.\n",
    "    \n",
    "The most important aspect of this is the update rule. The update rule captures the sentiment that motivated us to move away from direct evaluation. We are now learning as we are taking actions. The update rule also answers the question of how to incorporate our new sample into our running estimate. According to the update rule, we use a learning rate $\\alpha$ to guide how much weight to give the new sample in our estimate. This should make sense because as we learn, our estimate should get better, so we should give more weight to newer samples. Try unravelling the recursion in the update rule for a few steps so you can see how past estimates are given less and less weight in our overall estimate. \n",
    "\n",
    "In the cell below, implement temporal difference learning. You are given an environment, a policy, a learning rate alpha, and the number of episodes of learning to do. If you would like to make more sense of your result. Set env.discount_factor = gamma and env.slip = slip, after each time you reset your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference_learning(env, policy, alpha, gamma=1, slip=0, episodes=10):\n",
    "    # TODO: IMPLEMENT TEMPORAL DIFFERENCE LEARNING #\n",
    "    v_pi = np.zeros([episodes, env.observation_space.n])\n",
    "    \n",
    "    \n",
    "    for episode in range(1, episodes):\n",
    "        env.reset()\n",
    "        env.discount_factor = gamma\n",
    "        env.slip = slip\n",
    "        state = env.s\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action) \n",
    "\n",
    "            old_value_state = v_pi[episode - 1][state]\n",
    "            value_next_state = v_pi[episode - 1][next_state]\n",
    "            \n",
    "            sample = (reward + env.discount_factor * value_next_state)\n",
    "\n",
    "            new_value_state = (1 - alpha) * old_value_state + alpha * sample \n",
    "            v_pi[episode][state] = new_value_state\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "    return v_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the table\n",
    "def print_temporal_difference_table(table, num_rows):\n",
    "    i = 0\n",
    "    total = len(table)\n",
    "    table = table[-num_rows:]\n",
    "    print(\"Temporal Difference Table:\")\n",
    "    print(\"{:<8} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\"episode\", \"state 0\", \"state 1\", \"state 2\", \"state 3\", \"state 4\", \"state 5\"))\n",
    "    for episode in table:\n",
    "        print(\"{:<8} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format((total - num_rows + 1) + i, round(table[i][0], 8), round(table[i][1], 8), round(table[i][2], 8), round(table[i][3], 8), round(table[i][4], 8), round(table[i][5], 8)))\n",
    "        i+=1\n",
    "        \n",
    "chain_env.reset()      \n",
    "alpha = 0.75\n",
    "episodes = 1000\n",
    "table = temporal_difference_learning(chain_env, get_initial_policy(chain_env, stable=True), alpha, gamma=1, slip=0, episodes=episodes)\n",
    "print_temporal_difference_table(table, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above should show the last few rows of temporal difference learning algorithm. Try changing alpha, what happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Free: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal Difference learning and Direct Evaluation are great ways to evaluate the policy we have learned, but what about learning the policy directly through exploration. This is where we introduce Q-Learning. \n",
    "\n",
    "Q-Learning is an algorithm that learns a policy through learning the q-values of each state. Like temporal difference learning, q-learning takes samples and incorporates the samples into the running estimates of the q-states using $alpha$, the learning rate.\n",
    "\n",
    "Once we have learned the q-values, extracting the policy is extremely easy. From each state s, take the action a that maximizes $Q(s, a)$. This is a simple argmax. As we have been doing, we will build a lookup table of q-values that we will iteratively improve as we explore the world. But this brings up a question, there is no policy that q-learning follows, how will the agent explore the world?\n",
    "\n",
    "If you remember the random agent from part 1 of this part of the project, the random agent sampled actions randomly, q-learning will take some proportion of random actions to **explore** the world. The q-learning agent will also **exploit** its estimate of the optimal policy some proportion of the time by finding the action that maximizes the q-value from the state it is currently in using the running estimates of the q-value. This balance between **exploration** and **exploitation** is called an $\\epsilon$-greedy strategy, because the q-learning agent explores (takes random actions) some $0 < \\epsilon < 1$ proportion of the time.\n",
    "\n",
    "Here is the q-learning algorithm in all its glory.\n",
    "\n",
    "1. Initialize the Q-Table to all zeroes.\n",
    "2. For some number of episodes,\n",
    "    * While the episode is not over (a terminal state is not reached):\n",
    "        * Take a random action with probability $\\epsilon$\n",
    "        * Otherwise take an action according to the current estimates using: $$a = \\text{argmax}_{a\\in actions}{Q(s, a)}$$ This equates to saying for all possible actions from state s, select the one with the highest Q-value.\n",
    "        * Travel to the next state $s'$ as a result of the action.\n",
    "        * For all possible actions from state $s'$ select the one with the highest q-value.\n",
    "        * Update the Q-table using the update rule: $$ Q(s, a) \\leftarrow (1-\\alpha)*Q(s, a) + \\alpha*(R(s,a,s') + \\gamma*\\max_{a}{Q(s', a)})$$\n",
    "        * Set the next state as the current state, and continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def q_learning_agent(env, alpha=0.1, gamma=0.6, epsilon=0.1):\n",
    "\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    \n",
    "    ## TODO: YOUR CODE HERE ##\n",
    "    # Answer #\n",
    "    for i in range(1, 100001):\n",
    "        env.reset()\n",
    "        state = env.s\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample() \n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) \n",
    "\n",
    "            next_state, reward, done, _ = env.step(action) \n",
    "\n",
    "            next_value = np.max(q_table[next_state])\n",
    "            old_state_value = q_table[state, action]\n",
    "\n",
    "            q_table[state, action] = (1 - alpha) * old_state_value + alpha * (reward + gamma*next_value)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "    # TODO: END OF YOUR CODE #\n",
    "    return q_table\n",
    "\n",
    "taxi_env = gym.make(\"Taxi-v3\").env\n",
    "q_table = q_learning_agent(taxi_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let the agent exploit its policy. ##\n",
    "episodes = 10 # You can change the number of episodes if you'd like to admire your agent for more or less time.\n",
    "frames = []\n",
    "\n",
    "for _ in range(episodes):\n",
    "    env.reset()\n",
    "    state = env.s\n",
    "    reward = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        frame = env.render(mode='ansi')\n",
    "        frames.append({\n",
    "            'frame': frame,\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'total reward': total_reward\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize the q-learning agent.\n",
    "print_frames(frames, t=0.2, time=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! This is awesome! You should be able to see the q-learning agent initialize in a random state and quickly move toward the pickup location to pickup the passenger and then quickly move to the drop off location to dropoff the passenger before being reinitialized! Hopefully this project was a fun exercise to see the power of RL in some simple examples. In the next part of the project, we'll be going through some 2D physics RL examples with continuous observation spaces! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] https://medium.com/@apoddar573/making-your-own-custom-environment-in-gym-c3b65ff8cdaa\n",
    "\n",
    "[2] https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "\n",
    "[3] https://inst.eecs.berkeley.edu/~cs188/fa20/assets/notes/note04.pdf\n",
    "\n",
    "[4] https://inst.eecs.berkeley.edu/~cs188/fa20/assets/notes/note05.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
