{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Walkthrough (No Coding)\n",
    "In this part of the project, we will be using OpenAI's gym API for various reinforcement learning exercises.\n",
    "\n",
    "In this part of the project, we will explore each of the following topics:\n",
    "\n",
    "1. **Control, Agents, and Expanding on EECS16A/B** (1 Hour)\n",
    "    * The Control Problem\n",
    "    * States, Actions, Rewards, Values\n",
    "    * Reinforcement Learning\n",
    "    * Introduction to OpenAI Gym API\n",
    "2. **Looking at Some Action-Value Policy Approaches**(1 Hour)\n",
    "    * Simple World\n",
    "    * Taxi World\n",
    "    * Random-Policy Agent\n",
    "    * Q-Learning Agent\n",
    " \n",
    "4. **Optional: Extra for Experts**    \n",
    "    * Value Iteration\n",
    "    * Policy Extraction\n",
    "    * Policy Iteration\n",
    "    * Policy Evaluation\n",
    "    \n",
    "The majority of the time you will be spending on this project will be in visualizing different agents solve environments. Throughout this notebook, you will be asked to reflect on various demonstrations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Control, Agents, and Expanding on EECS 16A/B (1 hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Control Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In EECS 16A/B, we looked at ideas of control in building the robotic car. In that classic EECS 16AB problem, we implemented a controller for a robotic car that kept the car on a straight path despite differences in the motors and turned correctly each time. \n",
    "\n",
    "In a lot of situations, we would like to implement controllers for complex systems like robots and autonomous systems. Using Reinforcement Learning, we can implement decision making algorithms and controllers for complex systems by allowing the agent (the decision maker) to explore and learn how to act in different situations.\n",
    "\n",
    "In this problem, we will introduce reinforcement learning using the OpenAI Gym API, an API that lets us easily train, visualize, and test learning agents in different environments. The majority of this problem will be providing a background for reinforcement learning without diving too deeply into the details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents, States, Actions, Rewards, and Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the OpenAI Gym API and visualizing reinforcement learning agents, lets review some terms you've likely seen in other projects you've done this semester. \n",
    "\n",
    "In previous sections on search, minimax, and so forth, you have learned about agents. To refresh your memory, the <b> agent is the decision-maker </b>. Our goal in Reinforcement Learning is to train the agent to learn the optimal behavior. The agent exists in the environment, and at any one time step (let's consider discrete time for now),  the environment exists in a <b>state</b>.\n",
    "\n",
    "Recall the idea of the state-space that you've seen earlier. Here, we represent states as a vector of the relevant attributes of the system or environment.\n",
    "\n",
    "Now, instead of implementing control loops or complex sets of rules telling the agent what to do, we want to let the agent explore the environment by moving from state to state to learn how to act. The agent moves from state to state in the environment by taking <b>actions</b>. The agent chooses which actions to take based on rewards.\n",
    "\n",
    "Rewards are a key idea. The agent receives a <b>reward</b> when it takes an action. We can structure the rewards in such a way that the agent will learn the behavior we want it to learn. So going back to the robotic car, if we wanted the robotic car to learn how to go straight, we would give it a high reward if it chose actions that made the car go straight. Using this methodology, we can incentivize the agent to learn how to act to maximize the reward it achieves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at two environments using the OpenAI Gym API, and demonstrate the power of reinforcement learning. Read the cells below, and run the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym API\n",
    "The OpenAI Gym API provides an easy way for students and researchers to test out their Reinforcement Learning agents in pre-specified environments. The API also provides a framework to build your own environments to train your agents on. Throughout this project, you will be using the OpenAI Gym API. Here's a quick tutorial: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to OpenAI Gym Through Chain World\n",
    "\n",
    "To create chain world, we built our own environment using the OpenAI Gym API. An environment is its own simulated world where RL agents can explore, learn, and reap rewards. Every environment is defined just as we would define an MDP -- with states, action spaces, slippage probabilities (transition functions), rewards, etc.\n",
    "\n",
    "Please take a few minutes to look over the various environments included in the API at this link: [OpenAI Environments](https://gym.openai.com/envs/#classic_control). \n",
    "\n",
    "**Please carefully follow the code in the cells below and read the comments to get a full picture of the API you will be using throughout this part of the project.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in /Users/jsboygenius/.conda/envs/189_a3c/lib/python3.7/site-packages (3.18.4.post1)\n",
      "Processing /Users/jsboygenius/Downloads/MehtaKnights-189/prob1/gym-note4-mdp\n",
      "Requirement already satisfied: gym in /Users/jsboygenius/.conda/envs/189_a3c/lib/python3.7/site-packages (from gym-note4-mdp==0.0.1) (0.17.3)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /Users/jsboygenius/.conda/envs/189_a3c/lib/python3.7/site-packages (from gym->gym-note4-mdp==0.0.1) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/jsboygenius/.conda/envs/189_a3c/lib/python3.7/site-packages (from gym->gym-note4-mdp==0.0.1) (1.19.2)\n",
      "Requirement already satisfied: scipy in /Users/jsboygenius/.conda/envs/189_a3c/lib/python3.7/site-packages (from gym->gym-note4-mdp==0.0.1) (1.5.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /Users/jsboygenius/.conda/envs/189_a3c/lib/python3.7/site-packages (from gym->gym-note4-mdp==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: future in /Users/jsboygenius/.conda/envs/189_a3c/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-note4-mdp==0.0.1) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Users/jsboygenius/.conda/envs/189_a3c/lib/python3.7/site-packages (from gym->gym-note4-mdp==0.0.1) (1.19.2)\n",
      "Building wheels for collected packages: gym-note4-mdp\n",
      "  Building wheel for gym-note4-mdp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym-note4-mdp: filename=gym_note4_mdp-0.0.1-py3-none-any.whl size=1067 sha256=5ed558698d65e6a72beaf2729fc2bc1f0017ed0b27d0f1f11d041e76db7f7409\n",
      "  Stored in directory: /Users/jsboygenius/Library/Caches/pip/wheels/ef/fa/b4/2c46177bfe4606ed81acc5de64062061740302a991a3c8114f\n",
      "Successfully built gym-note4-mdp\n",
      "Installing collected packages: gym-note4-mdp\n",
      "  Attempting uninstall: gym-note4-mdp\n",
      "    Found existing installation: gym-note4-mdp 0.0.1\n",
      "    Uninstalling gym-note4-mdp-0.0.1:\n",
      "      Successfully uninstalled gym-note4-mdp-0.0.1\n",
      "Successfully installed gym-note4-mdp-0.0.1\n"
     ]
    }
   ],
   "source": [
    "# Importing gym\n",
    "import gym\n",
    "\n",
    "# Installing the chain world environment\n",
    "# If this is failing, please try reinitializing the notebook in the project folder and not this specific problem folder.\n",
    "!pip install cmake .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing chain world environment\n",
    "import gym_note4_mdp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the environment\n",
    "chain_env = gym.make('note4-mdp-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment\n",
      "+---------+\n",
      "|B: :\u001b[43m \u001b[0m: :S|\n",
      "+---------+\n",
      "+-+        \n",
      "|T|        \n",
      "+-+        \n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at this environment\n",
    "chain_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `render()` function, you can visualize the chain world environment. This visualization has different parts. The _Environment_ part shows the agent in the environment. The agent is the yellow rectangle.\n",
    "\n",
    "**Let's go over the specifics of the chain world as we are learning the API. Look to the code blocks for specific functions.**\n",
    "\n",
    "### Chain World\n",
    "\n",
    "This is a simple environment which represents a chain of states. \n",
    "\n",
    "#### States\n",
    "\n",
    "The states are 0, 1, 2, 3, and 4 and they correspond to the position of the agent (the yellow rectangle) in the chain. Initially the agent starts deterministically at State 2. Verify that the setup makes sense by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment\n",
      "+---------+\n",
      "|B: :\u001b[43m \u001b[0m: :S|\n",
      "+---------+\n",
      "+-+        \n",
      "|T|        \n",
      "+-+        \n",
      "Agent's current state: 2\n"
     ]
    }
   ],
   "source": [
    "# The env.reset() method reinitializes the env to default values\n",
    "chain_env.reset()\n",
    "chain_env.render()\n",
    "# We can access the environments's current state by using env.state\n",
    "print(f\"Agent's current state: {chain_env.state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "In the chain world, there are three actions that the agent can take:\n",
    "     \n",
    "* (1) forward, which moves the agent forward along the chain\n",
    "* (0) backward, which moves the agent backward along the chain\n",
    "* (2) exit, which tries to exit the world\n",
    "\n",
    "The cell below will show you how to make actions using the `step(action)` function and will take you through one episode where the agent goes forward, backward, backward, backward, and then exits with reward +10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1: Initial State\n",
      "Environment\n",
      "+---------+\n",
      "|B: :\u001b[43m \u001b[0m: :S|\n",
      "+---------+\n",
      "+-+        \n",
      "|T|        \n",
      "+-+        \n",
      "\n",
      "Frame 2: Agent moves forward to state 3 and receives reward 0\n",
      "Environment\n",
      "+---------+\n",
      "|B: : :\u001b[43m \u001b[0m:S|\n",
      "+---------+\n",
      "+-+        \n",
      "|T|        \n",
      "+-+        \n",
      "\n",
      "Frame 3: Agent moves backward to state 2 and receives reward 0\n",
      "Environment\n",
      "+---------+\n",
      "|B: :\u001b[43m \u001b[0m: :S|\n",
      "+---------+\n",
      "+-+        \n",
      "|T|        \n",
      "+-+        \n",
      "\n",
      "Frame 4: Agent moves backward to state 1 and receives reward 0\n",
      "Environment\n",
      "+---------+\n",
      "|B:\u001b[43m \u001b[0m: : :S|\n",
      "+---------+\n",
      "+-+        \n",
      "|T|        \n",
      "+-+        \n",
      "\n",
      "Frame 5: Agent moves backward to state 0 and receives reward 0\n",
      "Environment\n",
      "+---------+\n",
      "|\u001b[43mB\u001b[0m: : : :S|\n",
      "+---------+\n",
      "+-+        \n",
      "|T|        \n",
      "+-+        \n",
      "\n",
      "Frame 6: Agent moves exit to state 5 (terminal state) and receives reward 10\n",
      "Environment\n",
      "+---------+\n",
      "|B: : : :S|\n",
      "+---------+\n",
      "+-+        \n",
      "|\u001b[42mT\u001b[0m|        \n",
      "+-+        \n"
     ]
    }
   ],
   "source": [
    "chain_env.reset()\n",
    "chain_env.discount_factor = 1\n",
    "done = False\n",
    "\n",
    "chain_env.slip = 0 # This is the probability of the agent slipping\n",
    "print(\"Frame 1: Initial State\")\n",
    "chain_env.render()\n",
    "\n",
    "state, reward, done, _ = chain_env.step(1) # .step(action) function takes an action on the environment\n",
    "print(f\"\\nFrame 2: Agent moves forward to state {state} and receives reward {reward}\")\n",
    "chain_env.render()\n",
    "\n",
    "state, reward, done, _ = chain_env.step(0) # moves backward\n",
    "print(f\"\\nFrame 3: Agent moves backward to state {state} and receives reward {reward}\")\n",
    "chain_env.render()\n",
    "\n",
    "state, reward, done, _ = chain_env.step(0) # moves backward\n",
    "print(f\"\\nFrame 4: Agent moves backward to state {state} and receives reward {reward}\")\n",
    "chain_env.render()\n",
    "\n",
    "state, reward, done, _ = chain_env.step(0) # moves backward\n",
    "print(f\"\\nFrame 5: Agent moves backward to state {state} and receives reward {reward}\")\n",
    "chain_env.render()\n",
    "\n",
    "state, reward, done, _ = chain_env.step(2) # exits\n",
    "print(f\"\\nFrame 6: Agent moves exit to state {state} (terminal state) and receives reward {reward}\")\n",
    "chain_env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Transition Functions\n",
    "\n",
    "From any state, the agent can try to move fowards (1), move backwards (0), or exit (2). But there is no guarantee that the agent will succeed. You can imagine that chain world is a slippery place. The transitions are not determistic. If you choose to go forwards, you could slip and go backwards with some probability and vice versa. Also, if the agent tries to exit from any state other than the 0 or 4, nothing will happen because that is not valid action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slippage probability in deterministic world: 0\n",
      "Slippage probability in uncertain world: 0.2\n",
      "Experiment Measured slip rates: [0.28, 0.22, 0.22, 0.15, 0.2, 0.22, 0.24, 0.25, 0.15, 0.24]\n"
     ]
    }
   ],
   "source": [
    "chain_env.reset()\n",
    "chain_env.slip = 0\n",
    "print(f\"Slippage probability in deterministic world: {chain_env.slip}\")\n",
    "chain_env.reset() # By default the world is uncertain\n",
    "print(f\"Slippage probability in uncertain world: {chain_env.slip}\")\n",
    "# We can also verify the slippage probability experimentally. Convince yourself that 0.2 is the slip rate.\n",
    "slip_counts = []\n",
    "for j in range(10):\n",
    "    slip_count = 0\n",
    "    for i in range(100):\n",
    "        chain_env.reset()\n",
    "        prev_state = chain_env.state\n",
    "        chain_env.step(0) # Move backwards\n",
    "        if chain_env.state >= prev_state:\n",
    "            slip_count += 1\n",
    "    slip_counts.append(slip_count/100)\n",
    "print(f\"Experiment Measured slip rates: {slip_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewards\n",
    "At the beginning of the chain (State 0) there is a big reward of +10 (B) that the agent will be rewarded if it chooses to exit (Action 2) when in State 0.\n",
    "\n",
    "At the end of the chain (State 4) there is a small reward of +1 (S) that the agent will be rewarded if it chooses to exit (Action 2) when in State 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small Reward: 1\n",
      "Big Reward: 10\n"
     ]
    }
   ],
   "source": [
    "chain_env.reset()\n",
    "print(f\"Small Reward: {chain_env.small}\")\n",
    "print(f\"Big Reward: {chain_env.large}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Discount Factor\n",
    "    \n",
    "The discount factor is $\\gamma = 0.1$. So as we take steps the rewards are discounted such that $${reward_{t+1}} = {reward_t}*{\\gamma}$$ \n",
    "\n",
    "Observe what happens to the small and large rewards as we take steps in the cells below for various discount factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fadd4f67d10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain_env.reset()\n",
    "discount_factors = [0.1, 0.5, 0.9, 0.99]\n",
    "small_rewards = []\n",
    "large_rewards = []\n",
    "for j in range(len(discount_factors)):\n",
    "    discount_factor = discount_factors[j]\n",
    "    chain_env.reset()\n",
    "    chain_env.discount_factor = discount_factor\n",
    "    \n",
    "    smalls = []\n",
    "    larges = []\n",
    "    smalls.append(chain_env.small)\n",
    "    larges.append(chain_env.large)\n",
    "    for i in range(1, 5):\n",
    "        action = chain_env.action_space.sample() #This lets us randomly sample an action\n",
    "        chain_env.step(action)\n",
    "        smalls.append(chain_env.small)\n",
    "        larges.append(chain_env.large)\n",
    "        \n",
    "    small_rewards.append(smalls)\n",
    "    large_rewards.append(larges)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "steps = range(5)\n",
    "for i in small_rewards:\n",
    "    plt.plot(steps, i)\n",
    "\n",
    "plt.title(\"Small Rewards Decaying Over Time with Different Discount Factors\")\n",
    "plt.ylabel(\"Reward Size\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.legend(discount_factors)\n",
    "\n",
    "plt.figure()\n",
    "for i in large_rewards:\n",
    "    plt.plot(steps, i)\n",
    "\n",
    "plt.title(\"Large Rewards Decaying Over Time with Different Discount Factors\")\n",
    "plt.ylabel(\"Reward Size\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.legend(discount_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi World\n",
    "\n",
    "Now that you've gotten a chance to familiarize yourself with the OpenAI Gym API and the simple chain world environment, let's take a look at a more complicated (albeit still very simple) environment we call Taxi World.\n",
    "\n",
    "After looking at the Taxi World in depth, you will look at a \"dumb agent\" that chooses moves randomly for Taxi World and Chain World that will serve as a baseline for better agents we look at later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "import numpy as np\n",
    "!pip install cmake 'gym[atari]' scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[43mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make the taxi world environment\n",
    "taxi_env = gym.make(\"Taxi-v3\").env\n",
    "# Render the environment using the OpenAI Gym API\n",
    "## Your code here ## (Ans: env.render())\n",
    "taxi_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing Taxi World \n",
    "\n",
    "In the Taxi World, the agent is a taxi cab. In the visualization, the taxi cab is the yellow rectangle. The Taxi World has walls, represented by | and the taxi agent cannot go through these walls. The Taxi World also has pickup/dropoff locations represented by the letters (R, G, Y, B) in the visualization. \n",
    "\n",
    "In each run of Taxi World, the taxi agent must pickup a passenger at the blue letter location, transport them to the pink letter location, and dropoff the passenger at the pink letter location.\n",
    "\n",
    "When the Taxi has picked up a passenger, its color will change from yellow to blue.\n",
    "\n",
    "Formally, we can represent this in terms of the states, actions, and rewards.\n",
    "\n",
    "* **States**\n",
    "    * The Taxi World environment has many more states than the Chain World environment. For the chain world, it was relatively easy to see that the chain world had 6 states (0, 1, 2, 3, 4, terminal state), but how would we figure out the number of states in the Taxi World?\n",
    "    * How many possible locations are there for the taxi agent?\n",
    "        * The taxi world is a 5x5 grid so there are 25 possible locations.\n",
    "    * For each location of the taxi, how many possible passenger locations are there?\n",
    "        * There are 4 possible passenger locations if the passenger has not yet been picked up and 1 possible passenger location if the passenger has been picked up. This gives us a total of 5 passenger locations.\n",
    "    * For each location of the taxi and passenger location, how many drop off locations are there?\n",
    "        * There are still 4 possible drop off locations\n",
    "    * How many total states are there? What is the size of the state space?\n",
    "        * So, in total there are $5*5*5*4 = 500$ states in this world! That's a lot more than there were in the chain world!  \n",
    "\n",
    "* **Actions**\n",
    "    * 0: south\n",
    "    * 1: north\n",
    "    * 2: east\n",
    "    * 3: west\n",
    "    * 4: pickup\n",
    "    * 5: dropoff\n",
    "\n",
    "* **Rewards**\n",
    "    * Movement actions have reward -1.0. A negative reward makes moving costly. This makes sense, because the taxi agent should be incentivized to pickup and dropoff passengers as efficiently as possible.\n",
    "    * Pickup/dropoff actions have reward -10 if the taxi agent is not at a pickup or dropoff location. This is a large penalty.\n",
    "    * At dropoff/pickup locations, dropoff and pickup would have higher rewards (+20), if the agent succeeds in picking up and dropping off the passenger at the correct locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Exercise: Random Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this visualization exercise, you will be looking at a random agent. While this agent has not reached a terminal state (signified by the variable `done`), it will continue making random actions. This is a really bad controller! It just moves around randomly! It sometimes stumbles into the right state and receives rewards, but lets visualize just how bad this agent is as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(env):\n",
    "    frames = []\n",
    "    actions_taken = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        actions_taken += 1\n",
    "        \n",
    "        frame = env.render(mode='ansi')\n",
    "        frames.append({\n",
    "            'frame': frame,\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'total reward': total_reward\n",
    "            }\n",
    "        )\n",
    "    return frames, actions_taken, total_reward\n",
    "    \n",
    "## Utils Functions to Run and Visualize ## \n",
    "# Source: see reference [1]\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames, t=0.1, time=True):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        if time:\n",
    "            print(f\"Time: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        print(f\"Total Reward: {frame['total reward']}\")\n",
    "        sleep(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions taken: 2257\n",
      "Total Reward: -8725\n"
     ]
    }
   ],
   "source": [
    "taxi_env.reset()\n",
    "frames, actions_taken, total_reward = random_agent(taxi_env)\n",
    "print(\"Actions taken: {}\".format(actions_taken))\n",
    "print(\"Total Reward: {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "Run the code cell below to visualize the random agent in the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "Time: 1001\n",
      "State: 288\n",
      "Action: 1\n",
      "Reward: -1\n",
      "Total Reward: -3953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6d29eebe5e33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Hit ctrl-c to interrupt this if you would rather not watch the agent the whole time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-fe34daa5e355>\u001b[0m in \u001b[0;36mprint_frames\u001b[0;34m(frames, t, time)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reward: {frame['reward']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total Reward: {frame['total reward']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hit ctrl-c to interrupt this if you would rather not watch the agent the whole time\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your understanding**: What can you say about the random agent in the Taxi world? About how many actions does the taxi agent take before picking up the passenger for the first time? How many actions does the random agent take before finally stumbling into the terminal state? How much reward does it incurr in total? Do you think we can do better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terrible performance of this random agent should motivate our study of reinforcement learning. The random agents performance in our taxi world will serve as our baseline for future agents we write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral of this exercise is that we can do better! We can do better by learning from our experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Exercise: Reinforcement Learning Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a Reinforcement Learning Agent (specifically a Q-Learning Agent) and see how great it is as learning what to do! Run the cells below and visualize the q-learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_agent(env, alpha=0.1, gamma=0.6, epsilon=0.1):\n",
    "\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    for i in range(1, 100001):\n",
    "        env.reset()\n",
    "        state = env.s\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample() \n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) \n",
    "\n",
    "            next_state, reward, done, _ = env.step(action) \n",
    "\n",
    "            next_value = np.max(q_table[next_state])\n",
    "            old_state_value = q_table[state, action]\n",
    "\n",
    "            q_table[state, action] = (1 - alpha) * old_state_value + alpha * (reward + gamma*next_value)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "    return q_table\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "q_table = q_learning_agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let the agent exploit its policy. ##\n",
    "episodes = 10 # You can change the number of episodes if you'd like to admire your agent for more or less time.\n",
    "frames = []\n",
    "\n",
    "for _ in range(episodes):\n",
    "    env.reset()\n",
    "    state = env.s\n",
    "    reward = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        frame = env.render(mode='ansi')\n",
    "        frames.append({\n",
    "            'frame': frame,\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'total reward': total_reward\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "State: 85\n",
      "Action: 5\n",
      "Reward: 20\n",
      "Total Reward: 10\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to visualize the q-learning agent.\n",
    "print_frames(frames, t=0.2, time=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! This is awesome! You should be able to see the q-learning agent initialize in a random state and quickly move toward the pickup location to pickup the passenger and then quickly move to the drop off location to dropoff the passenger before being reinitialized! Hopefully this project was a fun exercise to see the power of RL in some simple examples. In the next part of the project, we'll be going through some 2D physics RL examples with continuous observation spaces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Extra for Experts: Solving MDPs, Policy, Value Iteration, Policy Iteration, Policy Evaluation, and Policy Improvement, Offline Learning, Online Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material is all information we believe is necessary to look at to truly understand the underlying phenomena of reinforcement learning agent above. Feel free to read through this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will take a closer look at the following topics: \n",
    "* Policies\n",
    "* The Bellman Equation\n",
    "    * Values $V^*(s)$\n",
    "    * Q-Value $Q^*(s, a)$\n",
    "* Value Iteration\n",
    "* Policy Extraction\n",
    "* Policy Iteration\n",
    "* Policy Evaluation\n",
    "\n",
    "Each of the exercises you will do in this part will include reviewing the material, implementing the producedure in python, and seeing the results on the chain world and the taxi world. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in studying MDPs was to model the environment. We motivated MDPs by proclaiming that modeling the environment would help us use algorithms to learn the best strategies and optimal behavior. But what does it mean to behave optimally?\n",
    "\n",
    "To behave optimally is to follow the best **policy**. A **policy** is a mapping from states to actions. The policy is like the agent's intelligence, it tells the agent what action to take given a state. \n",
    "\n",
    "Now, it should be clear that our goal is to find the optimal policy. The optimal policy is the policy that will give the agent the maximum expected total reward in the environment. \n",
    "\n",
    "Let's take a look at two policies in the chain world and in the cell below compare the two policies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Exercise: Test out different policies in the MDP provided\n",
    "In the code cell below we have defined two policies, policy1 and policy2. Run the code cell with policy1 first and note how the agent moves through the world while following this policy, also note the total reward this policy yields the agent. Do the same for policy2. Which policy is better? After you have evaluated policy1 and policy2, write your own policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discount factor is: 0.1\n"
     ]
    }
   ],
   "source": [
    "chain_env.reset()\n",
    "print(f\"discount factor is: {chain_env.discount_factor}\")\n",
    "\n",
    "## COMPARING POLICY CODE ##\n",
    "\n",
    "FORWARD = 1\n",
    "BACKWARD = 0\n",
    "EXIT = 2\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "# Notice how the policy is a mapping from states --> actions\n",
    "policy1 = {0: EXIT, 1: BACKWARD, 2: FORWARD, 3: FORWARD, 4: EXIT, 5:EXIT}\n",
    "policy2 = {0: EXIT, 1: BACKWARD, 2: BACKWARD, 3: BACKWARD, 4: EXIT, 5: EXIT}\n",
    "policy3 = {} ## <- ENTER YOUR POLICY HERE ##\n",
    "\n",
    "chain_env.state = 2 ## <- TODO: CHANGE THE STARTING STATE ##\n",
    "\n",
    "frames = [{'frame': chain_env.render(mode='ansi'),'state': state,'action': action,'reward': reward, 'total reward':0}]\n",
    "\n",
    "#Change the line below to change the policy from 1 to 2 to 3\n",
    "policy = policy1 ## <- TODO: CHANGE THE POLICY ##\n",
    "\n",
    "while not done:\n",
    "    # Make sure the following 3 lines make sense!\n",
    "    current_state = chain_env.state\n",
    "    action = policy[current_state] \n",
    "    state, reward, done, _ = chain_env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    frames.append({'frame': chain_env.render(mode='ansi'),'state': state,'action': action,'reward': reward, 'total reward':total_reward})    \n",
    "## RUN THE CELL BELOW TO VISUALIZE THE AGENT EXECUTING THE POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment\n",
      "+---------+\n",
      "|B: : : :S|\n",
      "+---------+\n",
      "+-+        \n",
      "|\u001b[42mT\u001b[0m|        \n",
      "+-+        \n",
      "\n",
      "Time: 4\n",
      "State: 5\n",
      "Action: 2\n",
      "Reward: 0.0010000000000000002\n",
      "Total Reward: 0.0010000000000000002\n"
     ]
    }
   ],
   "source": [
    "## Run this cell to visualize\n",
    "print_frames(frames, t=0.6) # You may see the agent not follow the policy! Why? Remember uncertainity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, while it is nice to manually choose polcies or to think about optimal policies and manually implement them as we did above, in difficult problems or in complicated MDPs this is not possible nor efficient.   \n",
    "\n",
    "Let's see how algorithms can come up with optimal policies for our agents to use! To start our exploration, let's derive the bellman equation. The bellman equation forms the corner stone for a lot of what we will be doing. You can also refer to the note for a more in depth explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it is important to understand two quantities: $V^*(s)$ and $Q^*(s, a)$.\n",
    "\n",
    "1. $V^*(s)$ is the expected value of the reward that an optimally behaving agent starting in state s will receive over its entire lifetime.[2]  $V^*(s)$ is the maximum reward an agent following an optimal **policy** will achieve over its lifetime.\n",
    "\n",
    "2. $Q^*(s, a)$ is the expected value of the reward that an optimally behaving agent starting in q-state (s,a) will receive over its entire lifetime. $Q^*(s, a)$ is the maximum reward an agent following an optimal **policy** will achieve over its lifetime given that it started in state s and has taken action a.\n",
    "\n",
    "The Bellman Equation provides an equation for $V^*(s)$ and $Q^*(s, a)$ in terms of simpler quantities. Let's derive this equation.\n",
    "\n",
    "If you were an agent and you knew $V^*(s)$ for all states, and you were in a state $s$, how would you choose which action to take?\n",
    "\n",
    "If you were reward maximizing, you would take the action that leads you to the state  $s'$ with the maximum $V^*(s')$. Why? Because, by definition this would maximize the total reward you would receive if you continued acting optimally.\n",
    "\n",
    "But, remember, there is uncertainty in the MDP, so each action comes with a probability that you will transition to any state, namely, $T(s, a, s')$ so you would like to choose the action that maximizes your _expected_ future reward. \n",
    "\n",
    "Let's put this into a mathematical equation. This is the Bellman Equation.\n",
    "\n",
    " $$V^*(s) = \\max_{{a \\in actions}}{\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V^*(s')]}$$\n",
    " \n",
    " Notice that this equation exactly follows our thinking. The optimal value of a state is the probability weighted reward the agent receives in transitioning from state s to s' and the maximum expected optimal value of the next state s' (discounted by gamma). This is a beautifully simple equation, that decomposes the quantity $V^*(s)$ into subproblems $V^*(s')$.\n",
    " \n",
    " Similarly, $$Q^*(s, a) = {\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V^*(s')]}$$ since in a q-state we are given an action, so there is no maximization over the actions.\n",
    " \n",
    " Putting the two equations together we have, \n",
    "     $$V^*(s) = \\max_{{a \\in actions}}Q^*(s, a)$$\n",
    "     \n",
    "While these elegant equations put represent the optimal values and q-values of states in terms of other states' optimal values and q-values, how would we go about solving for the optimal values and q-values? Enter value iteration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Bellman equation will only be satisfied if for every state $V(s) = V^*(s)$ where $V(s)$ is the value of a state s and $V^*(s)$ is the optimal value of a state s.\n",
    "\n",
    "In other words the bellman equation does not hold if the values that we have assigned to a state do not match the optimal values. Because of this feature, we can use the bellman equation to tell us whether our values are optimal.\n",
    "\n",
    "The idea of value iteration, is to iteratively improve our estimates of the values of each state until we have learned the true optimal values. The key subproblem that we will use to solve for $V^*(s)$ is $V_k(s)$.\n",
    "\n",
    "$V_k(s)$ is the expected value of the reward that an optimally behaving agent starting in state s will receive over the next k timesteps. Remember, $V^*(s)$ is the expected value of the reward that an optimally behaving agent starting in state s will receive over its entire lifetime.\n",
    "\n",
    "When, $V_k(s) = V_{k+1}(s) = ... = V_{k+i}(s) = ... = V^*(s)$ the value for a state will have converged and we will have found the optimal value of a state s.\n",
    "\n",
    "By expanding our time horizon (increasing k) until convergence we can iteratively find $V^*(s)$. What does the algorithm look like?\n",
    "\n",
    "1. For all states, initialize $V_0(s) = 0$.\n",
    "2. Repeat the following while $V_k(s) \\neq V_{k+1}(s)$,\n",
    "    $$V_{k+1}(s) \\leftarrow \\max_{{a \\in actions}}{\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V_k(s')]} $$\n",
    "    \n",
    "Let's implement this for our chain world!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful functions\n",
    "\n",
    "There is one last part of the OpenAI gym API that we haven't showed you yet, the MDP table. The MDP table stores all the information about the environment in an accessible way. A demo on how to access important information follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(0.8, 0, 0, False), (0.2, 1, 0, False)], 1: [(0.8, 1, 0, False), (0.2, 0, 0, False)], 2: [(1, 5, 10, True)]}, 1: {0: [(0.8, 0, 0, False), (0.2, 2, 0, False)], 1: [(0.8, 2, 0, False), (0.2, 0, 0, False)], 2: [(1, 1, 0, False)]}, 2: {0: [(0.8, 1, 0, False), (0.2, 3, 0, False)], 1: [(0.8, 3, 0, False), (0.2, 1, 0, False)], 2: [(1, 2, 0, False)]}, 3: {0: [(0.8, 2, 0, False), (0.2, 4, 0, False)], 1: [(0.8, 4, 0, False), (0.2, 2, 0, False)], 2: [(1, 3, 0, False)]}, 4: {0: [(0.8, 3, 0, False), (0.2, 4, 0, False)], 1: [(0.8, 4, 0, False), (0.2, 3, 0, False)], 2: [(1, 5, 1, True)]}, 5: {0: [(1, 5, 0, True)], 1: [(1, 5, 0, True)], 2: [(1, 5, 0, True)]}}\n"
     ]
    }
   ],
   "source": [
    "# Viewing The MDP table\n",
    "print(chain_env.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.8, 0, 0, False), (0.2, 2, 0, False)],\n",
       " 1: [(0.8, 2, 0, False), (0.2, 0, 0, False)],\n",
       " 2: [(1, 1, 0, False)]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Woah that's a lot\n",
    "# The MDP table stores the following information for each state:\n",
    "# {action1: [(probability1, nextstate1, reward1, done1), (probability2, nextstate2, reward2, done2)], \n",
    "# action2: [(probability, nextstate, reward, done)], etc}\n",
    "\n",
    "# This the the entry for state 1.\n",
    "chain_env.P[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8, 0, 0, False), (0.2, 2, 0, False)]\n"
     ]
    }
   ],
   "source": [
    "# The first row tells us what would happen if we took action 0 (backwards) from this state,\n",
    "# The second row tells us what would happen if we took action 1 (forwards) from this state,\n",
    "# The third row tells us what would happen if we took action 2 (exit) from this state\n",
    "\n",
    "# Let's look at what the first row is telling us.\n",
    "state = 1\n",
    "action = 0\n",
    "print(chain_env.P[state][action])\n",
    "# > [(0.8, 0, 0, False), (0.2, 2, 0, False)]\n",
    "\n",
    "# The first entry tells us that there is a 0.8 probability of transitioning to nextstate 0, receiving reward 0, and not entering a terminal state\n",
    "# The second entry tells us that there is a 0.2 probability of transitioning to nextstate 2, receving reward 0, and not entering a terminal state\n",
    "# This should make sense to you. Remember the probability of slipping is 0.2.\n",
    "\n",
    "# This is useful because it gives us the T(s,a,s') and R(s,a,s') for every state and action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Value Iteration\n",
    "\n",
    "In this coding exercise, you will be viewing an implementation of value iteration for the chain world.\n",
    "\n",
    "First, instantiate a table of zeroes called v_table of size num_iters by num_states (env.observation_space.n). The kth row of this table should contain the $V_k(s)$ values. \n",
    "\n",
    "|  $k$        | $V_k(0)$    | $V_k(1)$      | $V_k(2)$    | ... |\n",
    "| :---        |    :----:   |          ---: |     ---:    | ---:|\n",
    "|   0         |             |               |             |     |\n",
    "|   1         |             |               |             |     |\n",
    "|   2         |             |               |             |     |\n",
    "|   ...       |             |               |             |     |\n",
    "|   num_iters         |             |               |             |     |\n",
    "\n",
    "\n",
    "Loop through possible actions and next states (s') and use the update rule to fill in the k+1th row of the table using the kth row of the table. This should follow from the update rule: \n",
    "$$V_{k+1}(s) \\leftarrow \\max_{{a \\in actions}}{\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V_k(s')]} $$\n",
    "\n",
    "or, in tabular form/pseudocode,\n",
    "\n",
    "`\n",
    "new_value_for_state = max(sum(transition_prob*(reward + discount*old_value_for_nextstate)\n",
    "`\n",
    "\n",
    "Remember you can get the transaction probability and the reward from `env.P` and can get the old values of the state from the value table as you create it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, num_iters):\n",
    "    ## This is necessary because taxi environment has no discount factor\n",
    "    try:\n",
    "        discount_factor = env.discount_factor\n",
    "    except:\n",
    "        discount_factor = 1\n",
    "        \n",
    "    v_table = np.zeros([num_iters+1, env.observation_space.n])\n",
    "    \n",
    "    for k in range(0, num_iters):\n",
    "        for state in range(env.observation_space.n):\n",
    "            reward_for_action = []\n",
    "            \n",
    "            for action in range(env.action_space.n):\n",
    "                reward_for_state = 0\n",
    "                \n",
    "                for entry in range(len(env.P[state][action])):\n",
    "                    transition_probability, next_state, reward, done = env.P[state][action][entry]\n",
    "                    reward_for_state += transition_probability*(reward + discount_factor*v_table[k][next_state])\n",
    "                \n",
    "                reward_for_action.append(reward_for_state)\n",
    "                \n",
    "            v_table[k+1][state] = np.max(reward_for_action)\n",
    "\n",
    "    return v_table, num_iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize convergence of values for different number of iterations. Run the cell below and change the number of iterations to see the convergence of value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = chain_env ## You can change this to taxi_env as well, but the taxi env is a larger environment so it will take more iterations.\n",
    "num_iters = 5 ## Change this value\n",
    "v_table, num_iters = value_iteration(env, num_iters)\n",
    "env.reset()\n",
    "\n",
    "def print_value_table(v_table, num_iters):\n",
    "    for i in range(len(v_table[-1])):\n",
    "        if i == 5:\n",
    "            print(f\"Value for terminal state after {num_iters} iterations = {v_table[-1][i]}\")\n",
    "        else:\n",
    "            print(f\"Value for State {i} after {num_iters} iterations = {v_table[-1][i]}\")\n",
    "\n",
    "def plot_convergence(v_table, num_iters):\n",
    "    plt.figure(figsize = (10, 4))\n",
    "    advantage = [np.sum([np.abs(v_table[i][j] - v_table[i-1][j]) for j in range(len(v_table[i]))]) for i in range(0, num_iters+1)]\n",
    "    plt.plot(range(0, num_iters+1), advantage)\n",
    "    plt.title(\"Convergence of Values in Value Iteration\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.ylabel(\"Difference between values\")\n",
    "\n",
    "# print_value_table(v_table, num_iters)\n",
    "plot_convergence(v_table, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the optimal values of each state for both taxi world and chain world, we would like to use these values to generate a policy. \n",
    "\n",
    "Remember a policy is like the agent's intelligence, given a state the policy will tell the agent what to do (what action to take). \n",
    "\n",
    "How would we extract a policy from the value table? With **policy extraction**, of course!\n",
    "\n",
    "**Policy extraction** is a method which extracts the actions that make up our policy.\n",
    "\n",
    "Again, let's logically reason through this. Given that you are in a state s, which action should you take? You should take the action which gives you the maximum expected total reward. Well, which action gives you the maximum expected total reward? If you remember the definition of a q-state from earlier. The action a that maximizes q(current state, a) will give you the biggest expected total utility. Going back to the bellman equation, this is: \n",
    "\n",
    "$$\\pi^*(s) = \\text{argmax}_{{a \\in actions}}{\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V^*(s')]}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_extraction(env, v_star):\n",
    "    '''v_star is an array with the optimal values of the states. It would be the last row of v_table.'''\n",
    "    policy = {}\n",
    "\n",
    "    for state in range(env.observation_space.n):\n",
    "        reward_for_action = []\n",
    "        for action in range(env.action_space.n):\n",
    "            reward_for_state = 0\n",
    "            for entry in range(len(env.P[state][action])):\n",
    "                transition_probability, next_state, reward, done = env.P[state][action][entry]\n",
    "                reward_for_state += transition_probability*(reward + discount_factor*v_star[next_state])\n",
    "            reward_for_action.append(reward_for_state) \n",
    "\n",
    "        policy[state] = np.argmax(np.asarray(reward_for_action))\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see if your policy makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From state 0 take action EXIT to maximize reward\n",
      "From state 1 take action BACKWARDS to maximize reward\n",
      "From state 2 take action BACKWARDS to maximize reward\n",
      "From state 3 take action FORWARDS to maximize reward\n",
      "From state 4 take action EXIT to maximize reward\n"
     ]
    }
   ],
   "source": [
    "env = chain_env\n",
    "env.reset()\n",
    "num_iters = 25\n",
    "v_table, _ = value_iteration(env, num_iters) # The best values are in the last row of the v_table\n",
    "\n",
    "def print_policy(env, policy):\n",
    "    if env.observation_space.n > 10:\n",
    "        print(\"We only print policies for chain world. If you want to, you can write your own code to print policy for taxi env.\")\n",
    "        return\n",
    "        \n",
    "    word = {0: \"BACKWARDS\", 1: \"FORWARDS\", 2: \"EXIT\"}\n",
    "    for state in policy:\n",
    "        if state != 5:\n",
    "            action = policy[state]\n",
    "            action = word[action]\n",
    "            print(f\"From state {state} take action {action} to maximize reward\")\n",
    "\n",
    "policy = policy_extraction(env, v_table[-1])\n",
    "print_policy(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to visualize the optimal policy that we learned through policy extraction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE POLICY IN ACTION\n",
    "env = chain_env\n",
    "env.reset()\n",
    "num_iters = 25\n",
    "v_table, _ = value_iteration(env, num_iters) \n",
    "policy = policy_extraction(env, v_table[-1])\n",
    "total_reward = 0\n",
    "# print_policy(env, policy)\n",
    "\n",
    "frames = [{'frame': env.render(mode='ansi'),'state': env.s,'action': 0,'reward': 0, 'total reward':0}]\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # Make sure the following 3 lines make sense!\n",
    "    current_state = env.s\n",
    "    action = policy[current_state] \n",
    "    state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    frames.append({'frame': env.render(mode='ansi'),'state': state,'action': action,'reward': reward, 'total reward':total_reward})    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment\n",
      "+---------+\n",
      "|B: : : :S|\n",
      "+---------+\n",
      "+-+        \n",
      "|\u001b[42mT\u001b[0m|        \n",
      "+-+        \n",
      "\n",
      "Time: 6\n",
      "State: 5\n",
      "Action: 2\n",
      "Reward: 0.00010000000000000003\n",
      "Total Reward: 0.00010000000000000003\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames, t=0.5) #Verify that this policy is optimal! Run the cell above and this cell a couple of times to \n",
    "# verify that even when the agent slips it makes the correct decisions afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in this section we extracted a policy from values we learned through value iteration. But what if we could learn our policy directly without learning the values first? Let's see how we could do this in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of value iteration is that it can be very slow. We have to learn all the values before we can learn the policy. **Policy iteration** improves on value iteration because we work directly to improve some initial policy until our policy converges to the optimal policy.\n",
    "\n",
    "Here are the basic steps of the policy iteration algorithm.\n",
    "\n",
    "1. Initialize the policy to an arbitrary policy. Remember a policy is simply a mapping between states and actions.\n",
    "\n",
    "2. Repeat the following:\n",
    "    * Evaluate the policy (**policy evaluation**) by computing the expected total reward of starting in state s and following policy $\\pi$. This is represented by the following equation: $$V^\\pi(s) = {\\sum_{s'}T(s, \\pi(s), s')[R(s, \\pi(s), s') + \\gamma*V^\\pi(s')]}$$\n",
    "        * We can compute this by iteratively updating until $V^\\pi(s)$ converges. $$V^\\pi_{k+1}(s) = {\\sum_{s'}T(s, \\pi_i(s), s')[R(s, \\pi_i(s), s') + \\gamma*V^{\\pi_i}_k(s')]}$$\n",
    "    * Then improve the policy (**policy improvement**) by running the policy extraction routine on the values of the states generated as a result of policy evaluation.\n",
    "        * $$\\pi_{i+1}(s) = \\text{argmax}_{{a \\in actions}}{\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma*V^{\\pi_i}(s')]}$$\n",
    "    * We can continue this routine until the policy converges and the policy at time step i is the same as the policy in time step \n",
    "    \n",
    "Implement the policy iteration algorithm below. Be sure to reference earlier parts, and try not to repeat yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initial Policy:\n",
      "From state 0 take action EXIT to maximize reward\n",
      "From state 1 take action FORWARDS to maximize reward\n",
      "From state 2 take action EXIT to maximize reward\n",
      "From state 3 take action BACKWARDS to maximize reward\n",
      "From state 4 take action EXIT to maximize reward\n",
      "\n",
      " Optimal Policy:\n",
      "From state 0 take action EXIT to maximize reward\n",
      "From state 1 take action BACKWARDS to maximize reward\n",
      "From state 2 take action BACKWARDS to maximize reward\n",
      "From state 3 take action FORWARDS to maximize reward\n",
      "From state 4 take action EXIT to maximize reward\n",
      "\n",
      " Number of Iterations in Policy Iteration: 2\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(env, value_convergence_threshold=1e-04):\n",
    "    initial_policy = get_initial_policy(env) #This method just gives you a random policy\n",
    "    policy = initial_policy\n",
    "    num_iters_before_convergence = 0\n",
    "    \n",
    "    policies_converged = False\n",
    "    \n",
    "    while not policies_converged:\n",
    "        num_iters_before_convergence += 1\n",
    "        v_table = np.zeros([100, env.observation_space.n])\n",
    "        \n",
    "        values_converged = False\n",
    "        k = 0\n",
    "        while not values_converged:\n",
    "            for state in range(env.observation_space.n):\n",
    "                reward_for_action = 0\n",
    "            \n",
    "                action = policy[state]\n",
    "                \n",
    "                for entry in range(len(env.P[state][action])):\n",
    "                    transition_probability, next_state, reward, done = env.P[state][action][entry]\n",
    "                    reward_for_action += transition_probability*(reward + env.discount_factor*v_table[k][next_state])\n",
    "                \n",
    "                v_table[k+1][state] = reward_for_action\n",
    "            \n",
    "                \n",
    "            if np.all([np.abs(v_table[k+1][state] - v_table[k][state]) < value_convergence_threshold for state in range(env.observation_space.n)]):\n",
    "                values_converged = True\n",
    "            else:\n",
    "                k += 1\n",
    "        \n",
    "        new_policy = policy_extraction(env, v_table[k+1])\n",
    "        \n",
    "        policies_converged = is_same_policy(policy, new_policy)\n",
    "        policy = new_policy\n",
    "  \n",
    "    return policy, initial_policy, num_iters_before_convergence\n",
    "\n",
    "\n",
    "### Useful Utils ###\n",
    "def get_initial_policy(env, stable=False):\n",
    "    # This will return an arbitrarily chosen initial policy\n",
    "    if stable:\n",
    "        return {0: 2, 1: 1, 2: 1, 3: 1, 4: 2, 5: 0}\n",
    "    return {0: env.action_space.sample(), 1: env.action_space.sample(), 2: env.action_space.sample(), 3: env.action_space.sample(), 4: env.action_space.sample(), 5: env.action_space.sample()}\n",
    "    \n",
    "def is_same_policy(policy1, policy2):\n",
    "    for state in policy1:\n",
    "        if policy1[state] != policy2[state]:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "policy, initial_policy, num_iters_before_convergence = policy_iteration(chain_env)\n",
    "print(\" Initial Policy:\")\n",
    "print_policy(chain_env, initial_policy)\n",
    "print(\"\\n Optimal Policy:\")\n",
    "print_policy(chain_env, policy)\n",
    "print(f\"\\n Number of Iterations in Policy Iteration: {num_iters_before_convergence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code block above multiple times, and you have implemented policy iteration correctly, you should see that no matter the starting policy, policy iteration will learn the optimal policy we uncovered in the previous part in a few iterations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you should have experience writing random moving agents, working with OpenAI gym API, and solving MDPs using value iteration, and learning optimal policies by hand and by using value iteration with policy extraction and policy iteration on the simple chain world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline vs Online Planning\n",
    "\n",
    "What if we don't know the underlying MDP? Everything we have done so far assumes that the the agent has complete knowledge of both the transition function and the reward function. (We were directly accessing `env.P` the MDP table). This is called **offline planning** because the agent can learn an optimal policy without ever having to explore the world.\n",
    "\n",
    "The assumption that the agent has knowledge of the reward and transition function is not reasonable in the real world and in many environments. In situations where the agents does not know the transition of reward function, the agent must **explore** the environment to learn the environment's intrinsics. This is called **online planning** and it will be the focus of the remainder of this part of the project.\n",
    "\n",
    "There are two broad distinctions within online planning. The first category of algorithms is called **model based** and the second category is called **model free**. Let's take a closer look at both of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Planning: Model-Based\n",
    "\n",
    "In **model-based** learning algorithms, the agent tries to learn the underlying MDP through exploration. Essentially, the agent will simply maintain counts of the transitions and rewards it receives to build an approximation of the transition and reward functions. In other words, through exploration the agent will try to build the MDP table `env.P`.\n",
    "\n",
    "This is a rather simple learning algorithm and while it is effective, it is rather boring, since you would just be essentially letting the random agent move through the environment and keeping counts of the rewards and transitions as the agent explores and then normalizing these counts at the end of learning. Then, the underlying MDP could be solved using value iteration and policy extraction or policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Planning: Model-Free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **model-free** learning, the goal is to learn the policy directly without first estimating the intrinsics of the MDP (transition and reward functions). Let's first see how we can evaluate policies (**policy evaluation**) in a model-free learning scenario using **Direct Evaluation** and **Temporal Difference Learning**. Then, we will see how to learn the optimal policy through learning q-values in q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Free: Direct Evaluation\n",
    "\n",
    "In **Direct Evaluation** the goal is to evaluate a policy by following it for some number of **episodes**. An **episode** is a single run on the environment from intitial state to terminal state. As the agent goes through multiple episodes of following the policy, it maintains counts of the total reward it received from each each state (in all future steps after that state) and a count for how many times each state was visited. \n",
    "\n",
    "From the total reward received from each state and the number of times the state was visited, the agent can estimate $V^\\pi(s)$.\n",
    "\n",
    "Specifically, $V^\\pi(s) = \\frac{\\text{total reward}}{\\text{times state \n",
    "visited}}$\n",
    "\n",
    "Let's create a table that looks like the the table below and populate it with the results of running the agent on the environment following the specified policy.\n",
    "\n",
    "| State       | Total Reward| Times Visited |  $V^\\pi(s)$ |\n",
    "| :---        |    :----:   |          ---: |     ---:    |\n",
    "|   0         |             |               |             |\n",
    "|   1         |             |               |             |\n",
    "|   2         |             |               |             |\n",
    "|   3         |             |               |             |\n",
    "|   4         |             |               |             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_evaluation(env, policy, episodes=10):\n",
    "    table = np.zeros((env.observation_space.n, 3))\n",
    "    TOTAL_REWARD_COL = 0\n",
    "    TIMES_VISITED_COL = 1\n",
    "    VALUES_COL = 2\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        env.reset()\n",
    "        states_visited = []\n",
    "        done = False\n",
    "        \n",
    "        next_state = env.state\n",
    "        while not done:\n",
    "            current_state = next_state\n",
    "            states_visited.append(current_state)\n",
    "            table[current_state][TIMES_VISITED_COL] += 1\n",
    "        \n",
    "            action = policy[current_state]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            for s in states_visited:\n",
    "                table[s][TOTAL_REWARD_COL] += reward\n",
    "    \n",
    "    for state in table:\n",
    "        state[VALUES_COL] = float(state[TOTAL_REWARD_COL])/max(0.0000001, state[TIMES_VISITED_COL])\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize\n",
    "env = chain_env\n",
    "env.reset()\n",
    "random_policy = get_initial_policy(env, stable=True)\n",
    "table = direct_evaluation(env, random_policy)\n",
    "\n",
    "def print_direct_eval_table(table):\n",
    "    i = 0\n",
    "    print(\"Direct Evaluation Table:\")\n",
    "    print(\"{:<8} {:<25} {:<15} {:<10}\".format(\"state\", \"total reward\", \"times visited\", \"value\"))\n",
    "    for state in table:\n",
    "        print(\"{:<8} {:<25} {:<15} {:<10}\".format(i, state[0], state[1], state[2]))\n",
    "        i+=1\n",
    "print(\"Policy:\")\n",
    "print_policy(env, random_policy)\n",
    "print(\"\")\n",
    "print_direct_eval_table(table)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is one way to do policy evaluation without a model. Remember, we can always use policy evaluation along with policy improvement to iteratively improve our model. Let's take a look at another way to do model free policy evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Free: Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While, Direct Evaluation was very simple to understand and implement, there are some inconsistencies in the logic behind direct evaluation. Firstly, direct evaluation does not learn as it follows the policy, it simply collects information, and then, after reaching a terminal state, in combines all of this information into something it can learn from. While this model can work, in practice it would take much too long. Why don't we learn after each move?\n",
    "\n",
    "This is where temporal difference learning comes in. In temporal difference learning, the agent has a running estimate of each state's optimal value which it updates using new samples as it collects them. \n",
    "\n",
    "Let's see the procedure the agent follows and then we'll take it apart before implementing it.\n",
    "\n",
    "1. For all states s, set $V^\\pi(s) = 0$ to start.\n",
    "2. While the number of episodes of exploration have not finished:\n",
    "    * According to the agent's policy $\\pi$, take an action and receive reward $R(s, a, s')$.\n",
    "    * With this reward, construct a sample: $\\text{sample} = R(s, a, s') + \\gamma*V^\\pi(s')$, this sample is a new estimate of $V^\\pi(s)$.\n",
    "    * Update $V^\\pi(s)$ using the update rule: $$V^\\pi(s) \\leftarrow (1 - \\alpha)*V^\\pi(s) + \\alpha*\\text{sample}.$$\n",
    "    * As you progress, shrink $\\alpha$ as a function of the number of times we enter a state s'. (You do not have to implement this step.)\n",
    "    * If you reach a terminal state, increment the number of episodes, reset the environment, and continue learning.\n",
    "    \n",
    "The most important aspect of this is the update rule. The update rule captures the sentiment that motivated us to move away from direct evaluation. We are now learning as we are taking actions. The update rule also answers the question of how to incorporate our new sample into our running estimate. According to the update rule, we use a learning rate $\\alpha$ to guide how much weight to give the new sample in our estimate. This should make sense because as we learn, our estimate should get better, so we should give more weight to newer samples. Try unravelling the recursion in the update rule for a few steps so you can see how past estimates are given less and less weight in our overall estimate. \n",
    "\n",
    "In the cell below, implement temporal difference learning. You are given an environment, a policy, a learning rate alpha, and the number of episodes of learning to do. If you would like to make more sense of your result. Set env.discount_factor = gamma and env.slip = slip, after each time you reset your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference_learning(env, policy, alpha, gamma=1, slip=0, episodes=10):\n",
    "    # TODO: IMPLEMENT TEMPORAL DIFFERENCE LEARNING #\n",
    "    v_pi = np.zeros([episodes, env.observation_space.n])\n",
    "    \n",
    "    \n",
    "    for episode in range(1, episodes):\n",
    "        env.reset()\n",
    "        env.discount_factor = gamma\n",
    "        env.slip = slip\n",
    "        state = env.s\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action) \n",
    "\n",
    "            old_value_state = v_pi[episode - 1][state]\n",
    "            value_next_state = v_pi[episode - 1][next_state]\n",
    "            \n",
    "            sample = (reward + env.discount_factor * value_next_state)\n",
    "\n",
    "            new_value_state = (1 - alpha) * old_value_state + alpha * sample \n",
    "            v_pi[episode][state] = new_value_state\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "    return v_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the table\n",
    "def print_temporal_difference_table(table, num_rows):\n",
    "    i = 0\n",
    "    total = len(table)\n",
    "    table = table[-num_rows:]\n",
    "    print(\"Temporal Difference Table:\")\n",
    "    print(\"{:<8} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\"episode\", \"state 0\", \"state 1\", \"state 2\", \"state 3\", \"state 4\", \"state 5\"))\n",
    "    for episode in table:\n",
    "        print(\"{:<8} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format((total - num_rows + 1) + i, round(table[i][0], 8), round(table[i][1], 8), round(table[i][2], 8), round(table[i][3], 8), round(table[i][4], 8), round(table[i][5], 8)))\n",
    "        i+=1\n",
    "        \n",
    "chain_env.reset()      \n",
    "alpha = 0.75\n",
    "episodes = 1000\n",
    "table = temporal_difference_learning(chain_env, get_initial_policy(chain_env, stable=True), alpha, gamma=1, slip=0, episodes=episodes)\n",
    "print_temporal_difference_table(table, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above should show the last few rows of temporal difference learning algorithm. Try changing alpha, what happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Free: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal Difference learning and Direct Evaluation are great ways to evaluate the policy we have learned, but what about learning the policy directly through exploration. This is where we introduce Q-Learning. \n",
    "\n",
    "Q-Learning is an algorithm that learns a policy through learning the q-values of each state. Like temporal difference learning, q-learning takes samples and incorporates the samples into the running estimates of the q-states using $alpha$, the learning rate.\n",
    "\n",
    "Once we have learned the q-values, extracting the policy is extremely easy. From each state s, take the action a that maximizes $Q(s, a)$. This is a simple argmax. As we have been doing, we will build a lookup table of q-values that we will iteratively improve as we explore the world. But this brings up a question, there is no policy that q-learning follows, how will the agent explore the world?\n",
    "\n",
    "If you remember the random agent from part 1 of this part of the project, the random agent sampled actions randomly, q-learning will take some proportion of random actions to **explore** the world. The q-learning agent will also **exploit** its estimate of the optimal policy some proportion of the time by finding the action that maximizes the q-value from the state it is currently in using the running estimates of the q-value. This balance between **exploration** and **exploitation** is called an $\\epsilon$-greedy strategy, because the q-learning agent explores (takes random actions) some $0 < \\epsilon < 1$ proportion of the time.\n",
    "\n",
    "Here is the q-learning algorithm in all its glory.\n",
    "\n",
    "1. Initialize the Q-Table to all zeroes.\n",
    "2. For some number of episodes,\n",
    "    * While the episode is not over (a terminal state is not reached):\n",
    "        * Take a random action with probability $\\epsilon$\n",
    "        * Otherwise take an action according to the current estimates using: $$a = \\text{argmax}_{a\\in actions}{Q(s, a)}$$ This equates to saying for all possible actions from state s, select the one with the highest Q-value.\n",
    "        * Travel to the next state $s'$ as a result of the action.\n",
    "        * For all possible actions from state $s'$ select the one with the highest q-value.\n",
    "        * Update the Q-table using the update rule: $$ Q(s, a) \\leftarrow (1-\\alpha)*Q(s, a) + \\alpha*(R(s,a,s') + \\gamma*\\max_{a}{Q(s', a)})$$\n",
    "        * Set the next state as the current state, and continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] https://medium.com/@apoddar573/making-your-own-custom-environment-in-gym-c3b65ff8cdaa\n",
    "\n",
    "[2] https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "\n",
    "[3] https://inst.eecs.berkeley.edu/~cs188/fa20/assets/notes/note04.pdf\n",
    "\n",
    "[4] https://inst.eecs.berkeley.edu/~cs188/fa20/assets/notes/note05.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
